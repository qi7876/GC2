# ===============================================================================================
# The following shows the last analyze fail log message.
# ===============================================================================================

----------------------------------------------------
- Caught exception:
----------------------------------------------------
For 'MatMul' the input dimensions must be equal, but got 'x1_col': 3136 and 'x2_row': 2304.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore/core/ops/mat_mul.cc:107 InferShape

----------------------------------------------------
- The Traceback of Net Construct Code:
----------------------------------------------------
# 0 In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:417
        if not self.sense_flag:
# 1 In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:418
            return self._no_sens_impl(*inputs)
                   ^
# 2 In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:437
        if self.return_grad:
# 3 In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:433
        loss = self.network(*inputs)
               ^
# 4 In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:121
        out = self._backbone(data)
              ^
# 5 In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:135
        x = self.relu(self.fc1(x))
                      ^
# 6 In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:137
        x = self.fc3(x)
            ^
# 7 In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:628
        if self.has_bias:
# 8 In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:629
            x = self.bias_add(x, self.bias)
            ^
# 9 In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:630
        if self.activation_flag:
# 10 In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632
        if len(x_shape) != 2:
# 11 In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:627
        x = self.matmul(x, self.weight)
            ^

# ===============================================================================================
# The following shows the IR when the function graphs evaluation fails to help locate the problem.
# You can search the last ------------------------> to the node which is evaluated failure.
# Refer to https://www.mindspore.cn/search?inputValue=analyze_fail.ir to get more instructions.
# ===============================================================================================

# IR entry: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12
# Total subgraphs: 183

# Attrs:
training : 1

# Total params: 68
# Params:
%para1_inputs0 : <null>
%para2_inputs1 : <null>
%para3_conv1.weight : <Ref[Tensor[Float32]], (32, 1, 3, 3), ref_key=:conv1.weight>  :  has_default
%para4_conv2.weight : <Ref[Tensor[Float32]], (32, 1, 5, 5), ref_key=:conv2.weight>  :  has_default
%para5_conv3.weight : <Ref[Tensor[Float32]], (32, 1, 7, 7), ref_key=:conv3.weight>  :  has_default
%para6_conv4.weight : <Ref[Tensor[Float32]], (64, 96, 3, 3), ref_key=:conv4.weight>  :  has_default
%para7_bn1.gamma : <Ref[Tensor[Float32]], (32), ref_key=:bn1.gamma>  :  has_default
%para8_bn1.beta : <Ref[Tensor[Float32]], (32), ref_key=:bn1.beta>  :  has_default
%para9_bn2.gamma : <Ref[Tensor[Float32]], (32), ref_key=:bn2.gamma>  :  has_default
%para10_bn2.beta : <Ref[Tensor[Float32]], (32), ref_key=:bn2.beta>  :  has_default
%para11_bn3.gamma : <Ref[Tensor[Float32]], (32), ref_key=:bn3.gamma>  :  has_default
%para12_bn3.beta : <Ref[Tensor[Float32]], (32), ref_key=:bn3.beta>  :  has_default
%para13_bn4.gamma : <Ref[Tensor[Float32]], (64), ref_key=:bn4.gamma>  :  has_default
%para14_bn4.beta : <Ref[Tensor[Float32]], (64), ref_key=:bn4.beta>  :  has_default
%para15_fc1.weight : <Ref[Tensor[Float32]], (256, 2304), ref_key=:fc1.weight>  :  has_default
%para16_fc1.bias : <Ref[Tensor[Float32]], (256), ref_key=:fc1.bias>  :  has_default
%para17_fc2.weight : <Ref[Tensor[Float32]], (128, 256), ref_key=:fc2.weight>  :  has_default
%para18_fc2.bias : <Ref[Tensor[Float32]], (128), ref_key=:fc2.bias>  :  has_default
%para19_fc3.weight : <Ref[Tensor[Float32]], (10, 128), ref_key=:fc3.weight>  :  has_default
%para20_fc3.bias : <Ref[Tensor[Float32]], (10), ref_key=:fc3.bias>  :  has_default
%para21_moment1.conv1.weight : <Ref[Tensor[Float32]], (32, 1, 3, 3), ref_key=:moment1.conv1.weight>  :  has_default
%para22_moment1.conv2.weight : <Ref[Tensor[Float32]], (32, 1, 5, 5), ref_key=:moment1.conv2.weight>  :  has_default
%para23_moment1.conv3.weight : <Ref[Tensor[Float32]], (32, 1, 7, 7), ref_key=:moment1.conv3.weight>  :  has_default
%para24_moment1.conv4.weight : <Ref[Tensor[Float32]], (64, 96, 3, 3), ref_key=:moment1.conv4.weight>  :  has_default
%para25_moment1.bn1.gamma : <Ref[Tensor[Float32]], (32), ref_key=:moment1.bn1.gamma>  :  has_default
%para26_moment1.bn1.beta : <Ref[Tensor[Float32]], (32), ref_key=:moment1.bn1.beta>  :  has_default
%para27_moment1.bn2.gamma : <Ref[Tensor[Float32]], (32), ref_key=:moment1.bn2.gamma>  :  has_default
%para28_moment1.bn2.beta : <Ref[Tensor[Float32]], (32), ref_key=:moment1.bn2.beta>  :  has_default
%para29_moment1.bn3.gamma : <Ref[Tensor[Float32]], (32), ref_key=:moment1.bn3.gamma>  :  has_default
%para30_moment1.bn3.beta : <Ref[Tensor[Float32]], (32), ref_key=:moment1.bn3.beta>  :  has_default
%para31_moment1.bn4.gamma : <Ref[Tensor[Float32]], (64), ref_key=:moment1.bn4.gamma>  :  has_default
%para32_moment1.bn4.beta : <Ref[Tensor[Float32]], (64), ref_key=:moment1.bn4.beta>  :  has_default
%para33_moment1.fc1.weight : <Ref[Tensor[Float32]], (256, 2304), ref_key=:moment1.fc1.weight>  :  has_default
%para34_moment1.fc1.bias : <Ref[Tensor[Float32]], (256), ref_key=:moment1.fc1.bias>  :  has_default
%para35_moment1.fc2.weight : <Ref[Tensor[Float32]], (128, 256), ref_key=:moment1.fc2.weight>  :  has_default
%para36_moment1.fc2.bias : <Ref[Tensor[Float32]], (128), ref_key=:moment1.fc2.bias>  :  has_default
%para37_moment1.fc3.weight : <Ref[Tensor[Float32]], (10, 128), ref_key=:moment1.fc3.weight>  :  has_default
%para38_moment1.fc3.bias : <Ref[Tensor[Float32]], (10), ref_key=:moment1.fc3.bias>  :  has_default
%para39_moment2.conv1.weight : <Ref[Tensor[Float32]], (32, 1, 3, 3), ref_key=:moment2.conv1.weight>  :  has_default
%para40_moment2.conv2.weight : <Ref[Tensor[Float32]], (32, 1, 5, 5), ref_key=:moment2.conv2.weight>  :  has_default
%para41_moment2.conv3.weight : <Ref[Tensor[Float32]], (32, 1, 7, 7), ref_key=:moment2.conv3.weight>  :  has_default
%para42_moment2.conv4.weight : <Ref[Tensor[Float32]], (64, 96, 3, 3), ref_key=:moment2.conv4.weight>  :  has_default
%para43_moment2.bn1.gamma : <Ref[Tensor[Float32]], (32), ref_key=:moment2.bn1.gamma>  :  has_default
%para44_moment2.bn1.beta : <Ref[Tensor[Float32]], (32), ref_key=:moment2.bn1.beta>  :  has_default
%para45_moment2.bn2.gamma : <Ref[Tensor[Float32]], (32), ref_key=:moment2.bn2.gamma>  :  has_default
%para46_moment2.bn2.beta : <Ref[Tensor[Float32]], (32), ref_key=:moment2.bn2.beta>  :  has_default
%para47_moment2.bn3.gamma : <Ref[Tensor[Float32]], (32), ref_key=:moment2.bn3.gamma>  :  has_default
%para48_moment2.bn3.beta : <Ref[Tensor[Float32]], (32), ref_key=:moment2.bn3.beta>  :  has_default
%para49_moment2.bn4.gamma : <Ref[Tensor[Float32]], (64), ref_key=:moment2.bn4.gamma>  :  has_default
%para50_moment2.bn4.beta : <Ref[Tensor[Float32]], (64), ref_key=:moment2.bn4.beta>  :  has_default
%para51_moment2.fc1.weight : <Ref[Tensor[Float32]], (256, 2304), ref_key=:moment2.fc1.weight>  :  has_default
%para52_moment2.fc1.bias : <Ref[Tensor[Float32]], (256), ref_key=:moment2.fc1.bias>  :  has_default
%para53_moment2.fc2.weight : <Ref[Tensor[Float32]], (128, 256), ref_key=:moment2.fc2.weight>  :  has_default
%para54_moment2.fc2.bias : <Ref[Tensor[Float32]], (128), ref_key=:moment2.fc2.bias>  :  has_default
%para55_moment2.fc3.weight : <Ref[Tensor[Float32]], (10, 128), ref_key=:moment2.fc3.weight>  :  has_default
%para56_moment2.fc3.bias : <Ref[Tensor[Float32]], (10), ref_key=:moment2.fc3.bias>  :  has_default
%para57_beta1_power : <Ref[Tensor[Float32]], (), ref_key=:beta1_power>  :  has_default
%para58_beta2_power : <Ref[Tensor[Float32]], (), ref_key=:beta2_power>  :  has_default
%para59_global_step : <Ref[Tensor[Int32]], (1), ref_key=:global_step>  :  has_default
%para60_learning_rate : <Ref[Tensor[Float32]], (), ref_key=:learning_rate>  :  has_default
%para61_bn4.moving_mean : <Ref[Tensor[Float32]], (64), ref_key=:bn4.moving_mean>  :  has_default
%para62_bn4.moving_variance : <Ref[Tensor[Float32]], (64), ref_key=:bn4.moving_variance>  :  has_default
%para63_bn1.moving_mean : <Ref[Tensor[Float32]], (32), ref_key=:bn1.moving_mean>  :  has_default
%para64_bn1.moving_variance : <Ref[Tensor[Float32]], (32), ref_key=:bn1.moving_variance>  :  has_default
%para65_bn2.moving_mean : <Ref[Tensor[Float32]], (32), ref_key=:bn2.moving_mean>  :  has_default
%para66_bn2.moving_variance : <Ref[Tensor[Float32]], (32), ref_key=:bn2.moving_variance>  :  has_default
%para67_bn3.moving_mean : <Ref[Tensor[Float32]], (32), ref_key=:bn3.moving_mean>  :  has_default
%para68_bn3.moving_variance : <Ref[Tensor[Float32]], (32), ref_key=:bn3.moving_variance>  :  has_default

subgraph attr:
training : 1
subgraph instance: mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12 : 0x684a070
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:416/    def construct(self, *inputs):/
subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12(%para1_inputs0, %para2_inputs1, %para3_conv1.weight, %para4_conv2.weight, %para5_conv3.weight, %para6_conv4.weight, %para7_bn1.gamma, %para8_bn1.beta, %para9_bn2.gamma, %para10_bn2.beta, %para11_bn3.gamma, %para12_bn3.beta, %para13_bn4.gamma, %para14_bn4.beta, %para15_fc1.weight, %para16_fc1.bias, %para17_fc2.weight, %para18_fc2.bias, %para19_fc3.weight, %para20_fc3.bias, %para21_moment1.conv1.weight, %para22_moment1.conv2.weight, %para23_moment1.conv3.weight, %para24_moment1.conv4.weight, %para25_moment1.bn1.gamma, %para26_moment1.bn1.beta, %para27_moment1.bn2.gamma, %para28_moment1.bn2.beta, %para29_moment1.bn3.gamma, %para30_moment1.bn3.beta, %para31_moment1.bn4.gamma, %para32_moment1.bn4.beta, %para33_moment1.fc1.weight, %para34_moment1.fc1.bias, %para35_moment1.fc2.weight, %para36_moment1.fc2.bias, %para37_moment1.fc3.weight, %para38_moment1.fc3.bias, %para39_moment2.conv1.weight, %para40_moment2.conv2.weight, %para41_moment2.conv3.weight, %para42_moment2.conv4.weight, %para43_moment2.bn1.gamma, %para44_moment2.bn1.beta, %para45_moment2.bn2.gamma, %para46_moment2.bn2.beta, %para47_moment2.bn3.gamma, %para48_moment2.bn3.beta, %para49_moment2.bn4.gamma, %para50_moment2.bn4.beta, %para51_moment2.fc1.weight, %para52_moment2.fc1.bias, %para53_moment2.fc2.weight, %para54_moment2.fc2.bias, %para55_moment2.fc3.weight, %para56_moment2.fc3.bias, %para57_beta1_power, %para58_beta2_power, %para59_global_step, %para60_learning_rate, %para61_bn4.moving_mean, %para62_bn4.moving_variance, %para63_bn1.moving_mean, %para64_bn1.moving_variance, %para65_bn2.moving_mean, %para66_bn2.moving_variance, %para67_bn3.moving_mean, %para68_bn3.moving_variance) {

#------------------------> 0
  %1(CNode_33) = call @✓mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_13()
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:417/        if not self.sense_flag:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:417/        if not self.sense_flag:/
}
# Order:
#   1: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12:CNode_33{[0]: ValueNode<FuncGraph> ✓mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_13}
#   2: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12:CNode_34{[0]: ValueNode<Primitive> Return, [1]: CNode_33}


subgraph attr:
training : 1
subgraph instance: ✓mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_13 : 0x6a9c5e0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:416/    def construct(self, *inputs):/
subgraph @✓mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_13 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12]() {
  %1(CNode_35) = $(mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12):MakeTuple(%para1_inputs0, %para2_inputs1)
      : (<Tensor[Float32], (32, 1, 32, 32)>, <Tensor[Int32], (32)>) -> (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>)
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:416/    def construct(self, *inputs):/

#------------------------> 1
  %2(CNode_36) = UnpackCall_unpack_call(@_no_sens_impl_37, %1)
      : (<Func, NoShape>, <Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>) -> (<null>)
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
  Return(%2)
      : (<null>)
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
}
# Order:
#   1: @✓mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_13:CNode_36{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.38, [1]: ValueNode<FuncGraph> _no_sens_impl_37, [2]: CNode_35}
#   2: @✓mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_13:CNode_39{[0]: ValueNode<Primitive> Return, [1]: CNode_36}


subgraph attr:
core : 1
subgraph instance: UnpackCall_14 : 0x6dc4be0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
subgraph @UnpackCall_14(%para69_, %para70_) {
  %1(CNode_36) = TupleGetItem(%para70_16, I64(0))
      : (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>, <Int64, NoShape>) -> (<Tensor[Float32], (32, 1, 32, 32)>)
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
  %2(CNode_36) = TupleGetItem(%para70_16, I64(1))
      : (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>, <Int64, NoShape>) -> (<Tensor[Int32], (32)>)
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/

#------------------------> 2
  %3(CNode_36) = %para69_15(%1, %2)
      : (<Tensor[Float32], (32, 1, 32, 32)>, <Tensor[Int32], (32)>) -> (<null>)
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
  Return(%3)
      : (<null>)
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
}
# Order:
#   1: @UnpackCall_14:CNode_36{[0]: param_15, [1]: CNode_36, [2]: CNode_36}
#   2: @UnpackCall_14:CNode_36{[0]: ValueNode<Primitive> Return, [1]: CNode_36}


subgraph attr:
training : 1
subgraph instance: _no_sens_impl_17 : 0x6d5ed60
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @_no_sens_impl_17 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12](%para71_inputs0, %para72_inputs1) {

#------------------------> 3
  %1(CNode_40) = call @✗_no_sens_impl_18()
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:437/        if self.return_grad:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:437/        if self.return_grad:/
}
# Order:
#   1: @_no_sens_impl_17:loss{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.41, [1]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_23, [2]: CNode_42}
#   2: @_no_sens_impl_17:grads{[0]: ValueNode<UnpackGraphPrimitive> UnpackGraph, [1]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_23, [2]: CNode_42}
#   3: @_no_sens_impl_17:grads{[0]: ValueNode<DoSignaturePrimitive> S_Prim_grad, [1]: grads, [2]: CNode_43}
#   4: @_no_sens_impl_17:grads{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.44, [1]: grads, [2]: CNode_42}
#   5: @_no_sens_impl_17:grads{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Identity_construct_45, [1]: grads}
#   6: @_no_sens_impl_17:CNode_46{[0]: ValueNode<FuncGraph> mindspore_nn_optim_adam_Adam_construct_47, [1]: grads}
#   7: @_no_sens_impl_17:loss{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Depend, [1]: loss, [2]: CNode_46}
#   8: @_no_sens_impl_17:CNode_40{[0]: ValueNode<FuncGraph> ✗_no_sens_impl_18}
#   9: @_no_sens_impl_17:CNode_48{[0]: ValueNode<Primitive> Return, [1]: CNode_40}


subgraph attr:
training : 1
subgraph instance: ✗_no_sens_impl_18 : 0x6db1ff0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @✗_no_sens_impl_18 parent: [subgraph @_no_sens_impl_17]() {

#------------------------> 4
  %1(CNode_49) = call @↓_no_sens_impl_19()
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:437/        if self.return_grad:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:437/        if self.return_grad:/
}
# Order:
#   1: @✗_no_sens_impl_18:CNode_49{[0]: ValueNode<FuncGraph> ↓_no_sens_impl_19}
#   2: @✗_no_sens_impl_18:CNode_50{[0]: ValueNode<Primitive> Return, [1]: CNode_49}


subgraph attr:
training : 1
subgraph instance: ↓_no_sens_impl_19 : 0x6ddace0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @↓_no_sens_impl_19 parent: [subgraph @_no_sens_impl_17]() {
  %1(CNode_42) = $(_no_sens_impl_17):MakeTuple(%para71_inputs0, %para72_inputs1)
      : (<Tensor[Float32], (32, 1, 32, 32)>, <Tensor[Int32], (32)>) -> (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>)
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/

#------------------------> 5
  %2(loss) = $(_no_sens_impl_17):UnpackCall_unpack_call(@mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_23, %1)
      : (<Func, NoShape>, <Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>) -> (<null>)
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:433/        loss = self.network(*inputs)/
  %3(grads) = $(_no_sens_impl_17):UnpackGraph(@mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_23, %1)
      : (<null>, <Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>) -> (<null>)
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %4(CNode_43) = $(_no_sens_impl_17):MakeTuple(%para3_conv1.weight, %para4_conv2.weight, %para5_conv3.weight, %para6_conv4.weight, %para7_bn1.gamma, %para8_bn1.beta, %para9_bn2.gamma, %para10_bn2.beta, %para11_bn3.gamma, %para12_bn3.beta, %para13_bn4.gamma, %para14_bn4.beta, %para15_fc1.weight, %para16_fc1.bias, %para17_fc2.weight, %para18_fc2.bias, %para19_fc3.weight, %para20_fc3.bias)
      : (<Ref[Tensor[Float32]], (32, 1, 3, 3)>, <Ref[Tensor[Float32]], (32, 1, 5, 5)>, <Ref[Tensor[Float32]], (32, 1, 7, 7)>, <Ref[Tensor[Float32]], (64, 96, 3, 3)>, <Ref[Tensor[Float32]], (32)>, <Ref[Tensor[Float32]], (32)>, <Ref[Tensor[Float32]], (32)>, <Ref[Tensor[Float32]], (32)>, <Ref[Tensor[Float32]], (32)>, <Ref[Tensor[Float32]], (32)>, <Ref[Tensor[Float32]], (64)>, <Ref[Tensor[Float32]], (64)>, <Ref[Tensor[Float32]], (256, 2304)>, <Ref[Tensor[Float32]], (256)>, <Ref[Tensor[Float32]], (128, 256)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (10, 128)>, <Ref[Tensor[Float32]], (10)>) -> (<null>)
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %5(grads) = $(_no_sens_impl_17):S_Prim_grad(%3, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %6(grads) = $(_no_sens_impl_17):UnpackCall_unpack_call(%5, %1)
      : (<null>, <Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>) -> (<null>)
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %7(grads) = $(_no_sens_impl_17):call @mindspore_nn_layer_basic_Identity_construct_45(%6)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:435/        grads = self.grad_reducer(grads)/
  %8(CNode_46) = $(_no_sens_impl_17):call @mindspore_nn_optim_adam_Adam_construct_47(%7)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:436/        loss = F.depend(loss, self.optimizer(grads))/
  %9(loss) = $(_no_sens_impl_17):S_Prim_Depend[side_effect_propagate: I64(1)](%2, %8)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:436/        loss = F.depend(loss, self.optimizer(grads))/
  Return(%9)
      : (<null>)
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:442/        return loss/
}
# Order:
#   1: @↓_no_sens_impl_19:CNode_51{[0]: ValueNode<Primitive> Return, [1]: loss}


subgraph attr:
core : 1
subgraph instance: UnpackCall_20 : 0x6f44ec0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:433/        loss = self.network(*inputs)/
subgraph @UnpackCall_20(%para73_, %para74_) {
  %1(loss) = TupleGetItem(%para74_22, I64(0))
      : (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>, <Int64, NoShape>) -> (<Tensor[Float32], (32, 1, 32, 32)>)
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:433/        loss = self.network(*inputs)/
  %2(loss) = TupleGetItem(%para74_22, I64(1))
      : (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>, <Int64, NoShape>) -> (<Tensor[Int32], (32)>)
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:433/        loss = self.network(*inputs)/

#------------------------> 6
  %3(loss) = %para73_21(%1, %2)
      : (<Tensor[Float32], (32, 1, 32, 32)>, <Tensor[Int32], (32)>) -> (<null>)
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:433/        loss = self.network(*inputs)/
  Return(%3)
      : (<null>)
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:433/        loss = self.network(*inputs)/
}
# Order:
#   1: @UnpackCall_20:loss{[0]: param_21, [1]: loss, [2]: loss}
#   2: @UnpackCall_20:loss{[0]: ValueNode<Primitive> Return, [1]: loss}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_23 : 0x6dbaff0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:120/    def construct(self, data, label):/
subgraph @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_23 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12](%para75_data, %para76_label) {

#------------------------> 7
  %1(out) = call @__main___LeNet5_construct_24(%para75_data)
      : (<Tensor[Float32], (32, 1, 32, 32)>) -> (<null>)
      #scope: (Default/network-WithLossCell)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:121/        out = self._backbone(data)/
  %2(CNode_53) = call @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_52(%1, %para76_label)
      : (<null>, <Tensor[Int32], (32)>) -> (<null>)
      #scope: (Default/network-WithLossCell)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:122/        return self._loss_fn(out, label)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:122/        return self._loss_fn(out, label)/
}
# Order:
#   1: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_23:out{[0]: ValueNode<FuncGraph> __main___LeNet5_construct_24, [1]: param_data}
#   2: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_23:CNode_53{[0]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_52, [1]: out, [2]: param_label}
#   3: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_23:CNode_54{[0]: ValueNode<Primitive> Return, [1]: CNode_53}


subgraph attr:
training : 1
subgraph instance: __main___LeNet5_construct_24 : 0x6dc41f0
# In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:125/    def construct(self, x):/
subgraph @__main___LeNet5_construct_24 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12](%para77_x) {
  %1(CNode_56) = call @mindspore_nn_layer_conv_Conv2d_construct_55(%para77_x)
      : (<Tensor[Float32], (32, 1, 32, 32)>) -> (<Tensor[Float32], (32, 32, 32, 32)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:126/        branch1 = self.max_pool2d(self.relu(self.bn1(self.conv1(x))))/
  %2(CNode_58) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_57(%1)
      : (<Tensor[Float32], (32, 32, 32, 32)>) -> (<Tensor[Float32], (32, 32, 32, 32)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:126/        branch1 = self.max_pool2d(self.relu(self.bn1(self.conv1(x))))/
  %3(CNode_60) = call @mindspore_nn_layer_activation_ReLU_construct_59(%2)
      : (<Tensor[Float32], (32, 32, 32, 32)>) -> (<Tensor[Float32], (32, 32, 32, 32)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:126/        branch1 = self.max_pool2d(self.relu(self.bn1(self.conv1(x))))/
  %4(branch1) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_61(%3)
      : (<Tensor[Float32], (32, 32, 32, 32)>) -> (<Tensor[Float32], (32, 32, 16, 16)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:126/        branch1 = self.max_pool2d(self.relu(self.bn1(self.conv1(x))))/
  %5(CNode_63) = call @mindspore_nn_layer_conv_Conv2d_construct_62(%para77_x)
      : (<Tensor[Float32], (32, 1, 32, 32)>) -> (<Tensor[Float32], (32, 32, 32, 32)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:127/        branch2 = self.max_pool2d(self.relu(self.bn2(self.conv2(x))))/
  %6(CNode_65) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_64(%5)
      : (<Tensor[Float32], (32, 32, 32, 32)>) -> (<Tensor[Float32], (32, 32, 32, 32)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:127/        branch2 = self.max_pool2d(self.relu(self.bn2(self.conv2(x))))/
  %7(CNode_66) = call @mindspore_nn_layer_activation_ReLU_construct_59(%6)
      : (<Tensor[Float32], (32, 32, 32, 32)>) -> (<Tensor[Float32], (32, 32, 32, 32)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:127/        branch2 = self.max_pool2d(self.relu(self.bn2(self.conv2(x))))/
  %8(branch2) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_61(%7)
      : (<Tensor[Float32], (32, 32, 32, 32)>) -> (<Tensor[Float32], (32, 32, 16, 16)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:127/        branch2 = self.max_pool2d(self.relu(self.bn2(self.conv2(x))))/
  %9(CNode_68) = call @mindspore_nn_layer_conv_Conv2d_construct_67(%para77_x)
      : (<Tensor[Float32], (32, 1, 32, 32)>) -> (<Tensor[Float32], (32, 32, 32, 32)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:128/        branch3 = self.max_pool2d(self.relu(self.bn3(self.conv3(x))))/
  %10(CNode_70) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_69(%9)
      : (<Tensor[Float32], (32, 32, 32, 32)>) -> (<Tensor[Float32], (32, 32, 32, 32)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:128/        branch3 = self.max_pool2d(self.relu(self.bn3(self.conv3(x))))/
  %11(CNode_71) = call @mindspore_nn_layer_activation_ReLU_construct_59(%10)
      : (<Tensor[Float32], (32, 32, 32, 32)>) -> (<Tensor[Float32], (32, 32, 32, 32)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:128/        branch3 = self.max_pool2d(self.relu(self.bn3(self.conv3(x))))/
  %12(branch3) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_61(%11)
      : (<Tensor[Float32], (32, 32, 32, 32)>) -> (<Tensor[Float32], (32, 32, 16, 16)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:128/        branch3 = self.max_pool2d(self.relu(self.bn3(self.conv3(x))))/
  %13(CNode_72) = S_Prim_MakeTuple(%4, %8, %12)
      : (<Tensor[Float32], (32, 32, 16, 16)>, <Tensor[Float32], (32, 32, 16, 16)>, <Tensor[Float32], (32, 32, 16, 16)>) -> (<Tuple[Tensor[Float32]*3], TupleShape((32, 32, 16, 16), (32, 32, 16, 16), (32, 32, 16, 16))>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:130/        x = ops.concat((branch1, branch2, branch3), axis=1)/
  %14(CNode_73) = S_Prim_MakeTuple(%13)
      : (<Tuple[Tensor[Float32]*3], TupleShape((32, 32, 16, 16), (32, 32, 16, 16), (32, 32, 16, 16))>) -> (<Tuple[Tuple[Tensor[Float32]*3]], TupleShape(TupleShape((32, 32, 16, 16), (32, 32, 16, 16), (32, 32, 16, 16)))>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:130/        x = ops.concat((branch1, branch2, branch3), axis=1)/
  %15(CNode_74) = S_Prim_MakeTuple("axis")
      : (<String, NoShape>) -> (<Tuple[String], TupleShape(NoShape)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:130/        x = ops.concat((branch1, branch2, branch3), axis=1)/
  %16(CNode_75) = S_Prim_MakeTuple(I64(1))
      : (<Int64, NoShape>) -> (<Tuple[Int64], TupleShape(NoShape)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:130/        x = ops.concat((branch1, branch2, branch3), axis=1)/
  %17(CNode_76) = S_Prim_make_dict(%15, %16)
      : (<Tuple[String], TupleShape(NoShape)>, <Tuple[Int64], TupleShape(NoShape)>) -> (<Dictionary[[axis,],[Int64]], NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:130/        x = ops.concat((branch1, branch2, branch3), axis=1)/
  %18(x) = UnpackCall_unpack_call(@concat_77, %14, %17)
      : (<Func, NoShape>, <Tuple[Tuple[Tensor[Float32]*3]], TupleShape(TupleShape((32, 32, 16, 16), (32, 32, 16, 16), (32, 32, 16, 16)))>, <Dictionary[[axis,],[Int64]], NoShape>) -> (<Tensor[Float32], (32, 96, 16, 16)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:130/        x = ops.concat((branch1, branch2, branch3), axis=1)/
  %19(CNode_79) = call @mindspore_nn_layer_conv_Conv2d_construct_78(%18)
      : (<Tensor[Float32], (32, 96, 16, 16)>) -> (<Tensor[Float32], (32, 64, 14, 14)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:132/        x = self.max_pool2d(self.relu(self.bn4(self.conv4(x))))/
  %20(CNode_81) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_80(%19)
      : (<Tensor[Float32], (32, 64, 14, 14)>) -> (<Tensor[Float32], (32, 64, 14, 14)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:132/        x = self.max_pool2d(self.relu(self.bn4(self.conv4(x))))/
  %21(CNode_82) = call @mindspore_nn_layer_activation_ReLU_construct_59(%20)
      : (<Tensor[Float32], (32, 64, 14, 14)>) -> (<Tensor[Float32], (32, 64, 14, 14)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:132/        x = self.max_pool2d(self.relu(self.bn4(self.conv4(x))))/
  %22(x) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_61(%21)
      : (<Tensor[Float32], (32, 64, 14, 14)>) -> (<Tensor[Float32], (32, 64, 7, 7)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:132/        x = self.max_pool2d(self.relu(self.bn4(self.conv4(x))))/
  %23(x) = call @mindspore_nn_layer_basic_Flatten_construct_83(%22)
      : (<Tensor[Float32], (32, 64, 7, 7)>) -> (<Tensor[Float32], (32, 3136)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:134/        x = self.flatten(x)/

#------------------------> 8
  %24(CNode_84) = call @mindspore_nn_layer_basic_Dense_construct_25(%23)
      : (<Tensor[Float32], (32, 3136)>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:135/        x = self.relu(self.fc1(x))/
  %25(x) = call @mindspore_nn_layer_activation_ReLU_construct_59(%24)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:135/        x = self.relu(self.fc1(x))/
  %26(CNode_86) = call @mindspore_nn_layer_basic_Dense_construct_85(%25)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:136/        x = self.relu(self.fc2(x))/
  %27(x) = call @mindspore_nn_layer_activation_ReLU_construct_59(%26)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:136/        x = self.relu(self.fc2(x))/
  %28(x) = call @mindspore_nn_layer_basic_Dense_construct_87(%27)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:137/        x = self.fc3(x)/
  Return(%28)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:138/        return x/
}
# Order:
#   1: @__main___LeNet5_construct_24:CNode_56{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_55, [1]: param_x}
#   2: @__main___LeNet5_construct_24:CNode_58{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_57, [1]: CNode_56}
#   3: @__main___LeNet5_construct_24:CNode_60{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_59, [1]: CNode_58}
#   4: @__main___LeNet5_construct_24:branch1{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_61, [1]: CNode_60}
#   5: @__main___LeNet5_construct_24:CNode_63{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_62, [1]: param_x}
#   6: @__main___LeNet5_construct_24:CNode_65{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_64, [1]: CNode_63}
#   7: @__main___LeNet5_construct_24:CNode_66{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_59, [1]: CNode_65}
#   8: @__main___LeNet5_construct_24:branch2{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_61, [1]: CNode_66}
#   9: @__main___LeNet5_construct_24:CNode_68{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_67, [1]: param_x}
#  10: @__main___LeNet5_construct_24:CNode_70{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_69, [1]: CNode_68}
#  11: @__main___LeNet5_construct_24:CNode_71{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_59, [1]: CNode_70}
#  12: @__main___LeNet5_construct_24:branch3{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_61, [1]: CNode_71}
#  13: @__main___LeNet5_construct_24:CNode_72{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: branch1, [2]: branch2, [3]: branch3}
#  14: @__main___LeNet5_construct_24:CNode_73{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_72}
#  15: @__main___LeNet5_construct_24:CNode_74{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<StringImm> axis}
#  16: @__main___LeNet5_construct_24:CNode_75{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<Int64Imm> 1}
#  17: @__main___LeNet5_construct_24:CNode_76{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_dict, [1]: CNode_74, [2]: CNode_75}
#  18: @__main___LeNet5_construct_24:x{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.88, [1]: ValueNode<FuncGraph> concat_77, [2]: CNode_73, [3]: CNode_76}
#  19: @__main___LeNet5_construct_24:CNode_79{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_78, [1]: x}
#  20: @__main___LeNet5_construct_24:CNode_81{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_80, [1]: CNode_79}
#  21: @__main___LeNet5_construct_24:CNode_82{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_59, [1]: CNode_81}
#  22: @__main___LeNet5_construct_24:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_61, [1]: CNode_82}
#  23: @__main___LeNet5_construct_24:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Flatten_construct_83, [1]: x}
#  24: @__main___LeNet5_construct_24:CNode_84{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_25, [1]: x}
#  25: @__main___LeNet5_construct_24:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_59, [1]: CNode_84}
#  26: @__main___LeNet5_construct_24:CNode_86{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_85, [1]: x}
#  27: @__main___LeNet5_construct_24:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_59, [1]: CNode_86}
#  28: @__main___LeNet5_construct_24:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_87, [1]: x}
#  29: @__main___LeNet5_construct_24:CNode_89{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct_25 : 0x6e985d0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct_25 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12](%para78_x) {

#------------------------> 9
  %1(CNode_90) = call @L_mindspore_nn_layer_basic_Dense_construct_26(%para78_x, %para16_fc1.bias, %para15_fc1.weight)
      : (<Tensor[Float32], (32, 3136)>, <Ref[Tensor[Float32]], (256)>, <Ref[Tensor[Float32]], (256, 2304)>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc1-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct_25:CNode_90{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_26, [1]: param_x, [2]: param_fc1.bias, [3]: param_fc1.weight}
#   2: @mindspore_nn_layer_basic_Dense_construct_25:CNode_91{[0]: ValueNode<Primitive> Return, [1]: CNode_90}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_26 : 0x6e9a200
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_26(%para79_x, %para80_, %para81_) {
  %1(x_shape) = S_Prim_Shape(%para79_x)
      : (<Tensor[Float32], (32, 3136)>) -> (<Tuple[Int64*2], TupleShape(NoShape, NoShape)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:623/        x_shape = self.shape_op(x)/
  %2(CNode_92) = S_Prim_check_dense_input_shape[constexpr_prim: Bool(1)](%1, "Dense")
      : (<Tuple[Int64*2], TupleShape(NoShape, NoShape)>, <String, NoShape>) -> (<None, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:624/        check_dense_input_shape(x_shape, self.cls_name)/
  %3(CNode_93) = StopGradient(%2)
      : (<None, NoShape>) -> (<None, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
  %4(CNode_94) = S_Prim_inner_len(%1)
      : (<Tuple[Int64*2], TupleShape(NoShape, NoShape)>) -> (<Int64, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
  %5(CNode_95) = S_Prim_not_equal(%4, I64(2))
      : (<Int64, NoShape>, <Int64, NoShape>) -> (<Bool, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
  %6(CNode_96) = Cond(%5, Bool(0))
      : (<Bool, NoShape>, <Bool, NoShape>) -> (<Bool, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
  %7(CNode_97) = Switch(%6, @L_✓mindspore_nn_layer_basic_Dense_construct_98, @L_✗mindspore_nn_layer_basic_Dense_construct_99)
      : (<Bool, NoShape>, <Func, NoShape>, <Func, NoShape>) -> (<Func, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
  %8(CNode_100) = %7()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/

#------------------------> 10
  %9(CNode_101) = call @L_↓mindspore_nn_layer_basic_Dense_construct_27(%8)
      : (<Tensor[Float32], (32, 3136)>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:137/        x = self.fc3(x)/
  %10(CNode_102) = Depend[side_effect_propagate: I64(1)](%9, %3)
      : (<null>, <None, NoShape>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:137/        x = self.fc3(x)/
  Return(%10)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_26:x_shape{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_x}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_26:CNode_92{[0]: ValueNode<DoSignaturePrimitive> S_Prim_check_dense_input_shape, [1]: x_shape, [2]: ValueNode<StringImm> Dense}
#   3: @L_mindspore_nn_layer_basic_Dense_construct_26:CNode_94{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: x_shape}
#   4: @L_mindspore_nn_layer_basic_Dense_construct_26:CNode_95{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: CNode_94, [2]: ValueNode<Int64Imm> 2}
#   5: @L_mindspore_nn_layer_basic_Dense_construct_26:CNode_96{[0]: ValueNode<Primitive> Cond, [1]: CNode_95, [2]: ValueNode<BoolImm> false}
#   6: @L_mindspore_nn_layer_basic_Dense_construct_26:CNode_97{[0]: ValueNode<Primitive> Switch, [1]: CNode_96, [2]: ValueNode<FuncGraph> L_✓mindspore_nn_layer_basic_Dense_construct_98, [3]: ValueNode<FuncGraph> L_✗mindspore_nn_layer_basic_Dense_construct_99}
#   7: @L_mindspore_nn_layer_basic_Dense_construct_26:CNode_100{[0]: CNode_97}
#   8: @L_mindspore_nn_layer_basic_Dense_construct_26:CNode_101{[0]: ValueNode<FuncGraph> L_↓mindspore_nn_layer_basic_Dense_construct_27, [1]: CNode_100}
#   9: @L_mindspore_nn_layer_basic_Dense_construct_26:CNode_102{[0]: ValueNode<Primitive> Depend, [1]: CNode_101, [2]: CNode_93}
#  10: @L_mindspore_nn_layer_basic_Dense_construct_26:CNode_103{[0]: ValueNode<Primitive> Return, [1]: CNode_102}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_↓mindspore_nn_layer_basic_Dense_construct_27 : 0x6ea0ea0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_↓mindspore_nn_layer_basic_Dense_construct_27 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_26](%para82_) {

#------------------------> 11
  %1(CNode_104) = call @L_✓↓mindspore_nn_layer_basic_Dense_construct_28()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:628/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:628/        if self.has_bias:/
}
# Order:
#   1: @L_↓mindspore_nn_layer_basic_Dense_construct_27:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MatMul, [1]: param_фx, [2]: param_L_fc3.weight}
#   2: @L_↓mindspore_nn_layer_basic_Dense_construct_27:CNode_104{[0]: ValueNode<FuncGraph> L_✓↓mindspore_nn_layer_basic_Dense_construct_28}
#   3: @L_↓mindspore_nn_layer_basic_Dense_construct_27:CNode_105{[0]: ValueNode<Primitive> Return, [1]: CNode_104}


subgraph attr:
training : 1
subgraph instance: L_✓↓mindspore_nn_layer_basic_Dense_construct_28 : 0x6ea2b70
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_28 parent: [subgraph @L_↓mindspore_nn_layer_basic_Dense_construct_27]() {

#------------------------> 12
  %1(CNode_106) = call @L_2↓mindspore_nn_layer_basic_Dense_construct_29()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:629/            x = self.bias_add(x, self.bias)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:629/            x = self.bias_add(x, self.bias)/
}
# Order:
#   1: @L_✓↓mindspore_nn_layer_basic_Dense_construct_28:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BiasAdd, [1]: x, [2]: param_L_fc3.bias}
#   2: @L_✓↓mindspore_nn_layer_basic_Dense_construct_28:CNode_106{[0]: ValueNode<FuncGraph> L_2↓mindspore_nn_layer_basic_Dense_construct_29}
#   3: @L_✓↓mindspore_nn_layer_basic_Dense_construct_28:CNode_107{[0]: ValueNode<Primitive> Return, [1]: CNode_106}


subgraph attr:
training : 1
subgraph instance: L_2↓mindspore_nn_layer_basic_Dense_construct_29 : 0x6ea4770
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_2↓mindspore_nn_layer_basic_Dense_construct_29 parent: [subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_28]() {

#------------------------> 13
  %1(CNode_108) = call @L_✗2↓mindspore_nn_layer_basic_Dense_construct_30()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:630/        if self.activation_flag:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:630/        if self.activation_flag:/
}
# Order:
#   1: @L_2↓mindspore_nn_layer_basic_Dense_construct_29:CNode_108{[0]: ValueNode<FuncGraph> L_✗2↓mindspore_nn_layer_basic_Dense_construct_30}
#   2: @L_2↓mindspore_nn_layer_basic_Dense_construct_29:CNode_109{[0]: ValueNode<Primitive> Return, [1]: CNode_108}


subgraph attr:
training : 1
subgraph instance: L_✗2↓mindspore_nn_layer_basic_Dense_construct_30 : 0x6ea5bb0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_✗2↓mindspore_nn_layer_basic_Dense_construct_30 parent: [subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_28]() {

#------------------------> 14
  %1(CNode_110) = call @L_3↓mindspore_nn_layer_basic_Dense_construct_31()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:630/        if self.activation_flag:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:630/        if self.activation_flag:/
}
# Order:
#   1: @L_✗2↓mindspore_nn_layer_basic_Dense_construct_30:CNode_110{[0]: ValueNode<FuncGraph> L_3↓mindspore_nn_layer_basic_Dense_construct_31}
#   2: @L_✗2↓mindspore_nn_layer_basic_Dense_construct_30:CNode_111{[0]: ValueNode<Primitive> Return, [1]: CNode_110}


subgraph attr:
training : 1
subgraph instance: L_3↓mindspore_nn_layer_basic_Dense_construct_31 : 0x6ea6ff0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_3↓mindspore_nn_layer_basic_Dense_construct_31 parent: [subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_28]() {
  %1(x_shape) = $(L_mindspore_nn_layer_basic_Dense_construct_26):S_Prim_Shape(%para79_x)
      : (<Tensor[Float32], (32, 3136)>) -> (<Tuple[Int64*2], TupleShape(NoShape, NoShape)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:623/        x_shape = self.shape_op(x)/
  %2(CNode_112) = S_Prim_inner_len(%1)
      : (<Tuple[Int64*2], TupleShape(NoShape, NoShape)>) -> (<Int64, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
  %3(CNode_113) = S_Prim_not_equal(%2, I64(2))
      : (<Int64, NoShape>, <Int64, NoShape>) -> (<Bool, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
  %4(CNode_114) = Cond(%3, Bool(0))
      : (<Bool, NoShape>, <Bool, NoShape>) -> (<Bool, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
  %5(CNode_115) = Switch(%4, @L_✓3↓mindspore_nn_layer_basic_Dense_construct_116, @L_✗3↓mindspore_nn_layer_basic_Dense_construct_32)
      : (<Bool, NoShape>, <Func, NoShape>, <Func, NoShape>) -> (<Func, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/

#------------------------> 15
  %6(CNode_117) = %5()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
  %7(CNode_119) = call @L_4↓mindspore_nn_layer_basic_Dense_construct_118(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:137/        x = self.fc3(x)/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_3↓mindspore_nn_layer_basic_Dense_construct_31:CNode_112{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: x_shape}
#   2: @L_3↓mindspore_nn_layer_basic_Dense_construct_31:CNode_113{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: CNode_112, [2]: ValueNode<Int64Imm> 2}
#   3: @L_3↓mindspore_nn_layer_basic_Dense_construct_31:CNode_114{[0]: ValueNode<Primitive> Cond, [1]: CNode_113, [2]: ValueNode<BoolImm> false}
#   4: @L_3↓mindspore_nn_layer_basic_Dense_construct_31:CNode_115{[0]: ValueNode<Primitive> Switch, [1]: CNode_114, [2]: ValueNode<FuncGraph> L_✓3↓mindspore_nn_layer_basic_Dense_construct_116, [3]: ValueNode<FuncGraph> L_✗3↓mindspore_nn_layer_basic_Dense_construct_32}
#   5: @L_3↓mindspore_nn_layer_basic_Dense_construct_31:CNode_117{[0]: CNode_115}
#   6: @L_3↓mindspore_nn_layer_basic_Dense_construct_31:CNode_119{[0]: ValueNode<FuncGraph> L_4↓mindspore_nn_layer_basic_Dense_construct_118, [1]: CNode_117}
#   7: @L_3↓mindspore_nn_layer_basic_Dense_construct_31:CNode_120{[0]: ValueNode<Primitive> Return, [1]: CNode_119}


subgraph attr:
training : 1
subgraph instance: L_✗3↓mindspore_nn_layer_basic_Dense_construct_32 : 0x6ea9340
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_✗3↓mindspore_nn_layer_basic_Dense_construct_32 parent: [subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_28]() {

#------------------------> 16
  %1(x) = $(L_↓mindspore_nn_layer_basic_Dense_construct_27):S_Prim_MatMul[output_names: ["output"], transpose_a: Bool(0), input_names: ["x1", "x2"], transpose_x2: Bool(1), transpose_x1: Bool(0), transpose_b: Bool(1)](%para82_фx, %para81_L_fc3.weight)
      : (<Tensor[Float32], (32, 3136)>, <Ref[Tensor[Float32]], (256, 2304)>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:627/        x = self.matmul(x, self.weight)/
  %2(x) = $(L_✓↓mindspore_nn_layer_basic_Dense_construct_28):S_Prim_BiasAdd[output_names: ["output"], format: "NCHW", input_names: ["x", "b"]](%1, %para80_L_fc3.bias)
      : (<null>, <Ref[Tensor[Float32]], (256)>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:629/            x = self.bias_add(x, self.bias)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_✗3↓mindspore_nn_layer_basic_Dense_construct_32:CNode_121{[0]: ValueNode<Primitive> Return, [1]: x}


# ===============================================================================================
# The total of function graphs in evaluation stack: 17/18 (Ignored 1 internal frames).
# ===============================================================================================


# ===============================================================================================
# The rest function graphs are the following:
# ===============================================================================================
subgraph attr:
training : 1
subgraph instance: _no_sens_impl_37 : 0x6c155b0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @_no_sens_impl_37 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12](%para83_inputs) {
  %1(CNode_40) = call @✗_no_sens_impl_122()
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:437/        if self.return_grad:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:437/        if self.return_grad:/
}
# Order:
#   1: @_no_sens_impl_37:loss{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.41, [1]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_123, [2]: param_inputs}
#   2: @_no_sens_impl_37:grads{[0]: ValueNode<UnpackGraphPrimitive> UnpackGraph, [1]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_123, [2]: param_inputs}
#   3: @_no_sens_impl_37:grads{[0]: ValueNode<DoSignaturePrimitive> S_Prim_grad, [1]: grads, [2]: CNode_43}
#   4: @_no_sens_impl_37:grads{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.44, [1]: grads, [2]: param_inputs}
#   5: @_no_sens_impl_37:grads{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Identity_construct_124, [1]: grads}
#   6: @_no_sens_impl_37:CNode_46{[0]: ValueNode<FuncGraph> mindspore_nn_optim_adam_Adam_construct_125, [1]: grads}
#   7: @_no_sens_impl_37:loss{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Depend, [1]: loss, [2]: CNode_46}
#   8: @_no_sens_impl_37:CNode_40{[0]: ValueNode<FuncGraph> ✗_no_sens_impl_122}
#   9: @_no_sens_impl_37:CNode_48{[0]: ValueNode<Primitive> Return, [1]: CNode_40}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: mindspore_nn_optim_adam_Adam_construct_125 : 0x6c44370
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:910/    def construct(self, gradients):/
subgraph @mindspore_nn_optim_adam_Adam_construct_125 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12](%para84_gradients) {
  %1(CNode_127) = call @✓mindspore_nn_optim_adam_Adam_construct_126()
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:916/        if not self.use_offload:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:916/        if not self.use_offload:/
}
# Order:
#   1: @mindspore_nn_optim_adam_Adam_construct_125:gradients{[0]: ValueNode<FuncGraph> flatten_gradients_128, [1]: param_gradients}
#   2: @mindspore_nn_optim_adam_Adam_construct_125:gradients{[0]: ValueNode<FuncGraph> decay_weight_129, [1]: gradients}
#   3: @mindspore_nn_optim_adam_Adam_construct_125:CNode_127{[0]: ValueNode<FuncGraph> ✓mindspore_nn_optim_adam_Adam_construct_126}
#   4: @mindspore_nn_optim_adam_Adam_construct_125:CNode_130{[0]: ValueNode<Primitive> Return, [1]: CNode_127}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Identity_construct_124 : 0x69de6e0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:505/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Identity_construct_124(%para85_x) {
  Return(%para85_x)
      : (<null>)
      #scope: (Default/grad_reducer-Identity)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:506/        return x/
}
# Order:
#   1: @mindspore_nn_layer_basic_Identity_construct_124:CNode_131{[0]: ValueNode<Primitive> Return, [1]: param_x}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_123 : 0x68d8180
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:120/    def construct(self, data, label):/
subgraph @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_123 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12](%para86_data, %para87_label) {
  %1(out) = call @__main___LeNet5_construct_132(%para86_data)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:121/        out = self._backbone(data)/
  %2(CNode_53) = call @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_133(%1, %para87_label)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:122/        return self._loss_fn(out, label)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:122/        return self._loss_fn(out, label)/
}
# Order:
#   1: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_123:out{[0]: ValueNode<FuncGraph> __main___LeNet5_construct_132, [1]: param_data}
#   2: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_123:CNode_53{[0]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_133, [1]: out, [2]: param_label}
#   3: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_123:CNode_54{[0]: ValueNode<Primitive> Return, [1]: CNode_53}


subgraph attr:
training : 1
subgraph instance: ✗_no_sens_impl_122 : 0x6d976c0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @✗_no_sens_impl_122 parent: [subgraph @_no_sens_impl_37]() {
  %1(CNode_49) = call @↓_no_sens_impl_134()
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:437/        if self.return_grad:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:437/        if self.return_grad:/
}
# Order:
#   1: @✗_no_sens_impl_122:CNode_49{[0]: ValueNode<FuncGraph> ↓_no_sens_impl_134}
#   2: @✗_no_sens_impl_122:CNode_50{[0]: ValueNode<Primitive> Return, [1]: CNode_49}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: decay_weight_129 : 0x6c39130
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:429/    def decay_weight(self, gradients):/
subgraph @decay_weight_129(%para88_gradients) {
  %1(CNode_136) = call @✗decay_weight_135()
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:442/        if self.exec_weight_decay:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:442/        if self.exec_weight_decay:/
}
# Order:
#   1: @decay_weight_129:CNode_136{[0]: ValueNode<FuncGraph> ✗decay_weight_135}
#   2: @decay_weight_129:CNode_137{[0]: ValueNode<Primitive> Return, [1]: CNode_136}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: flatten_gradients_128 : 0x6b7d6f0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:410/    def flatten_gradients(self, gradients):/
subgraph @flatten_gradients_128(%para89_gradients) {
  %1(CNode_139) = call @✗flatten_gradients_138()
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:424/        if self._use_flattened_params:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:424/        if self._use_flattened_params:/
}
# Order:
#   1: @flatten_gradients_128:CNode_139{[0]: ValueNode<FuncGraph> ✗flatten_gradients_138}
#   2: @flatten_gradients_128:CNode_140{[0]: ValueNode<Primitive> Return, [1]: CNode_139}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ✓mindspore_nn_optim_adam_Adam_construct_126 : 0x6c14480
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:910/    def construct(self, gradients):/
subgraph @✓mindspore_nn_optim_adam_Adam_construct_126 parent: [subgraph @mindspore_nn_optim_adam_Adam_construct_125]() {
  %1(CNode_142) = call @↓mindspore_nn_optim_adam_Adam_construct_141()
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:917/            gradients = self.gradients_centralization(gradients)/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:917/            gradients = self.gradients_centralization(gradients)/
}
# Order:
#   1: @✓mindspore_nn_optim_adam_Adam_construct_126:gradients{[0]: ValueNode<FuncGraph> gradients_centralization_143, [1]: gradients}
#   2: @✓mindspore_nn_optim_adam_Adam_construct_126:CNode_142{[0]: ValueNode<FuncGraph> ↓mindspore_nn_optim_adam_Adam_construct_141}
#   3: @✓mindspore_nn_optim_adam_Adam_construct_126:CNode_144{[0]: ValueNode<Primitive> Return, [1]: CNode_142}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_133 : 0x6d81130
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_133(%para90_logits, %para91_labels) {
  %1(CNode_145) = S_Prim__check_is_tensor[constexpr_prim: Bool(1)]("logits", %para90_logits, "SoftmaxCrossEntropyWithLogits")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:778/        _check_is_tensor('logits', logits, self.cls_name)/
  %2(CNode_146) = S_Prim__check_is_tensor[constexpr_prim: Bool(1)]("labels", %para91_labels, "SoftmaxCrossEntropyWithLogits")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:779/        _check_is_tensor('labels', labels, self.cls_name)/
  %3(CNode_147) = MakeTuple(%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:777/    def construct(self, logits, labels):/
  %4(CNode_148) = StopGradient(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:777/    def construct(self, logits, labels):/
  %5(CNode_150) = call @✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_149()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:780/        if self.sparse:/
  %6(CNode_151) = Depend[side_effect_propagate: I64(1)](%5, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:780/        if self.sparse:/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:780/        if self.sparse:/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_133:CNode_145{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_is_tensor, [1]: ValueNode<StringImm> logits, [2]: param_logits, [3]: ValueNode<StringImm> SoftmaxCrossEntropyWithLogits}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_133:CNode_146{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_is_tensor, [1]: ValueNode<StringImm> labels, [2]: param_labels, [3]: ValueNode<StringImm> SoftmaxCrossEntropyWithLogits}
#   3: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_133:CNode_150{[0]: ValueNode<FuncGraph> ✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_149}
#   4: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_133:CNode_152{[0]: ValueNode<Primitive> Return, [1]: CNode_151}


subgraph attr:
training : 1
subgraph instance: __main___LeNet5_construct_132 : 0x68fd500
# In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:125/    def construct(self, x):/
subgraph @__main___LeNet5_construct_132 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12](%para92_x) {
  %1(CNode_56) = call @mindspore_nn_layer_conv_Conv2d_construct_153(%para92_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:126/        branch1 = self.max_pool2d(self.relu(self.bn1(self.conv1(x))))/
  %2(CNode_58) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_154(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:126/        branch1 = self.max_pool2d(self.relu(self.bn1(self.conv1(x))))/
  %3(CNode_60) = call @mindspore_nn_layer_activation_ReLU_construct_155(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:126/        branch1 = self.max_pool2d(self.relu(self.bn1(self.conv1(x))))/
  %4(branch1) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_156(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:126/        branch1 = self.max_pool2d(self.relu(self.bn1(self.conv1(x))))/
  %5(CNode_63) = call @mindspore_nn_layer_conv_Conv2d_construct_157(%para92_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:127/        branch2 = self.max_pool2d(self.relu(self.bn2(self.conv2(x))))/
  %6(CNode_65) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_158(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:127/        branch2 = self.max_pool2d(self.relu(self.bn2(self.conv2(x))))/
  %7(CNode_66) = call @mindspore_nn_layer_activation_ReLU_construct_155(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:127/        branch2 = self.max_pool2d(self.relu(self.bn2(self.conv2(x))))/
  %8(branch2) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_156(%7)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:127/        branch2 = self.max_pool2d(self.relu(self.bn2(self.conv2(x))))/
  %9(CNode_68) = call @mindspore_nn_layer_conv_Conv2d_construct_159(%para92_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:128/        branch3 = self.max_pool2d(self.relu(self.bn3(self.conv3(x))))/
  %10(CNode_70) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_160(%9)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:128/        branch3 = self.max_pool2d(self.relu(self.bn3(self.conv3(x))))/
  %11(CNode_71) = call @mindspore_nn_layer_activation_ReLU_construct_155(%10)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:128/        branch3 = self.max_pool2d(self.relu(self.bn3(self.conv3(x))))/
  %12(branch3) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_156(%11)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:128/        branch3 = self.max_pool2d(self.relu(self.bn3(self.conv3(x))))/
  %13(CNode_72) = S_Prim_MakeTuple(%4, %8, %12)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:130/        x = ops.concat((branch1, branch2, branch3), axis=1)/
  %14(CNode_73) = S_Prim_MakeTuple(%13)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:130/        x = ops.concat((branch1, branch2, branch3), axis=1)/
  %15(CNode_74) = S_Prim_MakeTuple("axis")
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:130/        x = ops.concat((branch1, branch2, branch3), axis=1)/
  %16(CNode_75) = S_Prim_MakeTuple(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:130/        x = ops.concat((branch1, branch2, branch3), axis=1)/
  %17(CNode_76) = S_Prim_make_dict(%15, %16)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:130/        x = ops.concat((branch1, branch2, branch3), axis=1)/
  %18(x) = UnpackCall_unpack_call(@concat_161, %14, %17)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:130/        x = ops.concat((branch1, branch2, branch3), axis=1)/
  %19(CNode_79) = call @mindspore_nn_layer_conv_Conv2d_construct_162(%18)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:132/        x = self.max_pool2d(self.relu(self.bn4(self.conv4(x))))/
  %20(CNode_81) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_163(%19)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:132/        x = self.max_pool2d(self.relu(self.bn4(self.conv4(x))))/
  %21(CNode_82) = call @mindspore_nn_layer_activation_ReLU_construct_155(%20)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:132/        x = self.max_pool2d(self.relu(self.bn4(self.conv4(x))))/
  %22(x) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_156(%21)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:132/        x = self.max_pool2d(self.relu(self.bn4(self.conv4(x))))/
  %23(x) = call @mindspore_nn_layer_basic_Flatten_construct_164(%22)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:134/        x = self.flatten(x)/
  %24(CNode_84) = call @mindspore_nn_layer_basic_Dense_construct_165(%23)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:135/        x = self.relu(self.fc1(x))/
  %25(x) = call @mindspore_nn_layer_activation_ReLU_construct_155(%24)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:135/        x = self.relu(self.fc1(x))/
  %26(CNode_86) = call @mindspore_nn_layer_basic_Dense_construct_166(%25)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:136/        x = self.relu(self.fc2(x))/
  %27(x) = call @mindspore_nn_layer_activation_ReLU_construct_155(%26)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:136/        x = self.relu(self.fc2(x))/
  %28(x) = call @mindspore_nn_layer_basic_Dense_construct_167(%27)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:137/        x = self.fc3(x)/
  Return(%28)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:138/        return x/
}
# Order:
#   1: @__main___LeNet5_construct_132:CNode_56{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_153, [1]: param_x}
#   2: @__main___LeNet5_construct_132:CNode_58{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_154, [1]: CNode_56}
#   3: @__main___LeNet5_construct_132:CNode_60{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_155, [1]: CNode_58}
#   4: @__main___LeNet5_construct_132:branch1{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_156, [1]: CNode_60}
#   5: @__main___LeNet5_construct_132:CNode_63{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_157, [1]: param_x}
#   6: @__main___LeNet5_construct_132:CNode_65{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_158, [1]: CNode_63}
#   7: @__main___LeNet5_construct_132:CNode_66{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_155, [1]: CNode_65}
#   8: @__main___LeNet5_construct_132:branch2{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_156, [1]: CNode_66}
#   9: @__main___LeNet5_construct_132:CNode_68{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_159, [1]: param_x}
#  10: @__main___LeNet5_construct_132:CNode_70{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_160, [1]: CNode_68}
#  11: @__main___LeNet5_construct_132:CNode_71{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_155, [1]: CNode_70}
#  12: @__main___LeNet5_construct_132:branch3{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_156, [1]: CNode_71}
#  13: @__main___LeNet5_construct_132:CNode_72{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: branch1, [2]: branch2, [3]: branch3}
#  14: @__main___LeNet5_construct_132:CNode_73{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_72}
#  15: @__main___LeNet5_construct_132:CNode_74{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<StringImm> axis}
#  16: @__main___LeNet5_construct_132:CNode_75{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<Int64Imm> 1}
#  17: @__main___LeNet5_construct_132:CNode_76{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_dict, [1]: CNode_74, [2]: CNode_75}
#  18: @__main___LeNet5_construct_132:x{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.88, [1]: ValueNode<FuncGraph> concat_161, [2]: CNode_73, [3]: CNode_76}
#  19: @__main___LeNet5_construct_132:CNode_79{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_162, [1]: x}
#  20: @__main___LeNet5_construct_132:CNode_81{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_163, [1]: CNode_79}
#  21: @__main___LeNet5_construct_132:CNode_82{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_155, [1]: CNode_81}
#  22: @__main___LeNet5_construct_132:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_156, [1]: CNode_82}
#  23: @__main___LeNet5_construct_132:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Flatten_construct_164, [1]: x}
#  24: @__main___LeNet5_construct_132:CNode_84{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_165, [1]: x}
#  25: @__main___LeNet5_construct_132:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_155, [1]: CNode_84}
#  26: @__main___LeNet5_construct_132:CNode_86{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_166, [1]: x}
#  27: @__main___LeNet5_construct_132:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_155, [1]: CNode_86}
#  28: @__main___LeNet5_construct_132:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_167, [1]: x}
#  29: @__main___LeNet5_construct_132:CNode_89{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: ↓_no_sens_impl_134 : 0x6d98a70
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @↓_no_sens_impl_134 parent: [subgraph @_no_sens_impl_37]() {
  %1(loss) = $(_no_sens_impl_37):UnpackCall_unpack_call(@mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_123, %para83_inputs)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:433/        loss = self.network(*inputs)/
  %2(grads) = $(_no_sens_impl_37):UnpackGraph(@mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_123, %para83_inputs)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %3(CNode_43) = $(_no_sens_impl_37):MakeTuple(%para3_conv1.weight, %para4_conv2.weight, %para5_conv3.weight, %para6_conv4.weight, %para7_bn1.gamma, %para8_bn1.beta, %para9_bn2.gamma, %para10_bn2.beta, %para11_bn3.gamma, %para12_bn3.beta, %para13_bn4.gamma, %para14_bn4.beta, %para15_fc1.weight, %para16_fc1.bias, %para17_fc2.weight, %para18_fc2.bias, %para19_fc3.weight, %para20_fc3.bias)
      : (<Ref[Tensor[Float32]], (32, 1, 3, 3), ref_key=:conv1.weight>, <Ref[Tensor[Float32]], (32, 1, 5, 5), ref_key=:conv2.weight>, <Ref[Tensor[Float32]], (32, 1, 7, 7), ref_key=:conv3.weight>, <Ref[Tensor[Float32]], (64, 96, 3, 3), ref_key=:conv4.weight>, <Ref[Tensor[Float32]], (32), ref_key=:bn1.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:bn1.beta>, <Ref[Tensor[Float32]], (32), ref_key=:bn2.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:bn2.beta>, <Ref[Tensor[Float32]], (32), ref_key=:bn3.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:bn3.beta>, <Ref[Tensor[Float32]], (64), ref_key=:bn4.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:bn4.beta>, <Ref[Tensor[Float32]], (256, 2304), ref_key=:fc1.weight>, <Ref[Tensor[Float32]], (256), ref_key=:fc1.bias>, <Ref[Tensor[Float32]], (128, 256), ref_key=:fc2.weight>, <Ref[Tensor[Float32]], (128), ref_key=:fc2.bias>, <Ref[Tensor[Float32]], (10, 128), ref_key=:fc3.weight>, <Ref[Tensor[Float32]], (10), ref_key=:fc3.bias>) -> (<null>)
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %4(grads) = $(_no_sens_impl_37):S_Prim_grad(%2, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %5(grads) = $(_no_sens_impl_37):UnpackCall_unpack_call(%4, %para83_inputs)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %6(grads) = $(_no_sens_impl_37):call @mindspore_nn_layer_basic_Identity_construct_124(%5)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:435/        grads = self.grad_reducer(grads)/
  %7(CNode_46) = $(_no_sens_impl_37):call @mindspore_nn_optim_adam_Adam_construct_125(%6)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:436/        loss = F.depend(loss, self.optimizer(grads))/
  %8(loss) = $(_no_sens_impl_37):S_Prim_Depend[side_effect_propagate: I64(1)](%1, %7)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:436/        loss = F.depend(loss, self.optimizer(grads))/
  Return(%8)
      : (<null>)
      #scope: (Default)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:442/        return loss/
}
# Order:
#   1: @↓_no_sens_impl_134:CNode_51{[0]: ValueNode<Primitive> Return, [1]: loss}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ✗decay_weight_135 : 0x6b76ae0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:429/    def decay_weight(self, gradients):/
subgraph @✗decay_weight_135 parent: [subgraph @decay_weight_129]() {
  %1(CNode_169) = call @↓decay_weight_168()
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:442/        if self.exec_weight_decay:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:442/        if self.exec_weight_decay:/
}
# Order:
#   1: @✗decay_weight_135:CNode_169{[0]: ValueNode<FuncGraph> ↓decay_weight_168}
#   2: @✗decay_weight_135:CNode_170{[0]: ValueNode<Primitive> Return, [1]: CNode_169}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ✗flatten_gradients_138 : 0x69f1660
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:410/    def flatten_gradients(self, gradients):/
subgraph @✗flatten_gradients_138 parent: [subgraph @flatten_gradients_128]() {
  %1(CNode_172) = call @↓flatten_gradients_171()
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:424/        if self._use_flattened_params:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:424/        if self._use_flattened_params:/
}
# Order:
#   1: @✗flatten_gradients_138:CNode_172{[0]: ValueNode<FuncGraph> ↓flatten_gradients_171}
#   2: @✗flatten_gradients_138:CNode_173{[0]: ValueNode<Primitive> Return, [1]: CNode_172}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: gradients_centralization_143 : 0x6b17970
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:452/    def gradients_centralization(self, gradients):/
subgraph @gradients_centralization_143(%para93_gradients) {
  %1(CNode_175) = call @✗gradients_centralization_174()
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:466/        if self.is_group:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:466/        if self.is_group:/
}
# Order:
#   1: @gradients_centralization_143:CNode_175{[0]: ValueNode<FuncGraph> ✗gradients_centralization_174}
#   2: @gradients_centralization_143:CNode_176{[0]: ValueNode<Primitive> Return, [1]: CNode_175}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ↓mindspore_nn_optim_adam_Adam_construct_141 : 0x69e8f50
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:910/    def construct(self, gradients):/
subgraph @↓mindspore_nn_optim_adam_Adam_construct_141 parent: [subgraph @✓mindspore_nn_optim_adam_Adam_construct_126]() {
  %1(CNode_177) = S_Prim_AssignAdd[output_names: ["ref"], side_effect_mem: Bool(1), input_names: ["ref", "value"]](%para59_global_step, Tensor(shape=[1], dtype=Int32, value=[1]))
      : (<Ref[Tensor[Int32]], (1), ref_key=:global_step>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:921/        self.assignadd(self.global_step, self.global_step_increase_tensor)/
  %2(beta1_power) = S_Prim_mul(%para57_beta1_power, Tensor(shape=[], dtype=Float32, value=0.9))
      : (<Ref[Tensor[Float32]], (), ref_key=:beta1_power>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:923/        beta1_power = self.beta1_power * self.beta1/
  %3(CNode_179) = call @assign_178(%para57_beta1_power, %2)
      : (<Ref[Tensor[Float32]], (), ref_key=:beta1_power>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:924/        self.beta1_power = beta1_power/
  %4(beta2_power) = S_Prim_mul(%para58_beta2_power, Tensor(shape=[], dtype=Float32, value=0.999))
      : (<Ref[Tensor[Float32]], (), ref_key=:beta2_power>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:925/        beta2_power = self.beta2_power * self.beta2/
  %5(CNode_180) = call @assign_178(%para58_beta2_power, %4)
      : (<Ref[Tensor[Float32]], (), ref_key=:beta2_power>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:926/        self.beta2_power = beta2_power/
  %6(CNode_181) = MakeTuple(%1, %3, %5)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:909/    @jit/
  %7(CNode_182) = StopGradient(%6)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:909/    @jit/
  %8(CNode_183) = $(mindspore_nn_optim_adam_Adam_construct_125):MakeTuple(%para3_conv1.weight, %para4_conv2.weight, %para5_conv3.weight, %para6_conv4.weight, %para7_bn1.gamma, %para8_bn1.beta, %para9_bn2.gamma, %para10_bn2.beta, %para11_bn3.gamma, %para12_bn3.beta, %para13_bn4.gamma, %para14_bn4.beta, %para15_fc1.weight, %para16_fc1.bias, %para17_fc2.weight, %para18_fc2.bias, %para19_fc3.weight, %para20_fc3.bias)
      : (<Ref[Tensor[Float32]], (32, 1, 3, 3), ref_key=:conv1.weight>, <Ref[Tensor[Float32]], (32, 1, 5, 5), ref_key=:conv2.weight>, <Ref[Tensor[Float32]], (32, 1, 7, 7), ref_key=:conv3.weight>, <Ref[Tensor[Float32]], (64, 96, 3, 3), ref_key=:conv4.weight>, <Ref[Tensor[Float32]], (32), ref_key=:bn1.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:bn1.beta>, <Ref[Tensor[Float32]], (32), ref_key=:bn2.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:bn2.beta>, <Ref[Tensor[Float32]], (32), ref_key=:bn3.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:bn3.beta>, <Ref[Tensor[Float32]], (64), ref_key=:bn4.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:bn4.beta>, <Ref[Tensor[Float32]], (256, 2304), ref_key=:fc1.weight>, <Ref[Tensor[Float32]], (256), ref_key=:fc1.bias>, <Ref[Tensor[Float32]], (128, 256), ref_key=:fc2.weight>, <Ref[Tensor[Float32]], (128), ref_key=:fc2.bias>, <Ref[Tensor[Float32]], (10, 128), ref_key=:fc3.weight>, <Ref[Tensor[Float32]], (10), ref_key=:fc3.bias>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:911/        params = self._parameters/
  %9(CNode_184) = $(mindspore_nn_optim_adam_Adam_construct_125):MakeTuple(%para21_moment1.conv1.weight, %para22_moment1.conv2.weight, %para23_moment1.conv3.weight, %para24_moment1.conv4.weight, %para25_moment1.bn1.gamma, %para26_moment1.bn1.beta, %para27_moment1.bn2.gamma, %para28_moment1.bn2.beta, %para29_moment1.bn3.gamma, %para30_moment1.bn3.beta, %para31_moment1.bn4.gamma, %para32_moment1.bn4.beta, %para33_moment1.fc1.weight, %para34_moment1.fc1.bias, %para35_moment1.fc2.weight, %para36_moment1.fc2.bias, %para37_moment1.fc3.weight, %para38_moment1.fc3.bias)
      : (<Ref[Tensor[Float32]], (32, 1, 3, 3), ref_key=:moment1.conv1.weight>, <Ref[Tensor[Float32]], (32, 1, 5, 5), ref_key=:moment1.conv2.weight>, <Ref[Tensor[Float32]], (32, 1, 7, 7), ref_key=:moment1.conv3.weight>, <Ref[Tensor[Float32]], (64, 96, 3, 3), ref_key=:moment1.conv4.weight>, <Ref[Tensor[Float32]], (32), ref_key=:moment1.bn1.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:moment1.bn1.beta>, <Ref[Tensor[Float32]], (32), ref_key=:moment1.bn2.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:moment1.bn2.beta>, <Ref[Tensor[Float32]], (32), ref_key=:moment1.bn3.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:moment1.bn3.beta>, <Ref[Tensor[Float32]], (64), ref_key=:moment1.bn4.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:moment1.bn4.beta>, <Ref[Tensor[Float32]], (256, 2304), ref_key=:moment1.fc1.weight>, <Ref[Tensor[Float32]], (256), ref_key=:moment1.fc1.bias>, <Ref[Tensor[Float32]], (128, 256), ref_key=:moment1.fc2.weight>, <Ref[Tensor[Float32]], (128), ref_key=:moment1.fc2.bias>, <Ref[Tensor[Float32]], (10, 128), ref_key=:moment1.fc3.weight>, <Ref[Tensor[Float32]], (10), ref_key=:moment1.fc3.bias>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:912/        moment1 = self.moment1/
  %10(CNode_185) = $(mindspore_nn_optim_adam_Adam_construct_125):MakeTuple(%para39_moment2.conv1.weight, %para40_moment2.conv2.weight, %para41_moment2.conv3.weight, %para42_moment2.conv4.weight, %para43_moment2.bn1.gamma, %para44_moment2.bn1.beta, %para45_moment2.bn2.gamma, %para46_moment2.bn2.beta, %para47_moment2.bn3.gamma, %para48_moment2.bn3.beta, %para49_moment2.bn4.gamma, %para50_moment2.bn4.beta, %para51_moment2.fc1.weight, %para52_moment2.fc1.bias, %para53_moment2.fc2.weight, %para54_moment2.fc2.bias, %para55_moment2.fc3.weight, %para56_moment2.fc3.bias)
      : (<Ref[Tensor[Float32]], (32, 1, 3, 3), ref_key=:moment2.conv1.weight>, <Ref[Tensor[Float32]], (32, 1, 5, 5), ref_key=:moment2.conv2.weight>, <Ref[Tensor[Float32]], (32, 1, 7, 7), ref_key=:moment2.conv3.weight>, <Ref[Tensor[Float32]], (64, 96, 3, 3), ref_key=:moment2.conv4.weight>, <Ref[Tensor[Float32]], (32), ref_key=:moment2.bn1.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:moment2.bn1.beta>, <Ref[Tensor[Float32]], (32), ref_key=:moment2.bn2.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:moment2.bn2.beta>, <Ref[Tensor[Float32]], (32), ref_key=:moment2.bn3.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:moment2.bn3.beta>, <Ref[Tensor[Float32]], (64), ref_key=:moment2.bn4.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:moment2.bn4.beta>, <Ref[Tensor[Float32]], (256, 2304), ref_key=:moment2.fc1.weight>, <Ref[Tensor[Float32]], (256), ref_key=:moment2.fc1.bias>, <Ref[Tensor[Float32]], (128, 256), ref_key=:moment2.fc2.weight>, <Ref[Tensor[Float32]], (128), ref_key=:moment2.fc2.bias>, <Ref[Tensor[Float32]], (10, 128), ref_key=:moment2.fc3.weight>, <Ref[Tensor[Float32]], (10), ref_key=:moment2.fc3.bias>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:913/        moment2 = self.moment2/
  %11(lr) = call @get_lr_186()
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:920/        lr = self.get_lr()/
  %12(gradients) = $(mindspore_nn_optim_adam_Adam_construct_125):call @flatten_gradients_128(%para84_gradients)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:914/        gradients = self.flatten_gradients(gradients)/
  %13(gradients) = $(mindspore_nn_optim_adam_Adam_construct_125):call @decay_weight_129(%12)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:915/        gradients = self.decay_weight(gradients)/
  %14(gradients) = $(✓mindspore_nn_optim_adam_Adam_construct_126):call @gradients_centralization_143(%13)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:917/            gradients = self.gradients_centralization(gradients)/
  %15(gradients) = call @scale_grad_187(%14)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:918/        gradients = self.scale_grad(gradients)/
  %16(gradients) = call @_grad_sparse_indices_deduplicate_188(%15)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:919/        gradients = self._grad_sparse_indices_deduplicate(gradients)/
  %17(CNode_190) = call @_apply_adam_189(%8, %2, %4, %9, %10, %11, %16)
      : (<null>, <null>, <null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:928/        return self._apply_adam(params, beta1_power, beta2_power, moment1, moment2, lr, gradients)/
  %18(CNode_191) = Depend[side_effect_propagate: I64(1)](%17, %7)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:928/        return self._apply_adam(params, beta1_power, beta2_power, moment1, moment2, lr, gradients)/
  Return(%18)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:928/        return self._apply_adam(params, beta1_power, beta2_power, moment1, moment2, lr, gradients)/
}
# Order:
#   1: @↓mindspore_nn_optim_adam_Adam_construct_141:gradients{[0]: ValueNode<FuncGraph> scale_grad_187, [1]: gradients}
#   2: @↓mindspore_nn_optim_adam_Adam_construct_141:gradients{[0]: ValueNode<FuncGraph> _grad_sparse_indices_deduplicate_188, [1]: gradients}
#   3: @↓mindspore_nn_optim_adam_Adam_construct_141:lr{[0]: ValueNode<FuncGraph> get_lr_186}
#   4: @↓mindspore_nn_optim_adam_Adam_construct_141:CNode_177{[0]: ValueNode<DoSignaturePrimitive> S_Prim_AssignAdd, [1]: param_global_step, [2]: ValueNode<Tensor> Tensor(shape=[1], dtype=Int32, value=[1])}
#   5: @↓mindspore_nn_optim_adam_Adam_construct_141:beta1_power{[0]: ValueNode<DoSignaturePrimitive> S_Prim_mul, [1]: param_beta1_power, [2]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=0.9)}
#   6: @↓mindspore_nn_optim_adam_Adam_construct_141:CNode_179{[0]: ValueNode<FuncGraph> assign_178, [1]: param_beta1_power, [2]: beta1_power}
#   7: @↓mindspore_nn_optim_adam_Adam_construct_141:beta2_power{[0]: ValueNode<DoSignaturePrimitive> S_Prim_mul, [1]: param_beta2_power, [2]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=0.999)}
#   8: @↓mindspore_nn_optim_adam_Adam_construct_141:CNode_180{[0]: ValueNode<FuncGraph> assign_178, [1]: param_beta2_power, [2]: beta2_power}
#   9: @↓mindspore_nn_optim_adam_Adam_construct_141:CNode_190{[0]: ValueNode<FuncGraph> _apply_adam_189, [1]: CNode_183, [2]: beta1_power, [3]: beta2_power, [4]: CNode_184, [5]: CNode_185, [6]: lr, [7]: gradients}
#  10: @↓mindspore_nn_optim_adam_Adam_construct_141:CNode_192{[0]: ValueNode<Primitive> Return, [1]: CNode_191}


subgraph attr:
training : 1
subgraph instance: ✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_149 : 0x6d83d10
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:777/    def construct(self, logits, labels):/
subgraph @✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_149 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_133]() {
  %1(CNode_193) = S_Prim_equal("mean", "mean")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:781/            if self.reduction == 'mean':/
  %2(CNode_194) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:781/            if self.reduction == 'mean':/
  %3(CNode_195) = Switch(%2, @2✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_196, @✗✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_197)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:781/            if self.reduction == 'mean':/
  %4(CNode_198) = %3()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:781/            if self.reduction == 'mean':/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:781/            if self.reduction == 'mean':/
}
# Order:
#   1: @✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_149:CNode_193{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: ValueNode<StringImm> mean, [2]: ValueNode<StringImm> mean}
#   2: @✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_149:CNode_194{[0]: ValueNode<Primitive> Cond, [1]: CNode_193, [2]: ValueNode<BoolImm> false}
#   3: @✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_149:CNode_195{[0]: ValueNode<Primitive> Switch, [1]: CNode_194, [2]: ValueNode<FuncGraph> 2✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_196, [3]: ValueNode<FuncGraph> ✗✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_197}
#   4: @✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_149:CNode_198{[0]: CNode_195}
#   5: @✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_149:CNode_199{[0]: ValueNode<Primitive> Return, [1]: CNode_198}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct_167 : 0x6d7f6e0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct_167 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12](%para94_x) {
  %1(CNode_201) = call @L_mindspore_nn_layer_basic_Dense_construct_200(%para94_x, %para20_fc3.bias, %para19_fc3.weight)
      : (<null>, <Ref[Tensor[Float32]], (10), ref_key=:fc3.bias>, <Ref[Tensor[Float32]], (10, 128), ref_key=:fc3.weight>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct_167:CNode_201{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_200, [1]: param_x, [2]: param_fc3.bias, [3]: param_fc3.weight}
#   2: @mindspore_nn_layer_basic_Dense_construct_167:CNode_103{[0]: ValueNode<Primitive> Return, [1]: CNode_201}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_activation_ReLU_construct_155 : 0x6d7dca0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/activation.py:437/    def construct(self, x):/
subgraph @mindspore_nn_layer_activation_ReLU_construct_155(%para95_x) {
  %1(CNode_202) = S_Prim_ReLU[output_names: ["output"], input_names: ["x"]](%para95_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/relu-ReLU)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/activation.py:438/        return self.relu(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/relu-ReLU)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/activation.py:438/        return self.relu(x)/
}
# Order:
#   1: @mindspore_nn_layer_activation_ReLU_construct_155:CNode_202{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: param_x}
#   2: @mindspore_nn_layer_activation_ReLU_construct_155:CNode_203{[0]: ValueNode<Primitive> Return, [1]: CNode_202}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct_166 : 0x6d7c070
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct_166 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12](%para96_x) {
  %1(CNode_204) = call @L_mindspore_nn_layer_basic_Dense_construct_200(%para96_x, %para18_fc2.bias, %para17_fc2.weight)
      : (<null>, <Ref[Tensor[Float32]], (128), ref_key=:fc2.bias>, <Ref[Tensor[Float32]], (128, 256), ref_key=:fc2.weight>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc2-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct_166:CNode_204{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_200, [1]: param_x, [2]: param_fc2.bias, [3]: param_fc2.weight}
#   2: @mindspore_nn_layer_basic_Dense_construct_166:CNode_205{[0]: ValueNode<Primitive> Return, [1]: CNode_204}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct_165 : 0x6d5ceb0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct_165 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12](%para97_x) {
  %1(CNode_90) = call @L_mindspore_nn_layer_basic_Dense_construct_200(%para97_x, %para16_fc1.bias, %para15_fc1.weight)
      : (<null>, <Ref[Tensor[Float32]], (256), ref_key=:fc1.bias>, <Ref[Tensor[Float32]], (256, 2304), ref_key=:fc1.weight>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc1-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct_165:CNode_90{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_200, [1]: param_x, [2]: param_fc1.bias, [3]: param_fc1.weight}
#   2: @mindspore_nn_layer_basic_Dense_construct_165:CNode_91{[0]: ValueNode<Primitive> Return, [1]: CNode_90}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Flatten_construct_164 : 0x6ce3f40
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:461/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Flatten_construct_164(%para98_x) {
  %1(x_rank) = call @rank_206(%para98_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:462/        x_rank = F.rank(x)/
  %2(CNode_207) = S_Prim_not_equal(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
  %3(CNode_208) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
  %4(CNode_209) = Switch(%3, @↰mindspore_nn_layer_basic_Flatten_construct_210, @↱mindspore_nn_layer_basic_Flatten_construct_211)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
  %5(ndim) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
  %6(CNode_213) = call @check_axis_valid_212(I64(1), %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:464/        self.check_axis_valid(self.start_dim, ndim)/
  %7(CNode_214) = call @check_axis_valid_212(I64(-1), %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:465/        self.check_axis_valid(self.end_dim, ndim)/
  %8(CNode_215) = MakeTuple(%6, %7)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:461/    def construct(self, x):/
  %9(CNode_216) = StopGradient(%8)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:461/    def construct(self, x):/
  %10(CNode_217) = S_Prim_MakeTuple(%para98_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %11(CNode_218) = S_Prim_MakeTuple("start_dim", "end_dim")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %12(CNode_219) = S_Prim_MakeTuple(I64(1), I64(-1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %13(CNode_220) = S_Prim_make_dict(%11, %12)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %14(CNode_221) = UnpackCall_unpack_call(@flatten_222, %10, %13)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %15(CNode_223) = Depend[side_effect_propagate: I64(1)](%14, %9)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%15)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
}
# Order:
#   1: @mindspore_nn_layer_basic_Flatten_construct_164:x_rank{[0]: ValueNode<FuncGraph> rank_206, [1]: param_x}
#   2: @mindspore_nn_layer_basic_Flatten_construct_164:CNode_207{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: x_rank, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_basic_Flatten_construct_164:CNode_208{[0]: ValueNode<Primitive> Cond, [1]: CNode_207, [2]: ValueNode<BoolImm> false}
#   4: @mindspore_nn_layer_basic_Flatten_construct_164:CNode_209{[0]: ValueNode<Primitive> Switch, [1]: CNode_208, [2]: ValueNode<FuncGraph> ↰mindspore_nn_layer_basic_Flatten_construct_210, [3]: ValueNode<FuncGraph> ↱mindspore_nn_layer_basic_Flatten_construct_211}
#   5: @mindspore_nn_layer_basic_Flatten_construct_164:ndim{[0]: CNode_209}
#   6: @mindspore_nn_layer_basic_Flatten_construct_164:CNode_213{[0]: ValueNode<FuncGraph> check_axis_valid_212, [1]: ValueNode<Int64Imm> 1, [2]: ndim}
#   7: @mindspore_nn_layer_basic_Flatten_construct_164:CNode_214{[0]: ValueNode<FuncGraph> check_axis_valid_212, [1]: ValueNode<Int64Imm> -1, [2]: ndim}
#   8: @mindspore_nn_layer_basic_Flatten_construct_164:CNode_217{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: param_x}
#   9: @mindspore_nn_layer_basic_Flatten_construct_164:CNode_218{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<StringImm> start_dim, [2]: ValueNode<StringImm> end_dim}
#  10: @mindspore_nn_layer_basic_Flatten_construct_164:CNode_219{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<Int64Imm> 1, [2]: ValueNode<Int64Imm> -1}
#  11: @mindspore_nn_layer_basic_Flatten_construct_164:CNode_220{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_dict, [1]: CNode_218, [2]: CNode_219}
#  12: @mindspore_nn_layer_basic_Flatten_construct_164:CNode_221{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.224, [1]: ValueNode<FuncGraph> flatten_222, [2]: CNode_217, [3]: CNode_220}
#  13: @mindspore_nn_layer_basic_Flatten_construct_164:CNode_225{[0]: ValueNode<Primitive> Return, [1]: CNode_223}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_156 : 0x68d3b40
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_156(%para99_x) {
  %1(CNode_226) = getattr(%para99_x, "ndim")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:570/        if x.ndim == 3:/
  %2(CNode_227) = S_Prim_equal(%1, I64(3))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:570/        if x.ndim == 3:/
  %3(CNode_228) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:570/        if x.ndim == 3:/
  %4(CNode_229) = Switch(%3, @✓mindspore_nn_layer_pooling_MaxPool2d_construct_230, @✗mindspore_nn_layer_pooling_MaxPool2d_construct_231)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:570/        if x.ndim == 3:/
  %5(CNode_232) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:570/        if x.ndim == 3:/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:570/        if x.ndim == 3:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_156:CNode_226{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> ndim}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_156:CNode_227{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: CNode_226, [2]: ValueNode<Int64Imm> 3}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_156:CNode_228{[0]: ValueNode<Primitive> Cond, [1]: CNode_227, [2]: ValueNode<BoolImm> false}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_156:CNode_229{[0]: ValueNode<Primitive> Switch, [1]: CNode_228, [2]: ValueNode<FuncGraph> ✓mindspore_nn_layer_pooling_MaxPool2d_construct_230, [3]: ValueNode<FuncGraph> ✗mindspore_nn_layer_pooling_MaxPool2d_construct_231}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_156:CNode_232{[0]: CNode_229}
#   6: @mindspore_nn_layer_pooling_MaxPool2d_construct_156:CNode_233{[0]: ValueNode<Primitive> Return, [1]: CNode_232}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_163 : 0x68b09d0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_163 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12](%para100_x) {
  %1(CNode_234) = S_Prim_Shape(%para100_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn4-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %2(CNode_235) = S_Prim__check_input_dim[constexpr_prim: Bool(1)](%1, "BatchNorm2d")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn4-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %3(CNode_236) = StopGradient(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn4-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:138/    def construct(self, x):/
  %4(CNode_237) = S_Prim_is_(None, None)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn4-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:140/        if self.use_batch_statistics is None:/
  %5(CNode_238) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn4-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:140/        if self.use_batch_statistics is None:/
  %6(CNode_239) = Switch(%5, @✓mindspore_nn_layer_normalization_BatchNorm2d_construct_240, @✗mindspore_nn_layer_normalization_BatchNorm2d_construct_241)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn4-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:140/        if self.use_batch_statistics is None:/
  %7(CNode_242) = %6()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn4-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:140/        if self.use_batch_statistics is None:/
  %8(CNode_243) = Depend[side_effect_propagate: I64(1)](%7, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn4-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn4-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_163:CNode_234{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_x}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_163:CNode_235{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_input_dim, [1]: CNode_234, [2]: ValueNode<StringImm> BatchNorm2d}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_163:CNode_237{[0]: ValueNode<DoSignaturePrimitive> S_Prim_is_, [1]: ValueNode<None> None, [2]: ValueNode<None> None}
#   4: @mindspore_nn_layer_normalization_BatchNorm2d_construct_163:CNode_238{[0]: ValueNode<Primitive> Cond, [1]: CNode_237, [2]: ValueNode<BoolImm> false}
#   5: @mindspore_nn_layer_normalization_BatchNorm2d_construct_163:CNode_239{[0]: ValueNode<Primitive> Switch, [1]: CNode_238, [2]: ValueNode<FuncGraph> ✓mindspore_nn_layer_normalization_BatchNorm2d_construct_240, [3]: ValueNode<FuncGraph> ✗mindspore_nn_layer_normalization_BatchNorm2d_construct_241}
#   6: @mindspore_nn_layer_normalization_BatchNorm2d_construct_163:CNode_242{[0]: CNode_239}
#   7: @mindspore_nn_layer_normalization_BatchNorm2d_construct_163:CNode_244{[0]: ValueNode<Primitive> Return, [1]: CNode_243}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_162 : 0x6a5eb60
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_162 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12](%para101_x) {
  %1(CNode_246) = call @✗mindspore_nn_layer_conv_Conv2d_construct_245()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv4-Conv2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv4-Conv2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_162:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_conv4.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_162:CNode_246{[0]: ValueNode<FuncGraph> ✗mindspore_nn_layer_conv_Conv2d_construct_245}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_162:CNode_247{[0]: ValueNode<Primitive> Return, [1]: CNode_246}


subgraph attr:
subgraph instance: concat_161 : 0x68807a0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:2163/def concat(tensors, axis=0):/
subgraph @concat_161(%para102_tensors, %para103_axis) {
  %1(CNode_249) = call @cat_248(%para102_tensors, %para103_axis)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:2174/    return cat(tensors, axis)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:2174/    return cat(tensors, axis)/
}
# Order:
#   1: @concat_161:CNode_249{[0]: ValueNode<FuncGraph> cat_248, [1]: param_tensors, [2]: param_axis}
#   2: @concat_161:CNode_250{[0]: ValueNode<Primitive> Return, [1]: CNode_249}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_160 : 0x695cba0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_160 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12](%para104_x) {
  %1(CNode_252) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_251(%para104_x, %para11_bn3.gamma, %para12_bn3.beta, %para67_bn3.moving_mean, %para68_bn3.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (32), ref_key=:bn3.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:bn3.beta>, <Ref[Tensor[Float32]], (32), ref_key=:bn3.moving_mean>, <Ref[Tensor[Float32]], (32), ref_key=:bn3.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn3-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_160:CNode_252{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_251, [1]: param_x, [2]: param_bn3.gamma, [3]: param_bn3.beta, [4]: param_bn3.moving_mean, [5]: param_bn3.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_160:CNode_253{[0]: ValueNode<Primitive> Return, [1]: CNode_252}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_159 : 0x696d710
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_159 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12](%para105_x) {
  %1(CNode_255) = call @✗mindspore_nn_layer_conv_Conv2d_construct_254()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv3-Conv2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv3-Conv2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_159:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_conv3.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_159:CNode_255{[0]: ValueNode<FuncGraph> ✗mindspore_nn_layer_conv_Conv2d_construct_254}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_159:CNode_256{[0]: ValueNode<Primitive> Return, [1]: CNode_255}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_158 : 0x6b5c700
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_158 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12](%para106_x) {
  %1(CNode_257) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_251(%para106_x, %para9_bn2.gamma, %para10_bn2.beta, %para65_bn2.moving_mean, %para66_bn2.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (32), ref_key=:bn2.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:bn2.beta>, <Ref[Tensor[Float32]], (32), ref_key=:bn2.moving_mean>, <Ref[Tensor[Float32]], (32), ref_key=:bn2.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn2-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_158:CNode_257{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_251, [1]: param_x, [2]: param_bn2.gamma, [3]: param_bn2.beta, [4]: param_bn2.moving_mean, [5]: param_bn2.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_158:CNode_258{[0]: ValueNode<Primitive> Return, [1]: CNode_257}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_157 : 0x6a0c790
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_157 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12](%para107_x) {
  %1(CNode_260) = call @✗mindspore_nn_layer_conv_Conv2d_construct_259()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv2-Conv2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv2-Conv2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_157:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_conv2.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_157:CNode_260{[0]: ValueNode<FuncGraph> ✗mindspore_nn_layer_conv_Conv2d_construct_259}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_157:CNode_261{[0]: ValueNode<Primitive> Return, [1]: CNode_260}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_154 : 0x6886100
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_154 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12](%para108_x) {
  %1(CNode_262) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_251(%para108_x, %para7_bn1.gamma, %para8_bn1.beta, %para63_bn1.moving_mean, %para64_bn1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (32), ref_key=:bn1.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:bn1.beta>, <Ref[Tensor[Float32]], (32), ref_key=:bn1.moving_mean>, <Ref[Tensor[Float32]], (32), ref_key=:bn1.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn1-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_154:CNode_262{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_251, [1]: param_x, [2]: param_bn1.gamma, [3]: param_bn1.beta, [4]: param_bn1.moving_mean, [5]: param_bn1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_154:CNode_263{[0]: ValueNode<Primitive> Return, [1]: CNode_262}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_153 : 0x6c3c150
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_153 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12](%para109_x) {
  %1(CNode_265) = call @✗mindspore_nn_layer_conv_Conv2d_construct_264()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv1-Conv2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv1-Conv2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_153:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_conv1.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_153:CNode_265{[0]: ValueNode<FuncGraph> ✗mindspore_nn_layer_conv_Conv2d_construct_264}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_153:CNode_266{[0]: ValueNode<Primitive> Return, [1]: CNode_265}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ↓decay_weight_168 : 0x6b113e0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:429/    def decay_weight(self, gradients):/
subgraph @↓decay_weight_168 parent: [subgraph @decay_weight_129]() {
  Return(%para88_gradients)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:450/        return gradients/
}
# Order:
#   1: @↓decay_weight_168:CNode_267{[0]: ValueNode<Primitive> Return, [1]: param_gradients}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ↓flatten_gradients_171 : 0x6951ab0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:410/    def flatten_gradients(self, gradients):/
subgraph @↓flatten_gradients_171 parent: [subgraph @flatten_gradients_128]() {
  Return(%para89_gradients)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:427/        return gradients/
}
# Order:
#   1: @↓flatten_gradients_171:CNode_268{[0]: ValueNode<Primitive> Return, [1]: param_gradients}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ✗gradients_centralization_174 : 0x6c82350
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:452/    def gradients_centralization(self, gradients):/
subgraph @✗gradients_centralization_174 parent: [subgraph @gradients_centralization_143]() {
  %1(CNode_270) = call @↓gradients_centralization_269()
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:466/        if self.is_group:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:466/        if self.is_group:/
}
# Order:
#   1: @✗gradients_centralization_174:CNode_270{[0]: ValueNode<FuncGraph> ↓gradients_centralization_269}
#   2: @✗gradients_centralization_174:CNode_271{[0]: ValueNode<Primitive> Return, [1]: CNode_270}


subgraph attr:
subgraph instance: assign_178 : 0x68e49e0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/parameter_func.py:24/def assign(variable, value):/
subgraph @assign_178(%para110_variable, %para111_value) {
  %1(CNode_272) = S_Prim_Assign[output_names: ["output"], side_effect_mem: Bool(1), input_names: ["ref", "value"]](%para110_variable, %para111_value)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/parameter_func.py:58/    return assign_(variable, value)/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/parameter_func.py:58/    return assign_(variable, value)/
}
# Order:
#   1: @assign_178:CNode_272{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Assign, [1]: param_variable, [2]: param_value}
#   2: @assign_178:CNode_273{[0]: ValueNode<Primitive> Return, [1]: CNode_272}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: _apply_adam_189 : 0x68ad0b0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_189(%para112_params, %para113_beta1_power, %para114_beta2_power, %para115_moment1, %para116_moment2, %para117_lr, %para118_gradients) {
  %1(CNode_275) = call @✗_apply_adam_274()
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:817/        if self.use_offload:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:817/        if self.use_offload:/
}
# Order:
#   1: @_apply_adam_189:CNode_275{[0]: ValueNode<FuncGraph> ✗_apply_adam_274}
#   2: @_apply_adam_189:CNode_276{[0]: ValueNode<Primitive> Return, [1]: CNode_275}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: _grad_sparse_indices_deduplicate_188 : 0x6c1ab80
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:517/    def _grad_sparse_indices_deduplicate(self, gradients):/
subgraph @_grad_sparse_indices_deduplicate_188(%para119_gradients) {
  %1(CNode_277) = S_Prim_not_equal("CPU", "CPU")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:519/        if self._target != 'CPU' and self._unique:/
  %2(CNode_278) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:519/        if self._target != 'CPU' and self._unique:/
  %3(CNode_279) = Switch(%2, @↰_grad_sparse_indices_deduplicate_280, @↱_grad_sparse_indices_deduplicate_281)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:519/        if self._target != 'CPU' and self._unique:/
  %4(CNode_282) = %3()
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:519/        if self._target != 'CPU' and self._unique:/
  %5(CNode_283) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:519/        if self._target != 'CPU' and self._unique:/
  %6(CNode_284) = Switch(%5, @✓_grad_sparse_indices_deduplicate_285, @✗_grad_sparse_indices_deduplicate_286)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:519/        if self._target != 'CPU' and self._unique:/
  %7(CNode_287) = %6()
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:519/        if self._target != 'CPU' and self._unique:/
  %8(CNode_289) = call @↓_grad_sparse_indices_deduplicate_288(%7)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:919/        gradients = self._grad_sparse_indices_deduplicate(gradients)/
  Return(%8)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:519/        if self._target != 'CPU' and self._unique:/
}
# Order:
#   1: @_grad_sparse_indices_deduplicate_188:CNode_277{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: ValueNode<StringImm> CPU, [2]: ValueNode<StringImm> CPU}
#   2: @_grad_sparse_indices_deduplicate_188:CNode_278{[0]: ValueNode<Primitive> Cond, [1]: CNode_277, [2]: ValueNode<BoolImm> false}
#   3: @_grad_sparse_indices_deduplicate_188:CNode_279{[0]: ValueNode<Primitive> Switch, [1]: CNode_278, [2]: ValueNode<FuncGraph> ↰_grad_sparse_indices_deduplicate_280, [3]: ValueNode<FuncGraph> ↱_grad_sparse_indices_deduplicate_281}
#   4: @_grad_sparse_indices_deduplicate_188:CNode_282{[0]: CNode_279}
#   5: @_grad_sparse_indices_deduplicate_188:CNode_283{[0]: ValueNode<Primitive> Cond, [1]: CNode_282, [2]: ValueNode<BoolImm> false}
#   6: @_grad_sparse_indices_deduplicate_188:CNode_284{[0]: ValueNode<Primitive> Switch, [1]: CNode_283, [2]: ValueNode<FuncGraph> ✓_grad_sparse_indices_deduplicate_285, [3]: ValueNode<FuncGraph> ✗_grad_sparse_indices_deduplicate_286}
#   7: @_grad_sparse_indices_deduplicate_188:CNode_287{[0]: CNode_284}
#   8: @_grad_sparse_indices_deduplicate_188:CNode_289{[0]: ValueNode<FuncGraph> ↓_grad_sparse_indices_deduplicate_288, [1]: CNode_287}
#   9: @_grad_sparse_indices_deduplicate_188:CNode_290{[0]: ValueNode<Primitive> Return, [1]: CNode_289}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: scale_grad_187 : 0x69103d0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:471/    def scale_grad(self, gradients):/
subgraph @scale_grad_187(%para120_gradients) {
  %1(CNode_292) = call @✗scale_grad_291()
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:485/        if self.need_scale:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:485/        if self.need_scale:/
}
# Order:
#   1: @scale_grad_187:CNode_292{[0]: ValueNode<FuncGraph> ✗scale_grad_291}
#   2: @scale_grad_187:CNode_293{[0]: ValueNode<Primitive> Return, [1]: CNode_292}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: get_lr_186 : 0x6c9c7f0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:739/    def get_lr(self):/
subgraph @get_lr_186 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12]() {
  %1(CNode_295) = call @✗get_lr_294()
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:748/        if self.dynamic_lr:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:748/        if self.dynamic_lr:/
}
# Order:
#   1: @get_lr_186:CNode_295{[0]: ValueNode<FuncGraph> ✗get_lr_294}
#   2: @get_lr_186:CNode_296{[0]: ValueNode<Primitive> Return, [1]: CNode_295}


subgraph attr:
training : 1
subgraph instance: 2✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_196 : 0x6d96400
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:777/    def construct(self, logits, labels):/
subgraph @2✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_196 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_133]() {
  %1(x) = S_Prim_SparseSoftmaxCrossEntropyWithLogits[output_names: ["output"], input_names: ["features", "labels"], sens: F32(1), is_grad: Bool(0)](%para90_logits, %para91_labels)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:782/                x = self.sparse_softmax_cross_entropy(logits, labels)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:783/                return x/
}
# Order:
#   1: @2✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_196:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_SparseSoftmaxCrossEntropyWithLogits, [1]: param_logits, [2]: param_labels}
#   2: @2✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_196:CNode_297{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: ✗✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_197 : 0x6d85dc0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:777/    def construct(self, logits, labels):/
subgraph @✗✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_197 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_133]() {
  %1(CNode_299) = call @↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_298()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:781/            if self.reduction == 'mean':/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:781/            if self.reduction == 'mean':/
}
# Order:
#   1: @✗✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_197:CNode_299{[0]: ValueNode<FuncGraph> ↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_298}
#   2: @✗✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_197:CNode_300{[0]: ValueNode<Primitive> Return, [1]: CNode_299}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_200 : 0x6cd6d40
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_200(%para121_x, %para122_, %para123_) {
  %1(x_shape) = S_Prim_Shape(%para121_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:623/        x_shape = self.shape_op(x)/
  %2(CNode_92) = S_Prim_check_dense_input_shape[constexpr_prim: Bool(1)](%1, "Dense")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:624/        check_dense_input_shape(x_shape, self.cls_name)/
  %3(CNode_93) = StopGradient(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
  %4(CNode_94) = S_Prim_inner_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
  %5(CNode_95) = S_Prim_not_equal(%4, I64(2))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
  %6(CNode_96) = Cond(%5, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
  %7(CNode_97) = Switch(%6, @L_✓mindspore_nn_layer_basic_Dense_construct_301, @L_✗mindspore_nn_layer_basic_Dense_construct_302)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
  %8(CNode_100) = %7()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
  %9(CNode_101) = call @L_↓mindspore_nn_layer_basic_Dense_construct_303(%8)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:137/        x = self.fc3(x)/
  %10(CNode_102) = Depend[side_effect_propagate: I64(1)](%9, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:137/        x = self.fc3(x)/
  Return(%10)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_200:x_shape{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_x}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_200:CNode_92{[0]: ValueNode<DoSignaturePrimitive> S_Prim_check_dense_input_shape, [1]: x_shape, [2]: ValueNode<StringImm> Dense}
#   3: @L_mindspore_nn_layer_basic_Dense_construct_200:CNode_94{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: x_shape}
#   4: @L_mindspore_nn_layer_basic_Dense_construct_200:CNode_95{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: CNode_94, [2]: ValueNode<Int64Imm> 2}
#   5: @L_mindspore_nn_layer_basic_Dense_construct_200:CNode_96{[0]: ValueNode<Primitive> Cond, [1]: CNode_95, [2]: ValueNode<BoolImm> false}
#   6: @L_mindspore_nn_layer_basic_Dense_construct_200:CNode_97{[0]: ValueNode<Primitive> Switch, [1]: CNode_96, [2]: ValueNode<FuncGraph> L_✓mindspore_nn_layer_basic_Dense_construct_301, [3]: ValueNode<FuncGraph> L_✗mindspore_nn_layer_basic_Dense_construct_302}
#   7: @L_mindspore_nn_layer_basic_Dense_construct_200:CNode_100{[0]: CNode_97}
#   8: @L_mindspore_nn_layer_basic_Dense_construct_200:CNode_101{[0]: ValueNode<FuncGraph> L_↓mindspore_nn_layer_basic_Dense_construct_303, [1]: CNode_100}
#   9: @L_mindspore_nn_layer_basic_Dense_construct_200:CNode_102{[0]: ValueNode<Primitive> Depend, [1]: CNode_101, [2]: CNode_93}
#  10: @L_mindspore_nn_layer_basic_Dense_construct_200:CNode_103{[0]: ValueNode<Primitive> Return, [1]: CNode_102}


subgraph attr:
training : 1
subgraph instance: check_axis_valid_212 : 0x6ce9f50
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @check_axis_valid_212(%para124_axis, %para125_ndim) {
  %1(CNode_304) = S_Prim_negative(%para125_ndim)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  %2(CNode_305) = S_Prim_less(%para124_axis, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  %3(CNode_306) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  %4(CNode_307) = Switch(%3, @↰check_axis_valid_308, @↱check_axis_valid_309)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  %5(CNode_310) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  %6(CNode_311) = Cond(%5, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  %7(CNode_312) = Switch(%6, @✓check_axis_valid_313, @✗check_axis_valid_314)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  %8(CNode_315) = %7()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_212:CNode_304{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: param_ndim}
#   2: @check_axis_valid_212:CNode_305{[0]: ValueNode<DoSignaturePrimitive> S_Prim_less, [1]: param_axis, [2]: CNode_304}
#   3: @check_axis_valid_212:CNode_306{[0]: ValueNode<Primitive> Cond, [1]: CNode_305, [2]: ValueNode<BoolImm> false}
#   4: @check_axis_valid_212:CNode_307{[0]: ValueNode<Primitive> Switch, [1]: CNode_306, [2]: ValueNode<FuncGraph> ↰check_axis_valid_308, [3]: ValueNode<FuncGraph> ↱check_axis_valid_309}
#   5: @check_axis_valid_212:CNode_310{[0]: CNode_307}
#   6: @check_axis_valid_212:CNode_311{[0]: ValueNode<Primitive> Cond, [1]: CNode_310, [2]: ValueNode<BoolImm> false}
#   7: @check_axis_valid_212:CNode_312{[0]: ValueNode<Primitive> Switch, [1]: CNode_311, [2]: ValueNode<FuncGraph> ✓check_axis_valid_313, [3]: ValueNode<FuncGraph> ✗check_axis_valid_314}
#   8: @check_axis_valid_212:CNode_315{[0]: CNode_312}
#   9: @check_axis_valid_212:CNode_316{[0]: ValueNode<Primitive> Return, [1]: CNode_315}


subgraph attr:
subgraph instance: rank_206 : 0x6d59c30
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1541/def rank(input_x):/
subgraph @rank_206(%para126_input_x) {
  %1(CNode_317) = S_Prim_Rank(%para126_input_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1571/    return rank_(input_x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1571/    return rank_(input_x)/
}
# Order:
#   1: @rank_206:CNode_317{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Rank, [1]: param_input_x}
#   2: @rank_206:CNode_318{[0]: ValueNode<Primitive> Return, [1]: CNode_317}


subgraph attr:
training : 1
subgraph instance: ↰mindspore_nn_layer_basic_Flatten_construct_210 : 0x6d5bf20
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
subgraph @↰mindspore_nn_layer_basic_Flatten_construct_210 parent: [subgraph @mindspore_nn_layer_basic_Flatten_construct_164]() {
  %1(x_rank) = $(mindspore_nn_layer_basic_Flatten_construct_164):call @rank_206(%para98_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:462/        x_rank = F.rank(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
}
# Order:
#   1: @↰mindspore_nn_layer_basic_Flatten_construct_210:CNode_319{[0]: ValueNode<Primitive> Return, [1]: x_rank}


subgraph attr:
training : 1
subgraph instance: ↱mindspore_nn_layer_basic_Flatten_construct_211 : 0x6d5af90
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
subgraph @↱mindspore_nn_layer_basic_Flatten_construct_211() {
  Return(I64(1))
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
}
# Order:
#   1: @↱mindspore_nn_layer_basic_Flatten_construct_211:CNode_320{[0]: ValueNode<Primitive> Return, [1]: ValueNode<Int64Imm> 1}


subgraph attr:
subgraph instance: flatten_222 : 0x6cf2140
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_222(%para127_input, %para128_order, %para129_start_dim, %para130_end_dim) {
  %1(CNode_321) = S_Prim_isinstance(%para127_input, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1731/    if not isinstance(input, Tensor):/
  %2(CNode_322) = S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1731/    if not isinstance(input, Tensor):/
  %3(CNode_323) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1731/    if not isinstance(input, Tensor):/
  %4(CNode_324) = Switch(%3, @✓flatten_325, @✗flatten_326)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1731/    if not isinstance(input, Tensor):/
  %5(CNode_327) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1731/    if not isinstance(input, Tensor):/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1731/    if not isinstance(input, Tensor):/
}
# Order:
#   1: @flatten_222:CNode_321{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_input, [2]: ValueNode<ClassType> class 'mindspore.common.tensor.Tensor'}
#   2: @flatten_222:CNode_322{[0]: ValueNode<DoSignaturePrimitive> S_Prim_logical_not, [1]: CNode_321}
#   3: @flatten_222:CNode_323{[0]: ValueNode<Primitive> Cond, [1]: CNode_322, [2]: ValueNode<BoolImm> false}
#   4: @flatten_222:CNode_324{[0]: ValueNode<Primitive> Switch, [1]: CNode_323, [2]: ValueNode<FuncGraph> ✓flatten_325, [3]: ValueNode<FuncGraph> ✗flatten_326}
#   5: @flatten_222:CNode_327{[0]: CNode_324}
#   6: @flatten_222:CNode_328{[0]: ValueNode<Primitive> Return, [1]: CNode_327}


subgraph attr:
training : 1
subgraph instance: ✓mindspore_nn_layer_pooling_MaxPool2d_construct_230 : 0x6ce1e80
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @✓mindspore_nn_layer_pooling_MaxPool2d_construct_230 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_156]() {
  %1(CNode_329) = getattr(%para99_x, "unsqueeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:571/            x = x.unsqueeze(0)/
  %2(x) = %1(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:571/            x = x.unsqueeze(0)/
  %3(CNode_331) = call @↓mindspore_nn_layer_pooling_MaxPool2d_construct_330(%2, Bool(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:132/        x = self.max_pool2d(self.relu(self.bn4(self.conv4(x))))/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:571/            x = x.unsqueeze(0)/
}
# Order:
#   1: @✓mindspore_nn_layer_pooling_MaxPool2d_construct_230:CNode_329{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> unsqueeze}
#   2: @✓mindspore_nn_layer_pooling_MaxPool2d_construct_230:x{[0]: CNode_329, [1]: ValueNode<Int64Imm> 0}
#   3: @✓mindspore_nn_layer_pooling_MaxPool2d_construct_230:CNode_332{[0]: ValueNode<Primitive> Return, [1]: CNode_331}
#   4: @✓mindspore_nn_layer_pooling_MaxPool2d_construct_230:CNode_331{[0]: ValueNode<FuncGraph> ↓mindspore_nn_layer_pooling_MaxPool2d_construct_330, [1]: x, [2]: ValueNode<BoolImm> true}


subgraph attr:
training : 1
subgraph instance: ✗mindspore_nn_layer_pooling_MaxPool2d_construct_231 : 0x68cbb80
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @✗mindspore_nn_layer_pooling_MaxPool2d_construct_231 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_156]() {
  %1(CNode_333) = call @↓mindspore_nn_layer_pooling_MaxPool2d_construct_330(%para99_x, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:132/        x = self.max_pool2d(self.relu(self.bn4(self.conv4(x))))/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:570/        if x.ndim == 3:/
}
# Order:
#   1: @✗mindspore_nn_layer_pooling_MaxPool2d_construct_231:CNode_334{[0]: ValueNode<Primitive> Return, [1]: CNode_333}
#   2: @✗mindspore_nn_layer_pooling_MaxPool2d_construct_231:CNode_333{[0]: ValueNode<FuncGraph> ↓mindspore_nn_layer_pooling_MaxPool2d_construct_330, [1]: param_x, [2]: ValueNode<BoolImm> false}


subgraph attr:
training : 1
subgraph instance: ✓mindspore_nn_layer_normalization_BatchNorm2d_construct_240 : 0x68e53a0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:138/    def construct(self, x):/
subgraph @✓mindspore_nn_layer_normalization_BatchNorm2d_construct_240 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_163]() {
  %1(CNode_336) = call @2✓mindspore_nn_layer_normalization_BatchNorm2d_construct_335()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn4-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:141/            if self.training:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn4-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:141/            if self.training:/
}
# Order:
#   1: @✓mindspore_nn_layer_normalization_BatchNorm2d_construct_240:CNode_336{[0]: ValueNode<FuncGraph> 2✓mindspore_nn_layer_normalization_BatchNorm2d_construct_335}
#   2: @✓mindspore_nn_layer_normalization_BatchNorm2d_construct_240:CNode_337{[0]: ValueNode<Primitive> Return, [1]: CNode_336}


subgraph attr:
training : 1
subgraph instance: ✗mindspore_nn_layer_normalization_BatchNorm2d_construct_241 : 0x69dc040
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:138/    def construct(self, x):/
subgraph @✗mindspore_nn_layer_normalization_BatchNorm2d_construct_241 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_163]() {
  %1(CNode_339) = call @↓mindspore_nn_layer_normalization_BatchNorm2d_construct_338()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn4-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn4-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @✗mindspore_nn_layer_normalization_BatchNorm2d_construct_241:CNode_339{[0]: ValueNode<FuncGraph> ↓mindspore_nn_layer_normalization_BatchNorm2d_construct_338}
#   2: @✗mindspore_nn_layer_normalization_BatchNorm2d_construct_241:CNode_340{[0]: ValueNode<Primitive> Return, [1]: CNode_339}


subgraph attr:
training : 1
subgraph instance: ✗mindspore_nn_layer_conv_Conv2d_construct_245 : 0x6a58620
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:360/    def construct(self, x):/
subgraph @✗mindspore_nn_layer_conv_Conv2d_construct_245 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_162]() {
  %1(CNode_342) = call @↓mindspore_nn_layer_conv_Conv2d_construct_341()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv4-Conv2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv4-Conv2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @✗mindspore_nn_layer_conv_Conv2d_construct_245:CNode_342{[0]: ValueNode<FuncGraph> ↓mindspore_nn_layer_conv_Conv2d_construct_341}
#   2: @✗mindspore_nn_layer_conv_Conv2d_construct_245:CNode_343{[0]: ValueNode<Primitive> Return, [1]: CNode_342}


subgraph attr:
subgraph instance: cat_248 : 0x686ab10
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:247/def cat(tensors, axis=0):/
subgraph @cat_248(%para131_tensors, %para132_axis) {
  %1(CNode_345) = call @_get_cache_prim_344(ClassType)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:299/    _concat = _get_cache_prim(P.Concat)(axis)/
  %2(_concat) = %1(%para132_axis)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:299/    _concat = _get_cache_prim(P.Concat)(axis)/
  %3(CNode_346) = %2(%para131_tensors)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:300/    return _concat(tensors)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:300/    return _concat(tensors)/
}
# Order:
#   1: @cat_248:CNode_345{[0]: ValueNode<FuncGraph> _get_cache_prim_344, [1]: ValueNode<ClassType> class 'mindspore.ops.operations.array_ops.Concat'}
#   2: @cat_248:_concat{[0]: CNode_345, [1]: param_axis}
#   3: @cat_248:CNode_346{[0]: _concat, [1]: param_tensors}
#   4: @cat_248:CNode_347{[0]: ValueNode<Primitive> Return, [1]: CNode_346}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_251 : 0x6b25c80
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_251(%para133_x, %para134_, %para135_, %para136_, %para137_) {
  %1(CNode_348) = S_Prim_Shape(%para133_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn1-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %2(CNode_349) = S_Prim__check_input_dim[constexpr_prim: Bool(1)](%1, "BatchNorm2d")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn1-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %3(CNode_350) = StopGradient(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn1-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:138/    def construct(self, x):/
  %4(CNode_351) = S_Prim_is_(None, None)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn1-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:140/        if self.use_batch_statistics is None:/
  %5(CNode_352) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn1-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:140/        if self.use_batch_statistics is None:/
  %6(CNode_353) = Switch(%5, @L_✓mindspore_nn_layer_normalization_BatchNorm2d_construct_354, @L_✗mindspore_nn_layer_normalization_BatchNorm2d_construct_355)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn1-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:140/        if self.use_batch_statistics is None:/
  %7(CNode_356) = %6()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn1-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:140/        if self.use_batch_statistics is None:/
  %8(CNode_357) = Depend[side_effect_propagate: I64(1)](%7, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn1-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn1-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_251:CNode_348{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_x}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_251:CNode_349{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_input_dim, [1]: CNode_348, [2]: ValueNode<StringImm> BatchNorm2d}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_251:CNode_351{[0]: ValueNode<DoSignaturePrimitive> S_Prim_is_, [1]: ValueNode<None> None, [2]: ValueNode<None> None}
#   4: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_251:CNode_352{[0]: ValueNode<Primitive> Cond, [1]: CNode_351, [2]: ValueNode<BoolImm> false}
#   5: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_251:CNode_353{[0]: ValueNode<Primitive> Switch, [1]: CNode_352, [2]: ValueNode<FuncGraph> L_✓mindspore_nn_layer_normalization_BatchNorm2d_construct_354, [3]: ValueNode<FuncGraph> L_✗mindspore_nn_layer_normalization_BatchNorm2d_construct_355}
#   6: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_251:CNode_356{[0]: CNode_353}
#   7: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_251:CNode_263{[0]: ValueNode<Primitive> Return, [1]: CNode_357}


subgraph attr:
training : 1
subgraph instance: ✗mindspore_nn_layer_conv_Conv2d_construct_254 : 0x6a0d540
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:360/    def construct(self, x):/
subgraph @✗mindspore_nn_layer_conv_Conv2d_construct_254 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_159]() {
  %1(CNode_359) = call @↓mindspore_nn_layer_conv_Conv2d_construct_358()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv3-Conv2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv3-Conv2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @✗mindspore_nn_layer_conv_Conv2d_construct_254:CNode_359{[0]: ValueNode<FuncGraph> ↓mindspore_nn_layer_conv_Conv2d_construct_358}
#   2: @✗mindspore_nn_layer_conv_Conv2d_construct_254:CNode_360{[0]: ValueNode<Primitive> Return, [1]: CNode_359}


subgraph attr:
training : 1
subgraph instance: ✗mindspore_nn_layer_conv_Conv2d_construct_259 : 0x68648e0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:360/    def construct(self, x):/
subgraph @✗mindspore_nn_layer_conv_Conv2d_construct_259 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_157]() {
  %1(CNode_362) = call @↓mindspore_nn_layer_conv_Conv2d_construct_361()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv2-Conv2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv2-Conv2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @✗mindspore_nn_layer_conv_Conv2d_construct_259:CNode_362{[0]: ValueNode<FuncGraph> ↓mindspore_nn_layer_conv_Conv2d_construct_361}
#   2: @✗mindspore_nn_layer_conv_Conv2d_construct_259:CNode_363{[0]: ValueNode<Primitive> Return, [1]: CNode_362}


subgraph attr:
training : 1
subgraph instance: ✗mindspore_nn_layer_conv_Conv2d_construct_264 : 0x6958970
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:360/    def construct(self, x):/
subgraph @✗mindspore_nn_layer_conv_Conv2d_construct_264 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_153]() {
  %1(CNode_365) = call @↓mindspore_nn_layer_conv_Conv2d_construct_364()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv1-Conv2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv1-Conv2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @✗mindspore_nn_layer_conv_Conv2d_construct_264:CNode_365{[0]: ValueNode<FuncGraph> ↓mindspore_nn_layer_conv_Conv2d_construct_364}
#   2: @✗mindspore_nn_layer_conv_Conv2d_construct_264:CNode_366{[0]: ValueNode<Primitive> Return, [1]: CNode_365}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ↓gradients_centralization_269 : 0x68c00c0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:452/    def gradients_centralization(self, gradients):/
subgraph @↓gradients_centralization_269 parent: [subgraph @gradients_centralization_143]() {
  Return(%para93_gradients)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:469/        return gradients/
}
# Order:
#   1: @↓gradients_centralization_269:CNode_367{[0]: ValueNode<Primitive> Return, [1]: param_gradients}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ✗_apply_adam_274 : 0x69f6a50
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @✗_apply_adam_274 parent: [subgraph @_apply_adam_189]() {
  %1(CNode_369) = call @2✗_apply_adam_368()
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:826/            if self.use_dist_optimizer:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:826/            if self.use_dist_optimizer:/
}
# Order:
#   1: @✗_apply_adam_274:CNode_369{[0]: ValueNode<FuncGraph> 2✗_apply_adam_368}
#   2: @✗_apply_adam_274:CNode_370{[0]: ValueNode<Primitive> Return, [1]: CNode_369}


subgraph attr:
after_block : 1
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ↓_grad_sparse_indices_deduplicate_288 : 0x69013e0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:517/    def _grad_sparse_indices_deduplicate(self, gradients):/
subgraph @↓_grad_sparse_indices_deduplicate_288(%para138_) {
  Return(%para138_фgradients)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:521/        return gradients/
}
# Order:
#   1: @↓_grad_sparse_indices_deduplicate_288:CNode_371{[0]: ValueNode<Primitive> Return, [1]: param_фgradients}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ✓_grad_sparse_indices_deduplicate_285 : 0x6a94960
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:517/    def _grad_sparse_indices_deduplicate(self, gradients):/
subgraph @✓_grad_sparse_indices_deduplicate_285 parent: [subgraph @_grad_sparse_indices_deduplicate_188]() {
  %1(CNode_372) = S_Prim_Partial[side_effect_propagate: I64(1)](S_Prim_indices_deduplicate)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:520/            gradients = self.map_(F.partial(_indices_deduplicate), gradients)/
  %2(gradients) = S_Prim_map(%1, %para119_gradients)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:520/            gradients = self.map_(F.partial(_indices_deduplicate), gradients)/
  Return(%2)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:520/            gradients = self.map_(F.partial(_indices_deduplicate), gradients)/
}
# Order:
#   1: @✓_grad_sparse_indices_deduplicate_285:CNode_372{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Partial, [1]: ValueNode<DoSignaturePrimitive> S_Prim_indices_deduplicate}
#   2: @✓_grad_sparse_indices_deduplicate_285:gradients{[0]: ValueNode<DoSignaturePrimitive> S_Prim_map, [1]: CNode_372, [2]: param_gradients}
#   3: @✓_grad_sparse_indices_deduplicate_285:CNode_373{[0]: ValueNode<Primitive> Return, [1]: gradients}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ✗_grad_sparse_indices_deduplicate_286 : 0x691c670
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:517/    def _grad_sparse_indices_deduplicate(self, gradients):/
subgraph @✗_grad_sparse_indices_deduplicate_286 parent: [subgraph @_grad_sparse_indices_deduplicate_188]() {
  Return(%para119_gradients)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:519/        if self._target != 'CPU' and self._unique:/
}
# Order:
#   1: @✗_grad_sparse_indices_deduplicate_286:CNode_374{[0]: ValueNode<Primitive> Return, [1]: param_gradients}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ↰_grad_sparse_indices_deduplicate_280 : 0x6952490
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:517/    def _grad_sparse_indices_deduplicate(self, gradients):/
subgraph @↰_grad_sparse_indices_deduplicate_280() {
  Return(Bool(1))
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:519/        if self._target != 'CPU' and self._unique:/
}
# Order:
#   1: @↰_grad_sparse_indices_deduplicate_280:CNode_375{[0]: ValueNode<Primitive> Return, [1]: ValueNode<BoolImm> true}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ↱_grad_sparse_indices_deduplicate_281 : 0x68b2e50
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:517/    def _grad_sparse_indices_deduplicate(self, gradients):/
subgraph @↱_grad_sparse_indices_deduplicate_281 parent: [subgraph @_grad_sparse_indices_deduplicate_188]() {
  %1(CNode_277) = $(_grad_sparse_indices_deduplicate_188):S_Prim_not_equal("CPU", "CPU")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:519/        if self._target != 'CPU' and self._unique:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:519/        if self._target != 'CPU' and self._unique:/
}
# Order:
#   1: @↱_grad_sparse_indices_deduplicate_281:CNode_376{[0]: ValueNode<Primitive> Return, [1]: CNode_277}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ✗scale_grad_291 : 0x6b5d130
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:471/    def scale_grad(self, gradients):/
subgraph @✗scale_grad_291 parent: [subgraph @scale_grad_187]() {
  %1(CNode_378) = call @↓scale_grad_377()
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:485/        if self.need_scale:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:485/        if self.need_scale:/
}
# Order:
#   1: @✗scale_grad_291:CNode_378{[0]: ValueNode<FuncGraph> ↓scale_grad_377}
#   2: @✗scale_grad_291:CNode_379{[0]: ValueNode<Primitive> Return, [1]: CNode_378}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ✗get_lr_294 : 0x6c16790
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:739/    def get_lr(self):/
subgraph @✗get_lr_294 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12]() {
  %1(CNode_381) = call @↓get_lr_380()
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:748/        if self.dynamic_lr:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:748/        if self.dynamic_lr:/
}
# Order:
#   1: @✗get_lr_294:CNode_381{[0]: ValueNode<FuncGraph> ↓get_lr_380}
#   2: @✗get_lr_294:CNode_382{[0]: ValueNode<Primitive> Return, [1]: CNode_381}


subgraph attr:
after_block : 1
training : 1
subgraph instance: ↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_298 : 0x6d87380
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:777/    def construct(self, logits, labels):/
subgraph @↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_298 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_133]() {
  %1(CNode_384) = call @↓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_383()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:781/            if self.reduction == 'mean':/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:781/            if self.reduction == 'mean':/
}
# Order:
#   1: @↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_298:CNode_385{[0]: ValueNode<FuncGraph> shape_386, [1]: param_logits}
#   2: @↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_298:CNode_387{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   3: @↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_298:CNode_388{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_385, [2]: CNode_387}
#   4: @↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_298:labels{[0]: ValueNode<DoSignaturePrimitive> S_Prim_OneHot, [1]: param_labels, [2]: CNode_388, [3]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=1), [4]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=0)}
#   5: @↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_298:CNode_384{[0]: ValueNode<FuncGraph> ↓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_383}
#   6: @↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_298:CNode_389{[0]: ValueNode<Primitive> Return, [1]: CNode_384}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_↓mindspore_nn_layer_basic_Dense_construct_303 : 0x6cdd9e0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_↓mindspore_nn_layer_basic_Dense_construct_303 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_200](%para139_) {
  %1(CNode_104) = call @L_✓↓mindspore_nn_layer_basic_Dense_construct_390()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:628/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:628/        if self.has_bias:/
}
# Order:
#   1: @L_↓mindspore_nn_layer_basic_Dense_construct_303:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MatMul, [1]: param_фx, [2]: param_L_fc3.weight}
#   2: @L_↓mindspore_nn_layer_basic_Dense_construct_303:CNode_104{[0]: ValueNode<FuncGraph> L_✓↓mindspore_nn_layer_basic_Dense_construct_390}
#   3: @L_↓mindspore_nn_layer_basic_Dense_construct_303:CNode_105{[0]: ValueNode<Primitive> Return, [1]: CNode_104}


subgraph attr:
training : 1
subgraph instance: L_✓mindspore_nn_layer_basic_Dense_construct_301 : 0x6cdbb20
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_✓mindspore_nn_layer_basic_Dense_construct_301 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_200]() {
  %1(CNode_391) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  %2(x_shape) = $(L_mindspore_nn_layer_basic_Dense_construct_200):S_Prim_Shape(%para121_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:623/        x_shape = self.shape_op(x)/
  %3(CNode_392) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  %4(CNode_393) = S_Prim_getitem(%2, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  %5(CNode_394) = S_Prim_MakeTuple(%1, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  %6(x) = S_Prim_Reshape[output_names: ["output"], input_names: ["tensor", "shape"]](%para121_x, %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
}
# Order:
#   1: @L_✓mindspore_nn_layer_basic_Dense_construct_301:CNode_391{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @L_✓mindspore_nn_layer_basic_Dense_construct_301:CNode_392{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   3: @L_✓mindspore_nn_layer_basic_Dense_construct_301:CNode_393{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: CNode_392}
#   4: @L_✓mindspore_nn_layer_basic_Dense_construct_301:CNode_394{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_391, [2]: CNode_393}
#   5: @L_✓mindspore_nn_layer_basic_Dense_construct_301:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_x, [2]: CNode_394}
#   6: @L_✓mindspore_nn_layer_basic_Dense_construct_301:CNode_395{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: L_✗mindspore_nn_layer_basic_Dense_construct_302 : 0x6cdac20
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_✗mindspore_nn_layer_basic_Dense_construct_302 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_200]() {
  Return(%para121_x)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_✗mindspore_nn_layer_basic_Dense_construct_302:CNode_396{[0]: ValueNode<Primitive> Return, [1]: param_x}


subgraph attr:
training : 1
subgraph instance: ✓check_axis_valid_313 : 0x6cf0e70
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @✓check_axis_valid_313() {
  %1(CNode_397) = raise[side_effect_io: Bool(1)]("ValueError", "'start_dim' or 'end_dim' out of range.", "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:459/            raise ValueError("'start_dim' or 'end_dim' out of range.")/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:459/            raise ValueError("'start_dim' or 'end_dim' out of range.")/
}
# Order:
#   1: @✓check_axis_valid_313:CNode_397{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> ValueError, [2]: ValueNode<StringImm> 'start_dim' or 'end_dim' out of range., [3]: ValueNode<StringImm> None}
#   2: @✓check_axis_valid_313:CNode_398{[0]: ValueNode<Primitive> Return, [1]: CNode_397}


subgraph attr:
training : 1
subgraph instance: ✗check_axis_valid_314 : 0x6ceeb30
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @✗check_axis_valid_314() {
  %1(CNode_400) = call @↓check_axis_valid_399()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @✗check_axis_valid_314:CNode_400{[0]: ValueNode<FuncGraph> ↓check_axis_valid_399}
#   2: @✗check_axis_valid_314:CNode_401{[0]: ValueNode<Primitive> Return, [1]: CNode_400}


subgraph attr:
training : 1
subgraph instance: ↰check_axis_valid_308 : 0x6cedba0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @↰check_axis_valid_308 parent: [subgraph @check_axis_valid_212]() {
  %1(CNode_304) = $(check_axis_valid_212):S_Prim_negative(%para125_ndim)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  %2(CNode_305) = $(check_axis_valid_212):S_Prim_less(%para124_axis, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @↰check_axis_valid_308:CNode_402{[0]: ValueNode<Primitive> Return, [1]: CNode_305}


subgraph attr:
training : 1
subgraph instance: ↱check_axis_valid_309 : 0x6cecb30
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @↱check_axis_valid_309 parent: [subgraph @check_axis_valid_212]() {
  %1(CNode_403) = S_Prim_greater_equal(%para124_axis, %para125_ndim)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @↱check_axis_valid_309:CNode_403{[0]: ValueNode<DoSignaturePrimitive> S_Prim_greater_equal, [1]: param_axis, [2]: param_ndim}
#   2: @↱check_axis_valid_309:CNode_404{[0]: ValueNode<Primitive> Return, [1]: CNode_403}


subgraph attr:
subgraph instance: ✓flatten_325 : 0x6d58740
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✓flatten_325() {
  %1(CNode_405) = JoinedStr("For 'flatten', argument 'input' must be Tensor.")
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1732/        raise TypeError(f"For 'flatten', argument 'input' must be Tensor.")/
  %2(CNode_406) = raise[side_effect_io: Bool(1)]("TypeError", %1, "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1732/        raise TypeError(f"For 'flatten', argument 'input' must be Tensor.")/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1732/        raise TypeError(f"For 'flatten', argument 'input' must be Tensor.")/
}
# Order:
#   1: @✓flatten_325:CNode_405{[0]: ValueNode<Primitive> JoinedStr, [1]: ValueNode<StringImm> For 'flatten', argument 'input' must be Tensor.}
#   2: @✓flatten_325:CNode_406{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> TypeError, [2]: CNode_405, [3]: ValueNode<StringImm> None}
#   3: @✓flatten_325:CNode_407{[0]: ValueNode<Primitive> Return, [1]: CNode_406}


subgraph attr:
subgraph instance: ✗flatten_326 : 0x6cf7bc0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✗flatten_326 parent: [subgraph @flatten_222]() {
  %1(CNode_409) = call @↓flatten_408()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1731/    if not isinstance(input, Tensor):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1731/    if not isinstance(input, Tensor):/
}
# Order:
#   1: @✗flatten_326:CNode_409{[0]: ValueNode<FuncGraph> ↓flatten_408}
#   2: @✗flatten_326:CNode_410{[0]: ValueNode<Primitive> Return, [1]: CNode_409}


subgraph attr:
after_block : 1
training : 1
subgraph instance: ↓mindspore_nn_layer_pooling_MaxPool2d_construct_330 : 0x684b000
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @↓mindspore_nn_layer_pooling_MaxPool2d_construct_330(%para140_, %para141_) {
  %1(CNode_412) = call @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_411()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:573/        if self.use_pad:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:573/        if self.use_pad:/
}
# Order:
#   1: @↓mindspore_nn_layer_pooling_MaxPool2d_construct_330:CNode_412{[0]: ValueNode<FuncGraph> ✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_411}
#   2: @↓mindspore_nn_layer_pooling_MaxPool2d_construct_330:CNode_413{[0]: ValueNode<Primitive> Return, [1]: CNode_412}


subgraph attr:
training : 1
subgraph instance: 2✓mindspore_nn_layer_normalization_BatchNorm2d_construct_335 : 0x68d97f0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:138/    def construct(self, x):/
subgraph @2✓mindspore_nn_layer_normalization_BatchNorm2d_construct_335 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_163]() {
  %1(CNode_414) = S_Prim_BatchNorm[side_effect_mem: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"], format: "NCHW", is_training: Bool(1), momentum: F32(0.1)](%para100_x, %para13_bn4.gamma, %para14_bn4.beta, %para61_bn4.moving_mean, %para62_bn4.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (64), ref_key=:bn4.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:bn4.beta>, <Ref[Tensor[Float32]], (64), ref_key=:bn4.moving_mean>, <Ref[Tensor[Float32]], (64), ref_key=:bn4.moving_variance>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn4-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:142/                return self.bn_train(x,/
  %2(CNode_415) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn4-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:142/                return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn4-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:142/                return self.bn_train(x,/
}
# Order:
#   1: @2✓mindspore_nn_layer_normalization_BatchNorm2d_construct_335:CNode_414{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_bn4.gamma, [3]: param_bn4.beta, [4]: param_bn4.moving_mean, [5]: param_bn4.moving_variance}
#   2: @2✓mindspore_nn_layer_normalization_BatchNorm2d_construct_335:CNode_415{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_414, [2]: ValueNode<Int64Imm> 0}
#   3: @2✓mindspore_nn_layer_normalization_BatchNorm2d_construct_335:CNode_416{[0]: ValueNode<Primitive> Return, [1]: CNode_415}


subgraph attr:
after_block : 1
training : 1
subgraph instance: ↓mindspore_nn_layer_normalization_BatchNorm2d_construct_338 : 0x6934c40
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:138/    def construct(self, x):/
subgraph @↓mindspore_nn_layer_normalization_BatchNorm2d_construct_338 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_163]() {
  %1(CNode_417) = Cond(None, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn4-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:154/        if self.use_batch_statistics:/
  %2(CNode_418) = Switch(%1, @✓↓mindspore_nn_layer_normalization_BatchNorm2d_construct_419, @✗↓mindspore_nn_layer_normalization_BatchNorm2d_construct_420)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn4-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:154/        if self.use_batch_statistics:/
  %3(CNode_421) = %2()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn4-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:154/        if self.use_batch_statistics:/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn4-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @↓mindspore_nn_layer_normalization_BatchNorm2d_construct_338:CNode_417{[0]: ValueNode<Primitive> Cond, [1]: ValueNode<None> None, [2]: ValueNode<BoolImm> false}
#   2: @↓mindspore_nn_layer_normalization_BatchNorm2d_construct_338:CNode_418{[0]: ValueNode<Primitive> Switch, [1]: CNode_417, [2]: ValueNode<FuncGraph> ✓↓mindspore_nn_layer_normalization_BatchNorm2d_construct_419, [3]: ValueNode<FuncGraph> ✗↓mindspore_nn_layer_normalization_BatchNorm2d_construct_420}
#   3: @↓mindspore_nn_layer_normalization_BatchNorm2d_construct_338:CNode_421{[0]: CNode_418}
#   4: @↓mindspore_nn_layer_normalization_BatchNorm2d_construct_338:CNode_422{[0]: ValueNode<Primitive> Return, [1]: CNode_421}


subgraph attr:
training : 1
subgraph instance: ↓mindspore_nn_layer_conv_Conv2d_construct_341 : 0x6b5fcb0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:360/    def construct(self, x):/
subgraph @↓mindspore_nn_layer_conv_Conv2d_construct_341 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_162]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_162):S_Prim_Conv2D[kernel_size: (I64(3), I64(3)), mode: I64(1), out_channel: I64(64), input_names: ["x", "w"], pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(2), format: "NCHW", pad_list: (I64(0), I64(0), I64(0), I64(0)), groups: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), group: I64(1), dilation: (I64(1), I64(1), I64(1), I64(1)), output_names: ["output"]](%para101_x, %para6_conv4.weight)
      : (<null>, <Ref[Tensor[Float32]], (64, 96, 3, 3), ref_key=:conv4.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv4-Conv2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv4-Conv2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:364/        return output/
}
# Order:
#   1: @↓mindspore_nn_layer_conv_Conv2d_construct_341:CNode_423{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
subgraph instance: _get_cache_prim_344 : 0x6aff7d0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/_primitive_cache.py:35/def _get_cache_prim(cls: Primitive) -> Primitive:/
subgraph @_get_cache_prim_344(%para142_cls) {
  %1(CNode_425) = call @✓_get_cache_prim_424()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/_primitive_cache.py:88/    if _is_need_compile(_temp_func): # @jit.cond: True/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/_primitive_cache.py:88/    if _is_need_compile(_temp_func): # @jit.cond: True/
}
# Order:
#   1: @_get_cache_prim_344:CNode_425{[0]: ValueNode<FuncGraph> ✓_get_cache_prim_424}
#   2: @_get_cache_prim_344:CNode_426{[0]: ValueNode<Primitive> Return, [1]: CNode_425}


subgraph attr:
training : 1
subgraph instance: L_✓mindspore_nn_layer_normalization_BatchNorm2d_construct_354 : 0x694cc80
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:138/    def construct(self, x):/
subgraph @L_✓mindspore_nn_layer_normalization_BatchNorm2d_construct_354 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_251]() {
  %1(CNode_428) = call @L_2✓mindspore_nn_layer_normalization_BatchNorm2d_construct_427()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn1-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:141/            if self.training:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn1-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:141/            if self.training:/
}
# Order:
#   1: @L_✓mindspore_nn_layer_normalization_BatchNorm2d_construct_354:CNode_428{[0]: ValueNode<FuncGraph> L_2✓mindspore_nn_layer_normalization_BatchNorm2d_construct_427}
#   2: @L_✓mindspore_nn_layer_normalization_BatchNorm2d_construct_354:CNode_429{[0]: ValueNode<Primitive> Return, [1]: CNode_428}


subgraph attr:
training : 1
subgraph instance: L_✗mindspore_nn_layer_normalization_BatchNorm2d_construct_355 : 0x6c9b480
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:138/    def construct(self, x):/
subgraph @L_✗mindspore_nn_layer_normalization_BatchNorm2d_construct_355 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_251]() {
  %1(CNode_431) = call @L_↓mindspore_nn_layer_normalization_BatchNorm2d_construct_430()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn1-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn1-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @L_✗mindspore_nn_layer_normalization_BatchNorm2d_construct_355:CNode_431{[0]: ValueNode<FuncGraph> L_↓mindspore_nn_layer_normalization_BatchNorm2d_construct_430}
#   2: @L_✗mindspore_nn_layer_normalization_BatchNorm2d_construct_355:CNode_432{[0]: ValueNode<Primitive> Return, [1]: CNode_431}


subgraph attr:
training : 1
subgraph instance: ↓mindspore_nn_layer_conv_Conv2d_construct_358 : 0x696c890
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:360/    def construct(self, x):/
subgraph @↓mindspore_nn_layer_conv_Conv2d_construct_358 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_159]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_159):S_Prim_Conv2D[kernel_size: (I64(7), I64(7)), mode: I64(1), out_channel: I64(32), input_names: ["x", "w"], pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), format: "NCHW", pad_list: (I64(3), I64(3), I64(3), I64(3)), groups: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), group: I64(1), dilation: (I64(1), I64(1), I64(1), I64(1)), output_names: ["output"]](%para105_x, %para5_conv3.weight)
      : (<null>, <Ref[Tensor[Float32]], (32, 1, 7, 7), ref_key=:conv3.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv3-Conv2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv3-Conv2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:364/        return output/
}
# Order:
#   1: @↓mindspore_nn_layer_conv_Conv2d_construct_358:CNode_433{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: ↓mindspore_nn_layer_conv_Conv2d_construct_361 : 0x6b2b670
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:360/    def construct(self, x):/
subgraph @↓mindspore_nn_layer_conv_Conv2d_construct_361 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_157]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_157):S_Prim_Conv2D[kernel_size: (I64(5), I64(5)), mode: I64(1), out_channel: I64(32), input_names: ["x", "w"], pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), format: "NCHW", pad_list: (I64(2), I64(2), I64(2), I64(2)), groups: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), group: I64(1), dilation: (I64(1), I64(1), I64(1), I64(1)), output_names: ["output"]](%para107_x, %para4_conv2.weight)
      : (<null>, <Ref[Tensor[Float32]], (32, 1, 5, 5), ref_key=:conv2.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv2-Conv2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv2-Conv2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:364/        return output/
}
# Order:
#   1: @↓mindspore_nn_layer_conv_Conv2d_construct_361:CNode_434{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: ↓mindspore_nn_layer_conv_Conv2d_construct_364 : 0x69f8560
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:360/    def construct(self, x):/
subgraph @↓mindspore_nn_layer_conv_Conv2d_construct_364 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_153]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_153):S_Prim_Conv2D[kernel_size: (I64(3), I64(3)), mode: I64(1), out_channel: I64(32), input_names: ["x", "w"], pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), format: "NCHW", pad_list: (I64(1), I64(1), I64(1), I64(1)), groups: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), group: I64(1), dilation: (I64(1), I64(1), I64(1), I64(1)), output_names: ["output"]](%para109_x, %para3_conv1.weight)
      : (<null>, <Ref[Tensor[Float32]], (32, 1, 3, 3), ref_key=:conv1.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv1-Conv2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv1-Conv2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:364/        return output/
}
# Order:
#   1: @↓mindspore_nn_layer_conv_Conv2d_construct_364:CNode_435{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: 2✗_apply_adam_368 : 0x6956e60
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @2✗_apply_adam_368 parent: [subgraph @_apply_adam_189]() {
  %1(CNode_437) = call @3✗_apply_adam_436()
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:866/                if self.is_group_lr:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:866/                if self.is_group_lr:/
}
# Order:
#   1: @2✗_apply_adam_368:CNode_437{[0]: ValueNode<FuncGraph> 3✗_apply_adam_436}
#   2: @2✗_apply_adam_368:CNode_438{[0]: ValueNode<Primitive> Return, [1]: CNode_437}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ↓scale_grad_377 : 0x6b39ac0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:471/    def scale_grad(self, gradients):/
subgraph @↓scale_grad_377 parent: [subgraph @scale_grad_187]() {
  Return(%para120_gradients)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:488/        return gradients/
}
# Order:
#   1: @↓scale_grad_377:CNode_439{[0]: ValueNode<Primitive> Return, [1]: param_gradients}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ↓get_lr_380 : 0x69f2040
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:739/    def get_lr(self):/
subgraph @↓get_lr_380 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12]() {
  Return(%para60_learning_rate)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:756/        return lr/
}
# Order:
#   1: @↓get_lr_380:CNode_440{[0]: ValueNode<Primitive> Return, [1]: param_learning_rate}


subgraph attr:
subgraph instance: shape_386 : 0x6d896d0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1484/def shape(input_x):/
subgraph @shape_386(%para143_input_x) {
  %1(CNode_441) = S_Prim_Shape(%para143_input_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1510/    return shape_(input_x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1510/    return shape_(input_x)/
}
# Order:
#   1: @shape_386:CNode_441{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_input_x}
#   2: @shape_386:CNode_442{[0]: ValueNode<Primitive> Return, [1]: CNode_441}


subgraph attr:
training : 1
subgraph instance: ↓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_383 : 0x6d8ab50
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:777/    def construct(self, logits, labels):/
subgraph @↓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_383 parent: [subgraph @↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_298]() {
  %1(CNode_385) = $(↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_298):call @shape_386(%para90_logits)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:784/            labels = self.one_hot(labels, F.shape(logits)[-1], self.on_value, self.off_value)/
  %2(CNode_387) = $(↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_298):S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:784/            labels = self.one_hot(labels, F.shape(logits)[-1], self.on_value, self.off_value)/
  %3(CNode_388) = $(↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_298):S_Prim_getitem(%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:784/            labels = self.one_hot(labels, F.shape(logits)[-1], self.on_value, self.off_value)/
  %4(labels) = $(↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_298):S_Prim_OneHot[output_names: ["output"], input_names: ["indices", "depth", "on_value", "off_value"], axis: I64(-1)](%para91_labels, %3, Tensor(shape=[], dtype=Float32, value=1), Tensor(shape=[], dtype=Float32, value=0))
      : (<null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:784/            labels = self.one_hot(labels, F.shape(logits)[-1], self.on_value, self.off_value)/
  %5(CNode_443) = S_Prim_SoftmaxCrossEntropyWithLogits(%para90_logits, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:785/        x = self.softmax_cross_entropy(logits, labels)[0]/
  %6(x) = S_Prim_getitem(%5, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:785/        x = self.softmax_cross_entropy(logits, labels)[0]/
  %7(CNode_445) = call @get_loss_444(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:786/        return self.get_loss(x)/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:786/        return self.get_loss(x)/
}
# Order:
#   1: @↓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_383:CNode_443{[0]: ValueNode<DoSignaturePrimitive> S_Prim_SoftmaxCrossEntropyWithLogits, [1]: param_logits, [2]: labels}
#   2: @↓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_383:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_443, [2]: ValueNode<Int64Imm> 0}
#   3: @↓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_383:CNode_445{[0]: ValueNode<FuncGraph> get_loss_444, [1]: x}
#   4: @↓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_383:CNode_446{[0]: ValueNode<Primitive> Return, [1]: CNode_445}


subgraph attr:
training : 1
subgraph instance: L_✓↓mindspore_nn_layer_basic_Dense_construct_390 : 0x6cdf6b0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_390 parent: [subgraph @L_↓mindspore_nn_layer_basic_Dense_construct_303]() {
  %1(CNode_106) = call @L_2↓mindspore_nn_layer_basic_Dense_construct_447()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:629/            x = self.bias_add(x, self.bias)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:629/            x = self.bias_add(x, self.bias)/
}
# Order:
#   1: @L_✓↓mindspore_nn_layer_basic_Dense_construct_390:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BiasAdd, [1]: x, [2]: param_L_fc3.bias}
#   2: @L_✓↓mindspore_nn_layer_basic_Dense_construct_390:CNode_106{[0]: ValueNode<FuncGraph> L_2↓mindspore_nn_layer_basic_Dense_construct_447}
#   3: @L_✓↓mindspore_nn_layer_basic_Dense_construct_390:CNode_107{[0]: ValueNode<Primitive> Return, [1]: CNode_106}


subgraph attr:
after_block : 1
training : 1
subgraph instance: ↓check_axis_valid_399 : 0x6cefee0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @↓check_axis_valid_399() {
  Return(None)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:457/    def check_axis_valid(self, axis, ndim):/
}
# Order:
#   1: @↓check_axis_valid_399:CNode_448{[0]: ValueNode<Primitive> Return, [1]: ValueNode<None> None}


subgraph attr:
after_block : 1
subgraph instance: ↓flatten_408 : 0x6cf9fa0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↓flatten_408 parent: [subgraph @flatten_222]() {
  %1(CNode_449) = S_Prim_isinstance(%para129_start_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %2(CNode_450) = S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %3(CNode_451) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %4(CNode_452) = Switch(%3, @↰↓flatten_453, @↱↓flatten_454)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %5(CNode_455) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %6(CNode_456) = Cond(%5, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %7(CNode_457) = Switch(%6, @✓↓flatten_458, @✗↓flatten_459)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %8(CNode_460) = %7()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @↓flatten_408:CNode_449{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_start_dim, [2]: ValueNode<ClassType> class 'int'}
#   2: @↓flatten_408:CNode_450{[0]: ValueNode<DoSignaturePrimitive> S_Prim_logical_not, [1]: CNode_449}
#   3: @↓flatten_408:CNode_451{[0]: ValueNode<Primitive> Cond, [1]: CNode_450, [2]: ValueNode<BoolImm> false}
#   4: @↓flatten_408:CNode_452{[0]: ValueNode<Primitive> Switch, [1]: CNode_451, [2]: ValueNode<FuncGraph> ↰↓flatten_453, [3]: ValueNode<FuncGraph> ↱↓flatten_454}
#   5: @↓flatten_408:CNode_455{[0]: CNode_452}
#   6: @↓flatten_408:CNode_456{[0]: ValueNode<Primitive> Cond, [1]: CNode_455, [2]: ValueNode<BoolImm> false}
#   7: @↓flatten_408:CNode_457{[0]: ValueNode<Primitive> Switch, [1]: CNode_456, [2]: ValueNode<FuncGraph> ✓↓flatten_458, [3]: ValueNode<FuncGraph> ✗↓flatten_459}
#   8: @↓flatten_408:CNode_460{[0]: CNode_457}
#   9: @↓flatten_408:CNode_461{[0]: ValueNode<Primitive> Return, [1]: CNode_460}


subgraph attr:
training : 1
subgraph instance: ✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_411 : 0x69daef0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_411 parent: [subgraph @↓mindspore_nn_layer_pooling_MaxPool2d_construct_330]() {
  %1(CNode_463) = call @2↓mindspore_nn_layer_pooling_MaxPool2d_construct_462()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:581/            out = self.max_pool(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:581/            out = self.max_pool(x)/
}
# Order:
#   1: @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_411:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MaxPool, [1]: param_фx}
#   2: @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_411:CNode_463{[0]: ValueNode<FuncGraph> 2↓mindspore_nn_layer_pooling_MaxPool2d_construct_462}
#   3: @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_411:CNode_464{[0]: ValueNode<Primitive> Return, [1]: CNode_463}


subgraph attr:
training : 1
subgraph instance: ✓↓mindspore_nn_layer_normalization_BatchNorm2d_construct_419 : 0x68e6d90
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:138/    def construct(self, x):/
subgraph @✓↓mindspore_nn_layer_normalization_BatchNorm2d_construct_419 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_163]() {
  %1(CNode_465) = S_Prim_BatchNorm[side_effect_mem: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"], format: "NCHW", is_training: Bool(1), momentum: F32(0.1)](%para100_x, %para13_bn4.gamma, %para14_bn4.beta, %para61_bn4.moving_mean, %para62_bn4.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (64), ref_key=:bn4.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:bn4.beta>, <Ref[Tensor[Float32]], (64), ref_key=:bn4.moving_mean>, <Ref[Tensor[Float32]], (64), ref_key=:bn4.moving_variance>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn4-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:155/            return self.bn_train(x,/
  %2(CNode_466) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn4-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:155/            return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn4-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:155/            return self.bn_train(x,/
}
# Order:
#   1: @✓↓mindspore_nn_layer_normalization_BatchNorm2d_construct_419:CNode_465{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_bn4.gamma, [3]: param_bn4.beta, [4]: param_bn4.moving_mean, [5]: param_bn4.moving_variance}
#   2: @✓↓mindspore_nn_layer_normalization_BatchNorm2d_construct_419:CNode_466{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_465, [2]: ValueNode<Int64Imm> 0}
#   3: @✓↓mindspore_nn_layer_normalization_BatchNorm2d_construct_419:CNode_467{[0]: ValueNode<Primitive> Return, [1]: CNode_466}


subgraph attr:
training : 1
subgraph instance: ✗↓mindspore_nn_layer_normalization_BatchNorm2d_construct_420 : 0x6b3e1e0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:138/    def construct(self, x):/
subgraph @✗↓mindspore_nn_layer_normalization_BatchNorm2d_construct_420 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_163]() {
  %1(CNode_469) = call @2↓mindspore_nn_layer_normalization_BatchNorm2d_construct_468()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn4-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:154/        if self.use_batch_statistics:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn4-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @✗↓mindspore_nn_layer_normalization_BatchNorm2d_construct_420:CNode_469{[0]: ValueNode<FuncGraph> 2↓mindspore_nn_layer_normalization_BatchNorm2d_construct_468}
#   2: @✗↓mindspore_nn_layer_normalization_BatchNorm2d_construct_420:CNode_470{[0]: ValueNode<Primitive> Return, [1]: CNode_469}


subgraph attr:
subgraph instance: ✓_get_cache_prim_424 : 0x6b334a0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/_primitive_cache.py:35/def _get_cache_prim(cls: Primitive) -> Primitive:/
subgraph @✓_get_cache_prim_424 parent: [subgraph @_get_cache_prim_344]() {
  Return(@_new_prim_for_graph_471)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/_primitive_cache.py:89/        return _new_prim_for_graph/
}
# Order:
#   1: @✓_get_cache_prim_424:CNode_472{[0]: ValueNode<Primitive> Return, [1]: ValueNode<FuncGraph> _new_prim_for_graph_471}


subgraph attr:
training : 1
subgraph instance: L_2✓mindspore_nn_layer_normalization_BatchNorm2d_construct_427 : 0x6907c00
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:138/    def construct(self, x):/
subgraph @L_2✓mindspore_nn_layer_normalization_BatchNorm2d_construct_427 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_251]() {
  %1(CNode_473) = S_Prim_BatchNorm[side_effect_mem: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"], format: "NCHW", is_training: Bool(1), momentum: F32(0.1)](%para133_x, %para134_L_bn1.gamma, %para135_L_bn1.beta, %para136_L_bn1.moving_mean, %para137_L_bn1.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn1-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:142/                return self.bn_train(x,/
  %2(CNode_474) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn1-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:142/                return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn1-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:142/                return self.bn_train(x,/
}
# Order:
#   1: @L_2✓mindspore_nn_layer_normalization_BatchNorm2d_construct_427:CNode_473{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_bn1.gamma, [3]: param_L_bn1.beta, [4]: param_L_bn1.moving_mean, [5]: param_L_bn1.moving_variance}
#   2: @L_2✓mindspore_nn_layer_normalization_BatchNorm2d_construct_427:CNode_474{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_473, [2]: ValueNode<Int64Imm> 0}
#   3: @L_2✓mindspore_nn_layer_normalization_BatchNorm2d_construct_427:CNode_475{[0]: ValueNode<Primitive> Return, [1]: CNode_474}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_↓mindspore_nn_layer_normalization_BatchNorm2d_construct_430 : 0x6a63d20
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:138/    def construct(self, x):/
subgraph @L_↓mindspore_nn_layer_normalization_BatchNorm2d_construct_430 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_251]() {
  %1(CNode_476) = Cond(None, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn1-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:154/        if self.use_batch_statistics:/
  %2(CNode_477) = Switch(%1, @L_✓↓mindspore_nn_layer_normalization_BatchNorm2d_construct_478, @L_✗↓mindspore_nn_layer_normalization_BatchNorm2d_construct_479)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn1-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:154/        if self.use_batch_statistics:/
  %3(CNode_480) = %2()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn1-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:154/        if self.use_batch_statistics:/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn1-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @L_↓mindspore_nn_layer_normalization_BatchNorm2d_construct_430:CNode_476{[0]: ValueNode<Primitive> Cond, [1]: ValueNode<None> None, [2]: ValueNode<BoolImm> false}
#   2: @L_↓mindspore_nn_layer_normalization_BatchNorm2d_construct_430:CNode_477{[0]: ValueNode<Primitive> Switch, [1]: CNode_476, [2]: ValueNode<FuncGraph> L_✓↓mindspore_nn_layer_normalization_BatchNorm2d_construct_478, [3]: ValueNode<FuncGraph> L_✗↓mindspore_nn_layer_normalization_BatchNorm2d_construct_479}
#   3: @L_↓mindspore_nn_layer_normalization_BatchNorm2d_construct_430:CNode_480{[0]: CNode_477}
#   4: @L_↓mindspore_nn_layer_normalization_BatchNorm2d_construct_430:CNode_481{[0]: ValueNode<Primitive> Return, [1]: CNode_480}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: 3✗_apply_adam_436 : 0x692b460
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @3✗_apply_adam_436 parent: [subgraph @_apply_adam_189]() {
  %1(CNode_483) = call @4✗_apply_adam_482()
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:887/                    if self.use_lazy:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:887/                    if self.use_lazy:/
}
# Order:
#   1: @3✗_apply_adam_436:CNode_483{[0]: ValueNode<FuncGraph> 4✗_apply_adam_482}
#   2: @3✗_apply_adam_436:CNode_484{[0]: ValueNode<Primitive> Return, [1]: CNode_483}


subgraph attr:
training : 1
subgraph instance: get_loss_444 : 0x6d8c800
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @get_loss_444(%para144_x, %para145_weights) {
  %1(CNode_486) = call @✓get_loss_485()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:143/        if self.reduce and self.average:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:143/        if self.reduce and self.average:/
}
# Order:
#   1: @get_loss_444:input_dtype{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> dtype}
#   2: @get_loss_444:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Cast, [1]: param_x, [2]: ValueNode<Float> Float32}
#   3: @get_loss_444:weights{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Cast, [1]: param_weights, [2]: ValueNode<Float> Float32}
#   4: @get_loss_444:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Mul, [1]: weights, [2]: x}
#   5: @get_loss_444:CNode_486{[0]: ValueNode<FuncGraph> ✓get_loss_485}
#   6: @get_loss_444:CNode_487{[0]: ValueNode<Primitive> Return, [1]: CNode_486}


subgraph attr:
training : 1
subgraph instance: L_2↓mindspore_nn_layer_basic_Dense_construct_447 : 0x6d712b0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_2↓mindspore_nn_layer_basic_Dense_construct_447 parent: [subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_390]() {
  %1(CNode_108) = call @L_✗2↓mindspore_nn_layer_basic_Dense_construct_488()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:630/        if self.activation_flag:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:630/        if self.activation_flag:/
}
# Order:
#   1: @L_2↓mindspore_nn_layer_basic_Dense_construct_447:CNode_108{[0]: ValueNode<FuncGraph> L_✗2↓mindspore_nn_layer_basic_Dense_construct_488}
#   2: @L_2↓mindspore_nn_layer_basic_Dense_construct_447:CNode_109{[0]: ValueNode<Primitive> Return, [1]: CNode_108}


subgraph attr:
subgraph instance: ✓↓flatten_458 : 0x6d57250
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✓↓flatten_458() {
  %1(CNode_489) = JoinedStr("For 'flatten', both 'start_dim' and 'end_dim' must be int.")
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1735/        raise TypeError(f"For 'flatten', both 'start_dim' and 'end_dim' must be int.")/
  %2(CNode_490) = raise[side_effect_io: Bool(1)]("TypeError", %1, "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1735/        raise TypeError(f"For 'flatten', both 'start_dim' and 'end_dim' must be int.")/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1735/        raise TypeError(f"For 'flatten', both 'start_dim' and 'end_dim' must be int.")/
}
# Order:
#   1: @✓↓flatten_458:CNode_489{[0]: ValueNode<Primitive> JoinedStr, [1]: ValueNode<StringImm> For 'flatten', both 'start_dim' and 'end_dim' must be int.}
#   2: @✓↓flatten_458:CNode_490{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> TypeError, [2]: CNode_489, [3]: ValueNode<StringImm> None}
#   3: @✓↓flatten_458:CNode_491{[0]: ValueNode<Primitive> Return, [1]: CNode_490}


subgraph attr:
subgraph instance: ✗↓flatten_459 : 0x6d0f290
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✗↓flatten_459 parent: [subgraph @flatten_222]() {
  %1(CNode_493) = call @2↓flatten_492()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @✗↓flatten_459:CNode_493{[0]: ValueNode<FuncGraph> 2↓flatten_492}
#   2: @✗↓flatten_459:CNode_494{[0]: ValueNode<Primitive> Return, [1]: CNode_493}


subgraph attr:
subgraph instance: ↰↓flatten_453 : 0x6d0e760
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↰↓flatten_453 parent: [subgraph @↓flatten_408]() {
  %1(CNode_449) = $(↓flatten_408):S_Prim_isinstance(%para129_start_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %2(CNode_450) = $(↓flatten_408):S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @↰↓flatten_453:CNode_495{[0]: ValueNode<Primitive> Return, [1]: CNode_450}


subgraph attr:
subgraph instance: ↱↓flatten_454 : 0x6cfdab0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↱↓flatten_454 parent: [subgraph @flatten_222]() {
  %1(CNode_496) = S_Prim_isinstance(%para130_end_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %2(CNode_497) = S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %3(CNode_498) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %4(CNode_499) = Switch(%3, @↰↱↓flatten_500, @2↱↓flatten_501)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %5(CNode_502) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @↱↓flatten_454:CNode_496{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_end_dim, [2]: ValueNode<ClassType> class 'int'}
#   2: @↱↓flatten_454:CNode_497{[0]: ValueNode<DoSignaturePrimitive> S_Prim_logical_not, [1]: CNode_496}
#   3: @↱↓flatten_454:CNode_498{[0]: ValueNode<Primitive> Cond, [1]: CNode_497, [2]: ValueNode<BoolImm> false}
#   4: @↱↓flatten_454:CNode_499{[0]: ValueNode<Primitive> Switch, [1]: CNode_498, [2]: ValueNode<FuncGraph> ↰↱↓flatten_500, [3]: ValueNode<FuncGraph> 2↱↓flatten_501}
#   5: @↱↓flatten_454:CNode_502{[0]: CNode_499}
#   6: @↱↓flatten_454:CNode_503{[0]: ValueNode<Primitive> Return, [1]: CNode_502}


subgraph attr:
training : 1
subgraph instance: 2↓mindspore_nn_layer_pooling_MaxPool2d_construct_462 : 0x6c3ee50
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @2↓mindspore_nn_layer_pooling_MaxPool2d_construct_462 parent: [subgraph @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_411]() {
  %1(CNode_504) = Cond(%para141_фexpand_batch, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:582/        if expand_batch:/
  %2(CNode_505) = Switch(%1, @✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_506, @✗2↓mindspore_nn_layer_pooling_MaxPool2d_construct_507)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:582/        if expand_batch:/
  %3(CNode_508) = %2()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:582/        if expand_batch:/
  %4(CNode_510) = call @3↓mindspore_nn_layer_pooling_MaxPool2d_construct_509(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:132/        x = self.max_pool2d(self.relu(self.bn4(self.conv4(x))))/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:582/        if expand_batch:/
}
# Order:
#   1: @2↓mindspore_nn_layer_pooling_MaxPool2d_construct_462:CNode_504{[0]: ValueNode<Primitive> Cond, [1]: param_фexpand_batch, [2]: ValueNode<BoolImm> false}
#   2: @2↓mindspore_nn_layer_pooling_MaxPool2d_construct_462:CNode_505{[0]: ValueNode<Primitive> Switch, [1]: CNode_504, [2]: ValueNode<FuncGraph> ✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_506, [3]: ValueNode<FuncGraph> ✗2↓mindspore_nn_layer_pooling_MaxPool2d_construct_507}
#   3: @2↓mindspore_nn_layer_pooling_MaxPool2d_construct_462:CNode_508{[0]: CNode_505}
#   4: @2↓mindspore_nn_layer_pooling_MaxPool2d_construct_462:CNode_510{[0]: ValueNode<FuncGraph> 3↓mindspore_nn_layer_pooling_MaxPool2d_construct_509, [1]: CNode_508}
#   5: @2↓mindspore_nn_layer_pooling_MaxPool2d_construct_462:CNode_511{[0]: ValueNode<Primitive> Return, [1]: CNode_510}


subgraph attr:
after_block : 1
training : 1
subgraph instance: 2↓mindspore_nn_layer_normalization_BatchNorm2d_construct_468 : 0x68eef10
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:138/    def construct(self, x):/
subgraph @2↓mindspore_nn_layer_normalization_BatchNorm2d_construct_468 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_163]() {
  %1(CNode_512) = S_Prim_BatchNorm[output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"], format: "NCHW", is_training: Bool(0), input_names: ["x", "scale", "offset", "mean", "variance"], momentum: F32(0.1), epsilon: F32(1e-05)](%para100_x, %para13_bn4.gamma, %para14_bn4.beta, %para61_bn4.moving_mean, %para62_bn4.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (64), ref_key=:bn4.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:bn4.beta>, <Ref[Tensor[Float32]], (64), ref_key=:bn4.moving_mean>, <Ref[Tensor[Float32]], (64), ref_key=:bn4.moving_variance>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn4-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:161/        return self.bn_infer(x,/
  %2(CNode_513) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn4-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:161/        return self.bn_infer(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn4-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:161/        return self.bn_infer(x,/
}
# Order:
#   1: @2↓mindspore_nn_layer_normalization_BatchNorm2d_construct_468:CNode_512{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_bn4.gamma, [3]: param_bn4.beta, [4]: param_bn4.moving_mean, [5]: param_bn4.moving_variance}
#   2: @2↓mindspore_nn_layer_normalization_BatchNorm2d_construct_468:CNode_513{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_512, [2]: ValueNode<Int64Imm> 0}
#   3: @2↓mindspore_nn_layer_normalization_BatchNorm2d_construct_468:CNode_514{[0]: ValueNode<Primitive> Return, [1]: CNode_513}


subgraph attr:
subgraph instance: _new_prim_for_graph_471 : 0x6a719a0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/_primitive_cache.py:67/    def _new_prim_for_graph(*args, **kwargs) -> Primitive:/
subgraph @_new_prim_for_graph_471 parent: [subgraph @_get_cache_prim_344](%para146_args, %para147_kwargs) {
  %1(CNode_515) = UnpackCall_unpack_call(%para142_cls, %para146_args, %para147_kwargs)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/_primitive_cache.py:68/        return cls(*args, **kwargs)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/_primitive_cache.py:68/        return cls(*args, **kwargs)/
}
# Order:
#   1: @_new_prim_for_graph_471:CNode_515{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.516, [1]: param_cls, [2]: param_args, [3]: param_kwargs}
#   2: @_new_prim_for_graph_471:CNode_517{[0]: ValueNode<Primitive> Return, [1]: CNode_515}


subgraph attr:
training : 1
subgraph instance: L_✓↓mindspore_nn_layer_normalization_BatchNorm2d_construct_478 : 0x6b5d8f0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:138/    def construct(self, x):/
subgraph @L_✓↓mindspore_nn_layer_normalization_BatchNorm2d_construct_478 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_251]() {
  %1(CNode_518) = S_Prim_BatchNorm[side_effect_mem: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"], format: "NCHW", is_training: Bool(1), momentum: F32(0.1)](%para133_x, %para134_L_bn1.gamma, %para135_L_bn1.beta, %para136_L_bn1.moving_mean, %para137_L_bn1.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn1-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:155/            return self.bn_train(x,/
  %2(CNode_519) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn1-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:155/            return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn1-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:155/            return self.bn_train(x,/
}
# Order:
#   1: @L_✓↓mindspore_nn_layer_normalization_BatchNorm2d_construct_478:CNode_518{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_bn1.gamma, [3]: param_L_bn1.beta, [4]: param_L_bn1.moving_mean, [5]: param_L_bn1.moving_variance}
#   2: @L_✓↓mindspore_nn_layer_normalization_BatchNorm2d_construct_478:CNode_519{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_518, [2]: ValueNode<Int64Imm> 0}
#   3: @L_✓↓mindspore_nn_layer_normalization_BatchNorm2d_construct_478:CNode_520{[0]: ValueNode<Primitive> Return, [1]: CNode_519}


subgraph attr:
training : 1
subgraph instance: L_✗↓mindspore_nn_layer_normalization_BatchNorm2d_construct_479 : 0x695e5a0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:138/    def construct(self, x):/
subgraph @L_✗↓mindspore_nn_layer_normalization_BatchNorm2d_construct_479 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_251]() {
  %1(CNode_522) = call @L_2↓mindspore_nn_layer_normalization_BatchNorm2d_construct_521()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn1-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:154/        if self.use_batch_statistics:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn1-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @L_✗↓mindspore_nn_layer_normalization_BatchNorm2d_construct_479:CNode_522{[0]: ValueNode<FuncGraph> L_2↓mindspore_nn_layer_normalization_BatchNorm2d_construct_521}
#   2: @L_✗↓mindspore_nn_layer_normalization_BatchNorm2d_construct_479:CNode_523{[0]: ValueNode<Primitive> Return, [1]: CNode_522}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: 4✗_apply_adam_482 : 0x68b39e0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @4✗_apply_adam_482 parent: [subgraph @_apply_adam_189]() {
  %1(CNode_525) = call @5✗_apply_adam_524()
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:894/                        if self.use_amsgrad:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:894/                        if self.use_amsgrad:/
}
# Order:
#   1: @4✗_apply_adam_482:CNode_525{[0]: ValueNode<FuncGraph> 5✗_apply_adam_524}
#   2: @4✗_apply_adam_482:CNode_526{[0]: ValueNode<Primitive> Return, [1]: CNode_525}


subgraph attr:
training : 1
subgraph instance: ✓get_loss_485 : 0x6d8f140
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @✓get_loss_485 parent: [subgraph @get_loss_444]() {
  %1(CNode_528) = call @↓get_loss_527()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:144/            x = self.reduce_mean(x, self.get_axis(x))/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:144/            x = self.reduce_mean(x, self.get_axis(x))/
}
# Order:
#   1: @✓get_loss_485:CNode_529{[0]: ValueNode<FuncGraph> get_axis_530, [1]: x}
#   2: @✓get_loss_485:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReduceMean, [1]: x, [2]: CNode_529}
#   3: @✓get_loss_485:CNode_528{[0]: ValueNode<FuncGraph> ↓get_loss_527}
#   4: @✓get_loss_485:CNode_531{[0]: ValueNode<Primitive> Return, [1]: CNode_528}


subgraph attr:
training : 1
subgraph instance: L_✗2↓mindspore_nn_layer_basic_Dense_construct_488 : 0x6d726f0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_✗2↓mindspore_nn_layer_basic_Dense_construct_488 parent: [subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_390]() {
  %1(CNode_110) = call @L_3↓mindspore_nn_layer_basic_Dense_construct_532()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:630/        if self.activation_flag:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:630/        if self.activation_flag:/
}
# Order:
#   1: @L_✗2↓mindspore_nn_layer_basic_Dense_construct_488:CNode_110{[0]: ValueNode<FuncGraph> L_3↓mindspore_nn_layer_basic_Dense_construct_532}
#   2: @L_✗2↓mindspore_nn_layer_basic_Dense_construct_488:CNode_111{[0]: ValueNode<Primitive> Return, [1]: CNode_110}


subgraph attr:
after_block : 1
subgraph instance: 2↓flatten_492 : 0x6d11800
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @2↓flatten_492 parent: [subgraph @flatten_222]() {
  %1(CNode_533) = S_Prim_check_flatten_order[constexpr_prim: Bool(1)](%para128_order)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1736/    check_flatten_order_const(order)/
  %2(CNode_534) = StopGradient(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
  %3(CNode_535) = S_Prim_equal(%para128_order, "F")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1737/    if order == 'F':/
  %4(CNode_536) = Cond(%3, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1737/    if order == 'F':/
  %5(CNode_537) = Switch(%4, @✓2↓flatten_538, @✗2↓flatten_539)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1737/    if order == 'F':/
  %6(CNode_540) = %5()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1737/    if order == 'F':/
  %7(CNode_541) = Depend[side_effect_propagate: I64(1)](%6, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1737/    if order == 'F':/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1737/    if order == 'F':/
}
# Order:
#   1: @2↓flatten_492:CNode_533{[0]: ValueNode<DoSignaturePrimitive> S_Prim_check_flatten_order, [1]: param_order}
#   2: @2↓flatten_492:CNode_535{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: param_order, [2]: ValueNode<StringImm> F}
#   3: @2↓flatten_492:CNode_536{[0]: ValueNode<Primitive> Cond, [1]: CNode_535, [2]: ValueNode<BoolImm> false}
#   4: @2↓flatten_492:CNode_537{[0]: ValueNode<Primitive> Switch, [1]: CNode_536, [2]: ValueNode<FuncGraph> ✓2↓flatten_538, [3]: ValueNode<FuncGraph> ✗2↓flatten_539}
#   5: @2↓flatten_492:CNode_540{[0]: CNode_537}
#   6: @2↓flatten_492:CNode_542{[0]: ValueNode<Primitive> Return, [1]: CNode_541}


subgraph attr:
subgraph instance: ↰↱↓flatten_500 : 0x6cd4850
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↰↱↓flatten_500 parent: [subgraph @↱↓flatten_454]() {
  %1(CNode_496) = $(↱↓flatten_454):S_Prim_isinstance(%para130_end_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %2(CNode_497) = $(↱↓flatten_454):S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
}
# Order:
#   1: @↰↱↓flatten_500:CNode_543{[0]: ValueNode<Primitive> Return, [1]: CNode_497}


subgraph attr:
subgraph instance: 2↱↓flatten_501 : 0x6cb65b0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @2↱↓flatten_501 parent: [subgraph @flatten_222]() {
  %1(CNode_544) = S_Prim_isinstance(%para129_start_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %2(CNode_545) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %3(CNode_546) = Switch(%2, @↰2↱↓flatten_547, @3↱↓flatten_548)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %4(CNode_549) = %3()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @2↱↓flatten_501:CNode_544{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_start_dim, [2]: ValueNode<ClassType> class 'bool'}
#   2: @2↱↓flatten_501:CNode_545{[0]: ValueNode<Primitive> Cond, [1]: CNode_544, [2]: ValueNode<BoolImm> false}
#   3: @2↱↓flatten_501:CNode_546{[0]: ValueNode<Primitive> Switch, [1]: CNode_545, [2]: ValueNode<FuncGraph> ↰2↱↓flatten_547, [3]: ValueNode<FuncGraph> 3↱↓flatten_548}
#   4: @2↱↓flatten_501:CNode_549{[0]: CNode_546}
#   5: @2↱↓flatten_501:CNode_550{[0]: ValueNode<Primitive> Return, [1]: CNode_549}


subgraph attr:
after_block : 1
training : 1
subgraph instance: 3↓mindspore_nn_layer_pooling_MaxPool2d_construct_509 : 0x6cc9ef0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @3↓mindspore_nn_layer_pooling_MaxPool2d_construct_509(%para148_) {
  %1(CNode_552) = call @✗3↓mindspore_nn_layer_pooling_MaxPool2d_construct_551()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:587/        if self.use_pad and not self.return_indices:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:587/        if self.use_pad and not self.return_indices:/
}
# Order:
#   1: @3↓mindspore_nn_layer_pooling_MaxPool2d_construct_509:CNode_552{[0]: ValueNode<FuncGraph> ✗3↓mindspore_nn_layer_pooling_MaxPool2d_construct_551}
#   2: @3↓mindspore_nn_layer_pooling_MaxPool2d_construct_509:CNode_553{[0]: ValueNode<Primitive> Return, [1]: CNode_552}


subgraph attr:
training : 1
subgraph instance: ✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_506 : 0x6925e90
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_506 parent: [subgraph @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_411]() {
  %1(out) = $(✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_411):S_Prim_MaxPool[pad_mode: I64(2), output_names: ["output"], kernel_size: (I64(1), I64(1), I64(2), I64(2)), format: "NCHW", strides: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"]](%para140_фx)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_554) = S_Prim_isinstance(%1, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:583/            if isinstance(out, tuple):/
  %3(CNode_555) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:583/            if isinstance(out, tuple):/
  %4(CNode_556) = Switch(%3, @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_557, @✗✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_558)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:583/            if isinstance(out, tuple):/
  %5(CNode_559) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:583/            if isinstance(out, tuple):/
  %6(CNode_561) = call @↓✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_560(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:132/        x = self.max_pool2d(self.relu(self.bn4(self.conv4(x))))/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:583/            if isinstance(out, tuple):/
}
# Order:
#   1: @✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_506:CNode_554{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: out, [2]: ValueNode<ClassType> class 'tuple'}
#   2: @✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_506:CNode_555{[0]: ValueNode<Primitive> Cond, [1]: CNode_554, [2]: ValueNode<BoolImm> false}
#   3: @✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_506:CNode_556{[0]: ValueNode<Primitive> Switch, [1]: CNode_555, [2]: ValueNode<FuncGraph> 2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_557, [3]: ValueNode<FuncGraph> ✗✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_558}
#   4: @✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_506:CNode_559{[0]: CNode_556}
#   5: @✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_506:CNode_561{[0]: ValueNode<FuncGraph> ↓✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_560, [1]: CNode_559}
#   6: @✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_506:CNode_562{[0]: ValueNode<Primitive> Return, [1]: CNode_561}


subgraph attr:
training : 1
subgraph instance: ✗2↓mindspore_nn_layer_pooling_MaxPool2d_construct_507 : 0x68ffd50
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @✗2↓mindspore_nn_layer_pooling_MaxPool2d_construct_507 parent: [subgraph @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_411]() {
  %1(out) = $(✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_411):S_Prim_MaxPool[pad_mode: I64(2), output_names: ["output"], kernel_size: (I64(1), I64(1), I64(2), I64(2)), format: "NCHW", strides: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"]](%para140_фx)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:581/            out = self.max_pool(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:582/        if expand_batch:/
}
# Order:
#   1: @✗2↓mindspore_nn_layer_pooling_MaxPool2d_construct_507:CNode_563{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_2↓mindspore_nn_layer_normalization_BatchNorm2d_construct_521 : 0x6c2b290
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:138/    def construct(self, x):/
subgraph @L_2↓mindspore_nn_layer_normalization_BatchNorm2d_construct_521 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_251]() {
  %1(CNode_564) = S_Prim_BatchNorm[output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"], format: "NCHW", is_training: Bool(0), input_names: ["x", "scale", "offset", "mean", "variance"], momentum: F32(0.1), epsilon: F32(1e-05)](%para133_x, %para134_L_bn1.gamma, %para135_L_bn1.beta, %para136_L_bn1.moving_mean, %para137_L_bn1.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn1-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:161/        return self.bn_infer(x,/
  %2(CNode_565) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn1-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:161/        return self.bn_infer(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/bn1-BatchNorm2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/normalization.py:161/        return self.bn_infer(x,/
}
# Order:
#   1: @L_2↓mindspore_nn_layer_normalization_BatchNorm2d_construct_521:CNode_564{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_bn1.gamma, [3]: param_L_bn1.beta, [4]: param_L_bn1.moving_mean, [5]: param_L_bn1.moving_variance}
#   2: @L_2↓mindspore_nn_layer_normalization_BatchNorm2d_construct_521:CNode_565{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_564, [2]: ValueNode<Int64Imm> 0}
#   3: @L_2↓mindspore_nn_layer_normalization_BatchNorm2d_construct_521:CNode_566{[0]: ValueNode<Primitive> Return, [1]: CNode_565}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: 5✗_apply_adam_524 : 0x69f2e50
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @5✗_apply_adam_524 parent: [subgraph @_apply_adam_189]() {
  %1(CNode_568) = call @↓4✗_apply_adam_567()
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:901/                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt, self._ps_push,/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:901/                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt, self._ps_push,/
}
# Order:
#   1: @5✗_apply_adam_524:CNode_569{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Partial, [1]: ValueNode<DoSignaturePrimitive> S_Prim_adam_opt, [2]: ValueNode<DoSignaturePrimitive> S_Prim_Adam, [3]: ValueNode<DoSignaturePrimitive> S_Prim_FusedSparseAdam, [4]: ValueNode<DoSignaturePrimitive> S_Prim_Push, [5]: ValueNode<DoSignaturePrimitive> S_Prim_Pull, [6]: ValueNode<BoolImm> false, [7]: ValueNode<BoolImm> false, [8]: ValueNode<BoolImm> true, [9]: param_beta1_power, [10]: param_beta2_power, [11]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=0.9), [12]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=0.999), [13]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=1e-08), [14]: param_lr}
#   2: @5✗_apply_adam_524:success{[0]: ValueNode<DoSignaturePrimitive> S_Prim_map, [1]: CNode_569, [2]: param_gradients, [3]: param_params, [4]: param_moment1, [5]: param_moment2, [6]: ValueNode<ValueTuple> (false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false), [7]: ValueNode<ValueTuple> (false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false)}
#   3: @5✗_apply_adam_524:CNode_568{[0]: ValueNode<FuncGraph> ↓4✗_apply_adam_567}
#   4: @5✗_apply_adam_524:CNode_570{[0]: ValueNode<Primitive> Return, [1]: CNode_568}


subgraph attr:
training : 1
subgraph instance: get_axis_530 : 0x6d90e00
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:113/    def get_axis(self, x):/
subgraph @get_axis_530(%para149_x) {
  %1(shape) = call @shape_386(%para149_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:120/        shape = F.shape(x)/
  %2(length) = S_Prim_sequence_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:121/        length = F.tuple_len(shape)/
  %3(perm) = S_Prim_make_range(I64(0), %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:122/        perm = F.make_range(0, length)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:123/        return perm/
}
# Order:
#   1: @get_axis_530:shape{[0]: ValueNode<FuncGraph> shape_386, [1]: param_x}
#   2: @get_axis_530:length{[0]: ValueNode<DoSignaturePrimitive> S_Prim_sequence_len, [1]: shape}
#   3: @get_axis_530:perm{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_range, [1]: ValueNode<Int64Imm> 0, [2]: length}
#   4: @get_axis_530:CNode_571{[0]: ValueNode<Primitive> Return, [1]: perm}


subgraph attr:
training : 1
subgraph instance: ↓get_loss_527 : 0x6d929b0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @↓get_loss_527 parent: [subgraph @✓get_loss_485]() {
  %1(CNode_573) = call @✗↓get_loss_572()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:145/        if self.reduce and not self.average:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:145/        if self.reduce and not self.average:/
}
# Order:
#   1: @↓get_loss_527:CNode_573{[0]: ValueNode<FuncGraph> ✗↓get_loss_572}
#   2: @↓get_loss_527:CNode_574{[0]: ValueNode<Primitive> Return, [1]: CNode_573}


subgraph attr:
training : 1
subgraph instance: L_3↓mindspore_nn_layer_basic_Dense_construct_532 : 0x6d73b30
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_3↓mindspore_nn_layer_basic_Dense_construct_532 parent: [subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_390]() {
  %1(x_shape) = $(L_mindspore_nn_layer_basic_Dense_construct_200):S_Prim_Shape(%para121_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:623/        x_shape = self.shape_op(x)/
  %2(CNode_112) = S_Prim_inner_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
  %3(CNode_113) = S_Prim_not_equal(%2, I64(2))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
  %4(CNode_114) = Cond(%3, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
  %5(CNode_115) = Switch(%4, @L_✓3↓mindspore_nn_layer_basic_Dense_construct_575, @L_✗3↓mindspore_nn_layer_basic_Dense_construct_576)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
  %6(CNode_117) = %5()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
  %7(CNode_119) = call @L_4↓mindspore_nn_layer_basic_Dense_construct_577(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /home/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:137/        x = self.fc3(x)/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_3↓mindspore_nn_layer_basic_Dense_construct_532:CNode_112{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: x_shape}
#   2: @L_3↓mindspore_nn_layer_basic_Dense_construct_532:CNode_113{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: CNode_112, [2]: ValueNode<Int64Imm> 2}
#   3: @L_3↓mindspore_nn_layer_basic_Dense_construct_532:CNode_114{[0]: ValueNode<Primitive> Cond, [1]: CNode_113, [2]: ValueNode<BoolImm> false}
#   4: @L_3↓mindspore_nn_layer_basic_Dense_construct_532:CNode_115{[0]: ValueNode<Primitive> Switch, [1]: CNode_114, [2]: ValueNode<FuncGraph> L_✓3↓mindspore_nn_layer_basic_Dense_construct_575, [3]: ValueNode<FuncGraph> L_✗3↓mindspore_nn_layer_basic_Dense_construct_576}
#   5: @L_3↓mindspore_nn_layer_basic_Dense_construct_532:CNode_117{[0]: CNode_115}
#   6: @L_3↓mindspore_nn_layer_basic_Dense_construct_532:CNode_119{[0]: ValueNode<FuncGraph> L_4↓mindspore_nn_layer_basic_Dense_construct_577, [1]: CNode_117}
#   7: @L_3↓mindspore_nn_layer_basic_Dense_construct_532:CNode_120{[0]: ValueNode<Primitive> Return, [1]: CNode_119}


subgraph attr:
subgraph instance: ✓2↓flatten_538 : 0x6d4d310
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✓2↓flatten_538 parent: [subgraph @flatten_222]() {
  %1(x_rank) = S_Prim_Rank(%para127_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1738/        x_rank = rank_(input)/
  %2(CNode_578) = S_Prim_MakeTuple(I64(0), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1740/        if x_rank in (0, 1):/
  %3(CNode_579) = S_Prim_in(%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1740/        if x_rank in (0, 1):/
  %4(CNode_580) = Cond(%3, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1740/        if x_rank in (0, 1):/
  %5(CNode_581) = Switch(%4, @2✓2↓flatten_582, @✗✓2↓flatten_583)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1740/        if x_rank in (0, 1):/
  %6(CNode_584) = %5()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1740/        if x_rank in (0, 1):/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1740/        if x_rank in (0, 1):/
}
# Order:
#   1: @✓2↓flatten_538:x_rank{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Rank, [1]: param_input}
#   2: @✓2↓flatten_538:CNode_578{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<Int64Imm> 0, [2]: ValueNode<Int64Imm> 1}
#   3: @✓2↓flatten_538:CNode_579{[0]: ValueNode<DoSignaturePrimitive> S_Prim_in, [1]: x_rank, [2]: CNode_578}
#   4: @✓2↓flatten_538:CNode_580{[0]: ValueNode<Primitive> Cond, [1]: CNode_579, [2]: ValueNode<BoolImm> false}
#   5: @✓2↓flatten_538:CNode_581{[0]: ValueNode<Primitive> Switch, [1]: CNode_580, [2]: ValueNode<FuncGraph> 2✓2↓flatten_582, [3]: ValueNode<FuncGraph> ✗✓2↓flatten_583}
#   6: @✓2↓flatten_538:CNode_584{[0]: CNode_581}
#   7: @✓2↓flatten_538:CNode_585{[0]: ValueNode<Primitive> Return, [1]: CNode_584}


subgraph attr:
subgraph instance: ✗2↓flatten_539 : 0x6d14dc0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✗2↓flatten_539 parent: [subgraph @flatten_222]() {
  %1(CNode_587) = call @3↓flatten_586(%para127_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1737/    if order == 'F':/
}
# Order:
#   1: @✗2↓flatten_539:CNode_588{[0]: ValueNode<Primitive> Return, [1]: CNode_587}
#   2: @✗2↓flatten_539:CNode_587{[0]: ValueNode<FuncGraph> 3↓flatten_586, [1]: param_input}


subgraph attr:
subgraph instance: ↰2↱↓flatten_547 : 0x6cd39c0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↰2↱↓flatten_547 parent: [subgraph @2↱↓flatten_501]() {
  %1(CNode_544) = $(2↱↓flatten_501):S_Prim_isinstance(%para129_start_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
}
# Order:
#   1: @↰2↱↓flatten_547:CNode_589{[0]: ValueNode<Primitive> Return, [1]: CNode_544}


subgraph attr:
subgraph instance: 3↱↓flatten_548 : 0x6cd2910
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @3↱↓flatten_548 parent: [subgraph @flatten_222]() {
  %1(CNode_590) = S_Prim_isinstance(%para130_end_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @3↱↓flatten_548:CNode_590{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_end_dim, [2]: ValueNode<ClassType> class 'bool'}
#   2: @3↱↓flatten_548:CNode_591{[0]: ValueNode<Primitive> Return, [1]: CNode_590}


subgraph attr:
training : 1
subgraph instance: ✗3↓mindspore_nn_layer_pooling_MaxPool2d_construct_551 : 0x6cdff40
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @✗3↓mindspore_nn_layer_pooling_MaxPool2d_construct_551 parent: [subgraph @3↓mindspore_nn_layer_pooling_MaxPool2d_construct_509]() {
  %1(CNode_593) = call @4↓mindspore_nn_layer_pooling_MaxPool2d_construct_592()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:587/        if self.use_pad and not self.return_indices:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:587/        if self.use_pad and not self.return_indices:/
}
# Order:
#   1: @✗3↓mindspore_nn_layer_pooling_MaxPool2d_construct_551:CNode_593{[0]: ValueNode<FuncGraph> 4↓mindspore_nn_layer_pooling_MaxPool2d_construct_592}
#   2: @✗3↓mindspore_nn_layer_pooling_MaxPool2d_construct_551:CNode_594{[0]: ValueNode<Primitive> Return, [1]: CNode_593}


subgraph attr:
after_block : 1
training : 1
subgraph instance: ↓✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_560 : 0x6cc8db0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @↓✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_560(%para150_) {
  Return(%para150_фout)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:583/            if isinstance(out, tuple):/
}
# Order:
#   1: @↓✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_560:CNode_595{[0]: ValueNode<Primitive> Return, [1]: param_фout}


subgraph attr:
training : 1
subgraph instance: 2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_557 : 0x6cc6ad0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_557 parent: [subgraph @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_411]() {
  %1(out) = $(✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_411):S_Prim_MaxPool[pad_mode: I64(2), output_names: ["output"], kernel_size: (I64(1), I64(1), I64(2), I64(2)), format: "NCHW", strides: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"]](%para140_фx)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_596) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %3(CNode_597) = getattr(%2, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %4(CNode_598) = %3(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %5(CNode_599) = S_Prim_getitem(%1, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %6(CNode_600) = getattr(%5, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %7(CNode_601) = %6(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %8(out) = S_Prim_MakeTuple(%4, %7)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
}
# Order:
#   1: @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_557:CNode_596{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: out, [2]: ValueNode<Int64Imm> 0}
#   2: @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_557:CNode_597{[0]: ValueNode<Primitive> getattr, [1]: CNode_596, [2]: ValueNode<StringImm> squeeze}
#   3: @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_557:CNode_598{[0]: CNode_597, [1]: ValueNode<Int64Imm> 0}
#   4: @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_557:CNode_599{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: out, [2]: ValueNode<Int64Imm> 1}
#   5: @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_557:CNode_600{[0]: ValueNode<Primitive> getattr, [1]: CNode_599, [2]: ValueNode<StringImm> squeeze}
#   6: @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_557:CNode_601{[0]: CNode_600, [1]: ValueNode<Int64Imm> 0}
#   7: @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_557:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_598, [2]: CNode_601}
#   8: @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_557:CNode_602{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: ✗✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_558 : 0x6cc5790
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @✗✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_558 parent: [subgraph @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_411]() {
  %1(out) = $(✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_411):S_Prim_MaxPool[pad_mode: I64(2), output_names: ["output"], kernel_size: (I64(1), I64(1), I64(2), I64(2)), format: "NCHW", strides: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"]](%para140_фx)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_603) = getattr(%1, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:586/                out = out.squeeze(0)/
  %3(out) = %2(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:586/                out = out.squeeze(0)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:586/                out = out.squeeze(0)/
}
# Order:
#   1: @✗✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_558:CNode_603{[0]: ValueNode<Primitive> getattr, [1]: out, [2]: ValueNode<StringImm> squeeze}
#   2: @✗✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_558:out{[0]: CNode_603, [1]: ValueNode<Int64Imm> 0}
#   3: @✗✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_558:CNode_604{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ↓4✗_apply_adam_567 : 0x69532a0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @↓4✗_apply_adam_567 parent: [subgraph @5✗_apply_adam_524]() {
  %1(CNode_606) = call @↓3✗_apply_adam_605()
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:894/                        if self.use_amsgrad:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:894/                        if self.use_amsgrad:/
}
# Order:
#   1: @↓4✗_apply_adam_567:CNode_606{[0]: ValueNode<FuncGraph> ↓3✗_apply_adam_605}
#   2: @↓4✗_apply_adam_567:CNode_607{[0]: ValueNode<Primitive> Return, [1]: CNode_606}


subgraph attr:
training : 1
subgraph instance: ✗↓get_loss_572 : 0x6d93d90
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @✗↓get_loss_572 parent: [subgraph @✓get_loss_485]() {
  %1(CNode_609) = call @2↓get_loss_608()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:145/        if self.reduce and not self.average:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:145/        if self.reduce and not self.average:/
}
# Order:
#   1: @✗↓get_loss_572:CNode_609{[0]: ValueNode<FuncGraph> 2↓get_loss_608}
#   2: @✗↓get_loss_572:CNode_610{[0]: ValueNode<Primitive> Return, [1]: CNode_609}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_4↓mindspore_nn_layer_basic_Dense_construct_577 : 0x6d7af30
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_4↓mindspore_nn_layer_basic_Dense_construct_577(%para151_) {
  Return(%para151_фx)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:635/        return x/
}
# Order:
#   1: @L_4↓mindspore_nn_layer_basic_Dense_construct_577:CNode_611{[0]: ValueNode<Primitive> Return, [1]: param_фx}


subgraph attr:
training : 1
subgraph instance: L_✓3↓mindspore_nn_layer_basic_Dense_construct_575 : 0x6d76de0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_✓3↓mindspore_nn_layer_basic_Dense_construct_575 parent: [subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_390]() {
  %1(x) = $(L_↓mindspore_nn_layer_basic_Dense_construct_303):S_Prim_MatMul[output_names: ["output"], transpose_a: Bool(0), input_names: ["x1", "x2"], transpose_x2: Bool(1), transpose_x1: Bool(0), transpose_b: Bool(1)](%para139_фx, %para123_L_fc3.weight)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:627/        x = self.matmul(x, self.weight)/
  %2(x) = $(L_✓↓mindspore_nn_layer_basic_Dense_construct_390):S_Prim_BiasAdd[output_names: ["output"], format: "NCHW", input_names: ["x", "b"]](%1, %para122_L_fc3.bias)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:629/            x = self.bias_add(x, self.bias)/
  %3(x_shape) = $(L_mindspore_nn_layer_basic_Dense_construct_200):S_Prim_Shape(%para121_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:623/        x_shape = self.shape_op(x)/
  %4(CNode_612) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %5(CNode_613) = S_Prim_make_slice(None, %4, None)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %6(CNode_614) = S_Prim_getitem(%3, %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %7(CNode_616) = call @L_shape_615(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %8(CNode_617) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %9(CNode_618) = S_Prim_getitem(%7, %8)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %10(CNode_619) = S_Prim_MakeTuple(%9)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %11(out_shape) = S_Prim_add(%6, %10)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %12(x) = S_Prim_Reshape[output_names: ["output"], input_names: ["tensor", "shape"]](%2, %11)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:634/            x = self.reshape(x, out_shape)/
  Return(%12)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
}
# Order:
#   1: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_575:CNode_612{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_575:CNode_613{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_slice, [1]: ValueNode<None> None, [2]: CNode_612, [3]: ValueNode<None> None}
#   3: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_575:CNode_614{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: CNode_613}
#   4: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_575:CNode_616{[0]: ValueNode<FuncGraph> L_shape_615, [1]: x}
#   5: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_575:CNode_617{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   6: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_575:CNode_618{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_616, [2]: CNode_617}
#   7: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_575:CNode_619{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_618}
#   8: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_575:out_shape{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: CNode_614, [2]: CNode_619}
#   9: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_575:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: x, [2]: out_shape}
#  10: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_575:CNode_620{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: L_✗3↓mindspore_nn_layer_basic_Dense_construct_576 : 0x6d75e80
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_✗3↓mindspore_nn_layer_basic_Dense_construct_576 parent: [subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_390]() {
  %1(x) = $(L_↓mindspore_nn_layer_basic_Dense_construct_303):S_Prim_MatMul[output_names: ["output"], transpose_a: Bool(0), input_names: ["x1", "x2"], transpose_x2: Bool(1), transpose_x1: Bool(0), transpose_b: Bool(1)](%para139_фx, %para123_L_fc3.weight)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:627/        x = self.matmul(x, self.weight)/
  %2(x) = $(L_✓↓mindspore_nn_layer_basic_Dense_construct_390):S_Prim_BiasAdd[output_names: ["output"], format: "NCHW", input_names: ["x", "b"]](%1, %para122_L_fc3.bias)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:629/            x = self.bias_add(x, self.bias)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_✗3↓mindspore_nn_layer_basic_Dense_construct_576:CNode_121{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
subgraph instance: 2✓2↓flatten_582 : 0x6d55ed0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @2✓2↓flatten_582 parent: [subgraph @flatten_222]() {
  %1(CNode_621) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1741/            return reshape_(input, (-1,))/
  %2(CNode_622) = S_Prim_MakeTuple(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1741/            return reshape_(input, (-1,))/
  %3(CNode_623) = S_Prim_Reshape[output_names: ["output"], input_names: ["tensor", "shape"]](%para127_input, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1741/            return reshape_(input, (-1,))/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1741/            return reshape_(input, (-1,))/
}
# Order:
#   1: @2✓2↓flatten_582:CNode_621{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @2✓2↓flatten_582:CNode_622{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_621}
#   3: @2✓2↓flatten_582:CNode_623{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_input, [2]: CNode_622}
#   4: @2✓2↓flatten_582:CNode_624{[0]: ValueNode<Primitive> Return, [1]: CNode_623}


subgraph attr:
subgraph instance: ✗✓2↓flatten_583 : 0x6d50ba0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✗✓2↓flatten_583 parent: [subgraph @✓2↓flatten_538]() {
  %1(CNode_626) = call @↓✓2↓flatten_625()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1740/        if x_rank in (0, 1):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1740/        if x_rank in (0, 1):/
}
# Order:
#   1: @✗✓2↓flatten_583:CNode_626{[0]: ValueNode<FuncGraph> ↓✓2↓flatten_625}
#   2: @✗✓2↓flatten_583:CNode_627{[0]: ValueNode<Primitive> Return, [1]: CNode_626}


subgraph attr:
after_block : 1
subgraph instance: 3↓flatten_586 : 0x6d16e40
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @3↓flatten_586 parent: [subgraph @flatten_222](%para152_) {
  %1(CNode_628) = S_Prim_equal(%para129_start_dim, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %2(CNode_629) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %3(CNode_630) = Switch(%2, @↰3↓flatten_631, @↱3↓flatten_632)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %4(CNode_633) = %3()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %5(CNode_634) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %6(CNode_635) = Switch(%5, @✓3↓flatten_636, @✗3↓flatten_637)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %7(CNode_638) = %6()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
}
# Order:
#   1: @3↓flatten_586:x_shape{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_фinput}
#   2: @3↓flatten_586:x_rank{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Rank, [1]: param_фinput}
#   3: @3↓flatten_586:CNode_628{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: param_start_dim, [2]: ValueNode<Int64Imm> 1}
#   4: @3↓flatten_586:CNode_629{[0]: ValueNode<Primitive> Cond, [1]: CNode_628, [2]: ValueNode<BoolImm> false}
#   5: @3↓flatten_586:CNode_630{[0]: ValueNode<Primitive> Switch, [1]: CNode_629, [2]: ValueNode<FuncGraph> ↰3↓flatten_631, [3]: ValueNode<FuncGraph> ↱3↓flatten_632}
#   6: @3↓flatten_586:CNode_633{[0]: CNode_630}
#   7: @3↓flatten_586:CNode_634{[0]: ValueNode<Primitive> Cond, [1]: CNode_633, [2]: ValueNode<BoolImm> false}
#   8: @3↓flatten_586:CNode_635{[0]: ValueNode<Primitive> Switch, [1]: CNode_634, [2]: ValueNode<FuncGraph> ✓3↓flatten_636, [3]: ValueNode<FuncGraph> ✗3↓flatten_637}
#   9: @3↓flatten_586:CNode_638{[0]: CNode_635}
#  10: @3↓flatten_586:CNode_639{[0]: ValueNode<Primitive> Return, [1]: CNode_638}


subgraph attr:
training : 1
subgraph instance: 4↓mindspore_nn_layer_pooling_MaxPool2d_construct_592 : 0x6ce0ef0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @4↓mindspore_nn_layer_pooling_MaxPool2d_construct_592 parent: [subgraph @3↓mindspore_nn_layer_pooling_MaxPool2d_construct_509]() {
  Return(%para148_фout)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:589/        return out/
}
# Order:
#   1: @4↓mindspore_nn_layer_pooling_MaxPool2d_construct_592:CNode_640{[0]: ValueNode<Primitive> Return, [1]: param_фout}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ↓3✗_apply_adam_605 : 0x69157b0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @↓3✗_apply_adam_605 parent: [subgraph @5✗_apply_adam_524]() {
  %1(CNode_642) = call @↓2✗_apply_adam_641()
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:887/                    if self.use_lazy:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:887/                    if self.use_lazy:/
}
# Order:
#   1: @↓3✗_apply_adam_605:CNode_642{[0]: ValueNode<FuncGraph> ↓2✗_apply_adam_641}
#   2: @↓3✗_apply_adam_605:CNode_643{[0]: ValueNode<Primitive> Return, [1]: CNode_642}


subgraph attr:
training : 1
subgraph instance: 2↓get_loss_608 : 0x6d95140
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @2↓get_loss_608 parent: [subgraph @✓get_loss_485]() {
  %1(weights) = $(get_loss_444):S_Prim_Cast[output_names: ["output"], input_names: ["x", "dst_type"]](%para145_weights, F32)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:141/        weights = self.cast(weights, mstype.float32)/
  %2(x) = $(get_loss_444):S_Prim_Cast[output_names: ["output"], input_names: ["x", "dst_type"]](%para144_x, F32)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:140/        x = self.cast(x, mstype.float32)/
  %3(x) = $(get_loss_444):S_Prim_Mul[output_names: ["output"], input_names: ["x", "y"]](%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:142/        x = self.mul(weights, x)/
  %4(CNode_529) = $(✓get_loss_485):call @get_axis_530(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:144/            x = self.reduce_mean(x, self.get_axis(x))/
  %5(x) = $(✓get_loss_485):S_Prim_ReduceMean[output_names: ["y"], keep_dims: Bool(0), input_names: ["input_x", "axis"]](%3, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:144/            x = self.reduce_mean(x, self.get_axis(x))/
  %6(input_dtype) = $(get_loss_444):getattr(%para144_x, "dtype")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:139/        input_dtype = x.dtype/
  %7(x) = S_Prim_Cast[output_names: ["output"], input_names: ["x", "dst_type"]](%5, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:147/        x = self.cast(x, input_dtype)/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:148/        return x/
}
# Order:
#   1: @2↓get_loss_608:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Cast, [1]: x, [2]: input_dtype}
#   2: @2↓get_loss_608:CNode_644{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
subgraph instance: L_shape_615 : 0x6d79bd0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1484/def shape(input_x):/
subgraph @L_shape_615(%para153_input_x) {
  %1(CNode_441) = S_Prim_Shape(%para153_input_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1510/    return shape_(input_x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1510/    return shape_(input_x)/
}
# Order:
#   1: @L_shape_615:CNode_441{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_input_x}
#   2: @L_shape_615:CNode_442{[0]: ValueNode<Primitive> Return, [1]: CNode_441}


subgraph attr:
after_block : 1
subgraph instance: ↓✓2↓flatten_625 : 0x6d52d00
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↓✓2↓flatten_625 parent: [subgraph @✓2↓flatten_538]() {
  %1(CNode_645) = call @_get_cache_prim_344(ClassType)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1744/        input = _get_cache_prim(P.Transpose)()(input, new_order)/
  %2(CNode_646) = %1()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1744/        input = _get_cache_prim(P.Transpose)()(input, new_order)/
  %3(x_rank) = $(✓2↓flatten_538):S_Prim_Rank(%para127_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1738/        x_rank = rank_(input)/
  %4(perm) = S_Prim_make_range(I64(0), %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1742/        perm = ops.make_range(0, x_rank)/
  %5(new_order) = S_Prim_tuple_reversed(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1743/        new_order = ops.tuple_reversed(perm)/
  %6(input) = %2(%para127_input, %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1744/        input = _get_cache_prim(P.Transpose)()(input, new_order)/
  %7(CNode_647) = call @3↓flatten_586(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1738/        x_rank = rank_(input)/
}
# Order:
#   1: @↓✓2↓flatten_625:perm{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_range, [1]: ValueNode<Int64Imm> 0, [2]: x_rank}
#   2: @↓✓2↓flatten_625:new_order{[0]: ValueNode<DoSignaturePrimitive> S_Prim_tuple_reversed, [1]: perm}
#   3: @↓✓2↓flatten_625:CNode_645{[0]: ValueNode<FuncGraph> _get_cache_prim_344, [1]: ValueNode<ClassType> class 'mindspore.ops.operations.array_ops.Transpose'}
#   4: @↓✓2↓flatten_625:CNode_646{[0]: CNode_645}
#   5: @↓✓2↓flatten_625:input{[0]: CNode_646, [1]: param_input, [2]: new_order}
#   6: @↓✓2↓flatten_625:CNode_648{[0]: ValueNode<Primitive> Return, [1]: CNode_647}
#   7: @↓✓2↓flatten_625:CNode_647{[0]: ValueNode<FuncGraph> 3↓flatten_586, [1]: input}


subgraph attr:
subgraph instance: ✓3↓flatten_636 : 0x6d46d10
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✓3↓flatten_636 parent: [subgraph @3↓flatten_586]() {
  %1(x_rank) = $(3↓flatten_586):S_Prim_Rank(%para152_фinput)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1748/    x_rank = rank_(input)/
  %2(CNode_649) = S_Prim_MakeTuple(I64(0), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1750/        if x_rank in (0, 1):/
  %3(CNode_650) = S_Prim_in(%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1750/        if x_rank in (0, 1):/
  %4(CNode_651) = Cond(%3, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1750/        if x_rank in (0, 1):/
  %5(CNode_652) = Switch(%4, @2✓3↓flatten_653, @✗✓3↓flatten_654)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1750/        if x_rank in (0, 1):/
  %6(CNode_655) = %5()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1750/        if x_rank in (0, 1):/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1750/        if x_rank in (0, 1):/
}
# Order:
#   1: @✓3↓flatten_636:CNode_649{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<Int64Imm> 0, [2]: ValueNode<Int64Imm> 1}
#   2: @✓3↓flatten_636:CNode_650{[0]: ValueNode<DoSignaturePrimitive> S_Prim_in, [1]: x_rank, [2]: CNode_649}
#   3: @✓3↓flatten_636:CNode_651{[0]: ValueNode<Primitive> Cond, [1]: CNode_650, [2]: ValueNode<BoolImm> false}
#   4: @✓3↓flatten_636:CNode_652{[0]: ValueNode<Primitive> Switch, [1]: CNode_651, [2]: ValueNode<FuncGraph> 2✓3↓flatten_653, [3]: ValueNode<FuncGraph> ✗✓3↓flatten_654}
#   5: @✓3↓flatten_636:CNode_655{[0]: CNode_652}
#   6: @✓3↓flatten_636:CNode_656{[0]: ValueNode<Primitive> Return, [1]: CNode_655}


subgraph attr:
subgraph instance: ✗3↓flatten_637 : 0x6d1cfa0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✗3↓flatten_637 parent: [subgraph @3↓flatten_586]() {
  %1(CNode_658) = call @4↓flatten_657()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
}
# Order:
#   1: @✗3↓flatten_637:CNode_658{[0]: ValueNode<FuncGraph> 4↓flatten_657}
#   2: @✗3↓flatten_637:CNode_659{[0]: ValueNode<Primitive> Return, [1]: CNode_658}


subgraph attr:
subgraph instance: ↰3↓flatten_631 : 0x6d1bb10
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↰3↓flatten_631 parent: [subgraph @flatten_222]() {
  %1(CNode_660) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %2(CNode_661) = S_Prim_equal(%para130_end_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
}
# Order:
#   1: @↰3↓flatten_631:CNode_660{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @↰3↓flatten_631:CNode_661{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: param_end_dim, [2]: CNode_660}
#   3: @↰3↓flatten_631:CNode_662{[0]: ValueNode<Primitive> Return, [1]: CNode_661}


subgraph attr:
subgraph instance: ↱3↓flatten_632 : 0x6d1adb0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↱3↓flatten_632 parent: [subgraph @3↓flatten_586]() {
  %1(CNode_628) = $(3↓flatten_586):S_Prim_equal(%para129_start_dim, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
}
# Order:
#   1: @↱3↓flatten_632:CNode_663{[0]: ValueNode<Primitive> Return, [1]: CNode_628}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ↓2✗_apply_adam_641 : 0x68ee5c0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @↓2✗_apply_adam_641 parent: [subgraph @5✗_apply_adam_524]() {
  %1(CNode_665) = call @↓✗_apply_adam_664()
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:866/                if self.is_group_lr:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:866/                if self.is_group_lr:/
}
# Order:
#   1: @↓2✗_apply_adam_641:CNode_665{[0]: ValueNode<FuncGraph> ↓✗_apply_adam_664}
#   2: @↓2✗_apply_adam_641:CNode_666{[0]: ValueNode<Primitive> Return, [1]: CNode_665}


subgraph attr:
subgraph instance: 2✓3↓flatten_653 : 0x6d4bb10
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @2✓3↓flatten_653 parent: [subgraph @3↓flatten_586]() {
  %1(CNode_667) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1751/            return reshape_(input, (-1,))/
  %2(CNode_668) = S_Prim_MakeTuple(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1751/            return reshape_(input, (-1,))/
  %3(CNode_669) = S_Prim_Reshape[output_names: ["output"], input_names: ["tensor", "shape"]](%para152_фinput, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1751/            return reshape_(input, (-1,))/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1751/            return reshape_(input, (-1,))/
}
# Order:
#   1: @2✓3↓flatten_653:CNode_667{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @2✓3↓flatten_653:CNode_668{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_667}
#   3: @2✓3↓flatten_653:CNode_669{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_фinput, [2]: CNode_668}
#   4: @2✓3↓flatten_653:CNode_670{[0]: ValueNode<Primitive> Return, [1]: CNode_669}


subgraph attr:
subgraph instance: ✗✓3↓flatten_654 : 0x6d48db0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✗✓3↓flatten_654 parent: [subgraph @3↓flatten_586]() {
  %1(CNode_672) = call @↓✓3↓flatten_671()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1750/        if x_rank in (0, 1):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1750/        if x_rank in (0, 1):/
}
# Order:
#   1: @✗✓3↓flatten_654:CNode_672{[0]: ValueNode<FuncGraph> ↓✓3↓flatten_671}
#   2: @✗✓3↓flatten_654:CNode_673{[0]: ValueNode<Primitive> Return, [1]: CNode_672}


subgraph attr:
after_block : 1
subgraph instance: 4↓flatten_657 : 0x6d1ecb0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @4↓flatten_657 parent: [subgraph @3↓flatten_586]() {
  %1(x_rank) = $(3↓flatten_586):S_Prim_Rank(%para152_фinput)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1748/    x_rank = rank_(input)/
  %2(idx) = call @canonicalize_axis_674(%para129_start_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1755/    start_dim = canonicalize_axis(start_dim, x_rank)/
  %3(end_dim) = call @canonicalize_axis_674(%para130_end_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1756/    end_dim = canonicalize_axis(end_dim, x_rank)/
  %4(CNode_676) = call @check_dim_valid_675(%2, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1757/    check_dim_valid(start_dim, end_dim)/
  %5(CNode_677) = StopGradient(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
  %6(CNode_678) = S_Prim_MakeTuple(I64(0), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1759/    if x_rank in (0, 1):/
  %7(CNode_679) = S_Prim_in(%1, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1759/    if x_rank in (0, 1):/
  %8(CNode_680) = Cond(%7, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1759/    if x_rank in (0, 1):/
  %9(CNode_681) = Switch(%8, @✓4↓flatten_682, @✗4↓flatten_683)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1759/    if x_rank in (0, 1):/
  %10(CNode_684) = %9()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1759/    if x_rank in (0, 1):/
  %11(CNode_685) = Depend[side_effect_propagate: I64(1)](%10, %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1759/    if x_rank in (0, 1):/
  Return(%11)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1759/    if x_rank in (0, 1):/
}
# Order:
#   1: @4↓flatten_657:idx{[0]: ValueNode<FuncGraph> canonicalize_axis_674, [1]: param_start_dim, [2]: x_rank}
#   2: @4↓flatten_657:end_dim{[0]: ValueNode<FuncGraph> canonicalize_axis_674, [1]: param_end_dim, [2]: x_rank}
#   3: @4↓flatten_657:CNode_676{[0]: ValueNode<FuncGraph> check_dim_valid_675, [1]: idx, [2]: end_dim}
#   4: @4↓flatten_657:CNode_678{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<Int64Imm> 0, [2]: ValueNode<Int64Imm> 1}
#   5: @4↓flatten_657:CNode_679{[0]: ValueNode<DoSignaturePrimitive> S_Prim_in, [1]: x_rank, [2]: CNode_678}
#   6: @4↓flatten_657:CNode_680{[0]: ValueNode<Primitive> Cond, [1]: CNode_679, [2]: ValueNode<BoolImm> false}
#   7: @4↓flatten_657:CNode_681{[0]: ValueNode<Primitive> Switch, [1]: CNode_680, [2]: ValueNode<FuncGraph> ✓4↓flatten_682, [3]: ValueNode<FuncGraph> ✗4↓flatten_683}
#   8: @4↓flatten_657:CNode_684{[0]: CNode_681}
#   9: @4↓flatten_657:CNode_686{[0]: ValueNode<Primitive> Return, [1]: CNode_685}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ↓✗_apply_adam_664 : 0x68eb2a0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @↓✗_apply_adam_664 parent: [subgraph @5✗_apply_adam_524]() {
  %1(CNode_688) = call @↓_apply_adam_687()
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:826/            if self.use_dist_optimizer:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:826/            if self.use_dist_optimizer:/
}
# Order:
#   1: @↓✗_apply_adam_664:CNode_688{[0]: ValueNode<FuncGraph> ↓_apply_adam_687}
#   2: @↓✗_apply_adam_664:CNode_689{[0]: ValueNode<Primitive> Return, [1]: CNode_688}


subgraph attr:
after_block : 1
subgraph instance: ↓✓3↓flatten_671 : 0x6d4a0c0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↓✓3↓flatten_671 parent: [subgraph @3↓flatten_586]() {
  %1(CNode_690) = call @_get_cache_prim_344(ClassType)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1752/        return _get_cache_prim(P.Flatten)()(input)/
  %2(CNode_691) = %1()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1752/        return _get_cache_prim(P.Flatten)()(input)/
  %3(CNode_692) = %2(%para152_фinput)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1752/        return _get_cache_prim(P.Flatten)()(input)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1752/        return _get_cache_prim(P.Flatten)()(input)/
}
# Order:
#   1: @↓✓3↓flatten_671:CNode_690{[0]: ValueNode<FuncGraph> _get_cache_prim_344, [1]: ValueNode<ClassType> class 'mindspore.ops.operations.nn_ops.Flatten'}
#   2: @↓✓3↓flatten_671:CNode_691{[0]: CNode_690}
#   3: @↓✓3↓flatten_671:CNode_692{[0]: CNode_691, [1]: param_фinput}
#   4: @↓✓3↓flatten_671:CNode_693{[0]: ValueNode<Primitive> Return, [1]: CNode_692}


subgraph attr:
subgraph instance: check_dim_valid_675 : 0x6d418c0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1721/    def check_dim_valid(start_dim, end_dim):/
subgraph @check_dim_valid_675(%para154_start_dim, %para155_end_dim) {
  %1(CNode_694) = S_Prim_greater(%para154_start_dim, %para155_end_dim)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1722/        if start_dim > end_dim:/
  %2(CNode_695) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1722/        if start_dim > end_dim:/
  %3(CNode_696) = Switch(%2, @✓check_dim_valid_697, @✗check_dim_valid_698)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1722/        if start_dim > end_dim:/
  %4(CNode_699) = %3()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1722/        if start_dim > end_dim:/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1722/        if start_dim > end_dim:/
}
# Order:
#   1: @check_dim_valid_675:CNode_694{[0]: ValueNode<DoSignaturePrimitive> S_Prim_greater, [1]: param_start_dim, [2]: param_end_dim}
#   2: @check_dim_valid_675:CNode_695{[0]: ValueNode<Primitive> Cond, [1]: CNode_694, [2]: ValueNode<BoolImm> false}
#   3: @check_dim_valid_675:CNode_696{[0]: ValueNode<Primitive> Switch, [1]: CNode_695, [2]: ValueNode<FuncGraph> ✓check_dim_valid_697, [3]: ValueNode<FuncGraph> ✗check_dim_valid_698}
#   4: @check_dim_valid_675:CNode_699{[0]: CNode_696}
#   5: @check_dim_valid_675:CNode_700{[0]: ValueNode<Primitive> Return, [1]: CNode_699}


subgraph attr:
subgraph instance: canonicalize_axis_674 : 0x6d31ae0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1725/    def canonicalize_axis(axis, x_rank):/
subgraph @canonicalize_axis_674(%para156_axis, %para157_x_rank) {
  %1(CNode_701) = S_Prim_not_equal(%para157_x_rank, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %2(CNode_702) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %3(CNode_703) = Switch(%2, @↰canonicalize_axis_704, @↱canonicalize_axis_705)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %4(ndim) = %3()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %5(CNode_707) = call @check_axis_valid_706(%para156_axis, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1727/        check_axis_valid(axis, ndim)/
  %6(CNode_708) = StopGradient(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1725/    def canonicalize_axis(axis, x_rank):/
  %7(CNode_709) = S_Prim_greater_equal(%para156_axis, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  %8(CNode_710) = Cond(%7, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  %9(CNode_711) = Switch(%8, @↰canonicalize_axis_712, @↱canonicalize_axis_713)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  %10(CNode_714) = %9()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  %11(CNode_715) = Depend[side_effect_propagate: I64(1)](%10, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  Return(%11)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
}
# Order:
#   1: @canonicalize_axis_674:CNode_701{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: param_x_rank, [2]: ValueNode<Int64Imm> 0}
#   2: @canonicalize_axis_674:CNode_702{[0]: ValueNode<Primitive> Cond, [1]: CNode_701, [2]: ValueNode<BoolImm> false}
#   3: @canonicalize_axis_674:CNode_703{[0]: ValueNode<Primitive> Switch, [1]: CNode_702, [2]: ValueNode<FuncGraph> ↰canonicalize_axis_704, [3]: ValueNode<FuncGraph> ↱canonicalize_axis_705}
#   4: @canonicalize_axis_674:ndim{[0]: CNode_703}
#   5: @canonicalize_axis_674:CNode_707{[0]: ValueNode<FuncGraph> check_axis_valid_706, [1]: param_axis, [2]: ndim}
#   6: @canonicalize_axis_674:CNode_709{[0]: ValueNode<DoSignaturePrimitive> S_Prim_greater_equal, [1]: param_axis, [2]: ValueNode<Int64Imm> 0}
#   7: @canonicalize_axis_674:CNode_710{[0]: ValueNode<Primitive> Cond, [1]: CNode_709, [2]: ValueNode<BoolImm> false}
#   8: @canonicalize_axis_674:CNode_711{[0]: ValueNode<Primitive> Switch, [1]: CNode_710, [2]: ValueNode<FuncGraph> ↰canonicalize_axis_712, [3]: ValueNode<FuncGraph> ↱canonicalize_axis_713}
#   9: @canonicalize_axis_674:CNode_714{[0]: CNode_711}
#  10: @canonicalize_axis_674:CNode_716{[0]: ValueNode<Primitive> Return, [1]: CNode_715}


subgraph attr:
subgraph instance: ✓4↓flatten_682 : 0x6d302e0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✓4↓flatten_682 parent: [subgraph @3↓flatten_586]() {
  %1(CNode_717) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1760/        return reshape_(input, (-1,))/
  %2(CNode_718) = S_Prim_MakeTuple(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1760/        return reshape_(input, (-1,))/
  %3(CNode_719) = S_Prim_Reshape[output_names: ["output"], input_names: ["tensor", "shape"]](%para152_фinput, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1760/        return reshape_(input, (-1,))/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1760/        return reshape_(input, (-1,))/
}
# Order:
#   1: @✓4↓flatten_682:CNode_717{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @✓4↓flatten_682:CNode_718{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_717}
#   3: @✓4↓flatten_682:CNode_719{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_фinput, [2]: CNode_718}
#   4: @✓4↓flatten_682:CNode_720{[0]: ValueNode<Primitive> Return, [1]: CNode_719}


subgraph attr:
subgraph instance: ✗4↓flatten_683 : 0x6cb37a0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✗4↓flatten_683 parent: [subgraph @4↓flatten_657]() {
  %1(CNode_722) = call @5↓flatten_721()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1759/    if x_rank in (0, 1):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1759/    if x_rank in (0, 1):/
}
# Order:
#   1: @✗4↓flatten_683:CNode_722{[0]: ValueNode<FuncGraph> 5↓flatten_721}
#   2: @✗4↓flatten_683:CNode_723{[0]: ValueNode<Primitive> Return, [1]: CNode_722}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ↓_apply_adam_687 : 0x68e8c80
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @↓_apply_adam_687 parent: [subgraph @5✗_apply_adam_524]() {
  %1(CNode_569) = $(5✗_apply_adam_524):S_Prim_Partial[side_effect_propagate: I64(1)](S_Prim_adam_opt, S_Prim_Adam[side_effect_mem: Bool(1), use_nesterov: Bool(0), use_locking: Bool(0)], S_Prim_FusedSparseAdam[output_names: ["var", "m", "v"], side_effect_mem: Bool(1), use_nesterov: Bool(0), input_names: ["var", "m", "v", "beta1_power", "beta2_power", "lr", "beta1", "beta2", "epsilon", "grad", "indices"], use_locking: Bool(0), primitive_target: "CPU"], S_Prim_Push[optim_type: "Adam", input_names: ["optim_inputs", "optim_input_shapes"], only_shape_indices: [I64(0), I64(1), I64(2)], output_names: ["key"], use_nesterov: Bool(0), side_effect_hidden: Bool(1), primitive_target: "CPU"], S_Prim_Pull[output_names: ["output"], input_names: ["key", "weight"], primitive_target: "CPU"], Bool(0), Bool(0), Bool(1), %para113_beta1_power, %para114_beta2_power, Tensor(shape=[], dtype=Float32, value=0.9), Tensor(shape=[], dtype=Float32, value=0.999), Tensor(shape=[], dtype=Float32, value=1e-08), %para117_lr)
      : (<null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:901/                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt, self._ps_push,/
  %2(success) = $(5✗_apply_adam_524):S_Prim_map(%1, %para118_gradients, %para112_params, %para115_moment1, %para116_moment2, (Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0)), (Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0)))
      : (<null>, <null>, <null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:901/                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt, self._ps_push,/
  Return(%2)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:907/        return success/
}
# Order:
#   1: @↓_apply_adam_687:CNode_724{[0]: ValueNode<Primitive> Return, [1]: success}


subgraph attr:
subgraph instance: ✓check_dim_valid_697 : 0x6d45b40
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1721/    def check_dim_valid(start_dim, end_dim):/
subgraph @✓check_dim_valid_697() {
  %1(CNode_725) = raise[side_effect_io: Bool(1)]("ValueError", "For 'flatten', 'start_dim' cannot come after 'end_dim'.", "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1723/            raise ValueError("For 'flatten', 'start_dim' cannot come after 'end_dim'.")/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1723/            raise ValueError("For 'flatten', 'start_dim' cannot come after 'end_dim'.")/
}
# Order:
#   1: @✓check_dim_valid_697:CNode_725{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> ValueError, [2]: ValueNode<StringImm> For 'flatten', 'start_dim' cannot come after 'end_dim'., [3]: ValueNode<StringImm> None}
#   2: @✓check_dim_valid_697:CNode_726{[0]: ValueNode<Primitive> Return, [1]: CNode_725}


subgraph attr:
subgraph instance: ✗check_dim_valid_698 : 0x6d43900
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1721/    def check_dim_valid(start_dim, end_dim):/
subgraph @✗check_dim_valid_698() {
  %1(CNode_728) = call @↓check_dim_valid_727()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1722/        if start_dim > end_dim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1722/        if start_dim > end_dim:/
}
# Order:
#   1: @✗check_dim_valid_698:CNode_728{[0]: ValueNode<FuncGraph> ↓check_dim_valid_727}
#   2: @✗check_dim_valid_698:CNode_729{[0]: ValueNode<Primitive> Return, [1]: CNode_728}


subgraph attr:
subgraph instance: check_axis_valid_706 : 0x6d39680
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @check_axis_valid_706(%para158_axis, %para159_ndim) {
  %1(CNode_730) = S_Prim_negative(%para159_ndim)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %2(CNode_731) = S_Prim_less(%para158_axis, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %3(CNode_732) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %4(CNode_733) = Switch(%3, @↰check_axis_valid_734, @↱check_axis_valid_735)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %5(CNode_736) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %6(CNode_737) = Cond(%5, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %7(CNode_738) = Switch(%6, @✓check_axis_valid_739, @✗check_axis_valid_740)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %8(CNode_741) = %7()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_706:CNode_730{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: param_ndim}
#   2: @check_axis_valid_706:CNode_731{[0]: ValueNode<DoSignaturePrimitive> S_Prim_less, [1]: param_axis, [2]: CNode_730}
#   3: @check_axis_valid_706:CNode_732{[0]: ValueNode<Primitive> Cond, [1]: CNode_731, [2]: ValueNode<BoolImm> false}
#   4: @check_axis_valid_706:CNode_733{[0]: ValueNode<Primitive> Switch, [1]: CNode_732, [2]: ValueNode<FuncGraph> ↰check_axis_valid_734, [3]: ValueNode<FuncGraph> ↱check_axis_valid_735}
#   5: @check_axis_valid_706:CNode_736{[0]: CNode_733}
#   6: @check_axis_valid_706:CNode_737{[0]: ValueNode<Primitive> Cond, [1]: CNode_736, [2]: ValueNode<BoolImm> false}
#   7: @check_axis_valid_706:CNode_738{[0]: ValueNode<Primitive> Switch, [1]: CNode_737, [2]: ValueNode<FuncGraph> ✓check_axis_valid_739, [3]: ValueNode<FuncGraph> ✗check_axis_valid_740}
#   8: @check_axis_valid_706:CNode_741{[0]: CNode_738}
#   9: @check_axis_valid_706:CNode_742{[0]: ValueNode<Primitive> Return, [1]: CNode_741}


subgraph attr:
subgraph instance: ↰canonicalize_axis_704 : 0x6d387f0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
subgraph @↰canonicalize_axis_704 parent: [subgraph @canonicalize_axis_674]() {
  Return(%para157_x_rank)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
}
# Order:
#   1: @↰canonicalize_axis_704:CNode_743{[0]: ValueNode<Primitive> Return, [1]: param_x_rank}


subgraph attr:
subgraph instance: ↱canonicalize_axis_705 : 0x6d37960
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
subgraph @↱canonicalize_axis_705() {
  Return(I64(1))
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
}
# Order:
#   1: @↱canonicalize_axis_705:CNode_744{[0]: ValueNode<Primitive> Return, [1]: ValueNode<Int64Imm> 1}


subgraph attr:
subgraph instance: ↰canonicalize_axis_712 : 0x6d36ad0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
subgraph @↰canonicalize_axis_712 parent: [subgraph @canonicalize_axis_674]() {
  Return(%para156_axis)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
}
# Order:
#   1: @↰canonicalize_axis_712:CNode_745{[0]: ValueNode<Primitive> Return, [1]: param_axis}


subgraph attr:
subgraph instance: ↱canonicalize_axis_713 : 0x6d35b40
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
subgraph @↱canonicalize_axis_713 parent: [subgraph @canonicalize_axis_674]() {
  %1(CNode_701) = $(canonicalize_axis_674):S_Prim_not_equal(%para157_x_rank, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %2(CNode_702) = $(canonicalize_axis_674):Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %3(CNode_703) = $(canonicalize_axis_674):Switch(%2, @↰canonicalize_axis_704, @↱canonicalize_axis_705)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %4(ndim) = $(canonicalize_axis_674):%3()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %5(CNode_746) = S_Prim_add(%para156_axis, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
}
# Order:
#   1: @↱canonicalize_axis_713:CNode_746{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: param_axis, [2]: ndim}
#   2: @↱canonicalize_axis_713:CNode_747{[0]: ValueNode<Primitive> Return, [1]: CNode_746}


subgraph attr:
after_block : 1
subgraph instance: 5↓flatten_721 : 0x6d24340
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @5↓flatten_721 parent: [subgraph @4↓flatten_657]() {
  %1(x_rank) = $(3↓flatten_586):S_Prim_Rank(%para152_фinput)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1748/    x_rank = rank_(input)/
  %2(idx) = $(4↓flatten_657):call @canonicalize_axis_674(%para129_start_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1755/    start_dim = canonicalize_axis(start_dim, x_rank)/
  %3(end_dim) = $(4↓flatten_657):call @canonicalize_axis_674(%para130_end_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1756/    end_dim = canonicalize_axis(end_dim, x_rank)/
  %4(CNode_748) = S_Prim_equal(%2, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1762/    if start_dim == end_dim:/
  %5(CNode_749) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1762/    if start_dim == end_dim:/
  %6(CNode_750) = Switch(%5, @✓5↓flatten_751, @✗5↓flatten_752)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1762/    if start_dim == end_dim:/
  %7(CNode_753) = %6()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1762/    if start_dim == end_dim:/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1762/    if start_dim == end_dim:/
}
# Order:
#   1: @5↓flatten_721:CNode_748{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: idx, [2]: end_dim}
#   2: @5↓flatten_721:CNode_749{[0]: ValueNode<Primitive> Cond, [1]: CNode_748, [2]: ValueNode<BoolImm> false}
#   3: @5↓flatten_721:CNode_750{[0]: ValueNode<Primitive> Switch, [1]: CNode_749, [2]: ValueNode<FuncGraph> ✓5↓flatten_751, [3]: ValueNode<FuncGraph> ✗5↓flatten_752}
#   4: @5↓flatten_721:CNode_753{[0]: CNode_750}
#   5: @5↓flatten_721:CNode_754{[0]: ValueNode<Primitive> Return, [1]: CNode_753}


subgraph attr:
after_block : 1
subgraph instance: ↓check_dim_valid_727 : 0x6d44bb0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1721/    def check_dim_valid(start_dim, end_dim):/
subgraph @↓check_dim_valid_727() {
  Return(None)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
}
# Order:
#   1: @↓check_dim_valid_727:CNode_755{[0]: ValueNode<Primitive> Return, [1]: ValueNode<None> None}


subgraph attr:
subgraph instance: ✓check_axis_valid_739 : 0x6d406f0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @✓check_axis_valid_739() {
  %1(CNode_756) = raise[side_effect_io: Bool(1)]("ValueError", "'start_dim' or 'end_dim' out of range.", "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1719/            raise ValueError("'start_dim' or 'end_dim' out of range.")/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1719/            raise ValueError("'start_dim' or 'end_dim' out of range.")/
}
# Order:
#   1: @✓check_axis_valid_739:CNode_756{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> ValueError, [2]: ValueNode<StringImm> 'start_dim' or 'end_dim' out of range., [3]: ValueNode<StringImm> None}
#   2: @✓check_axis_valid_739:CNode_757{[0]: ValueNode<Primitive> Return, [1]: CNode_756}


subgraph attr:
subgraph instance: ✗check_axis_valid_740 : 0x6d3e4b0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @✗check_axis_valid_740() {
  %1(CNode_759) = call @↓check_axis_valid_758()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @✗check_axis_valid_740:CNode_759{[0]: ValueNode<FuncGraph> ↓check_axis_valid_758}
#   2: @✗check_axis_valid_740:CNode_760{[0]: ValueNode<Primitive> Return, [1]: CNode_759}


subgraph attr:
subgraph instance: ↰check_axis_valid_734 : 0x6d3d620
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @↰check_axis_valid_734 parent: [subgraph @check_axis_valid_706]() {
  %1(CNode_730) = $(check_axis_valid_706):S_Prim_negative(%para159_ndim)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %2(CNode_731) = $(check_axis_valid_706):S_Prim_less(%para158_axis, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @↰check_axis_valid_734:CNode_761{[0]: ValueNode<Primitive> Return, [1]: CNode_731}


subgraph attr:
subgraph instance: ↱check_axis_valid_735 : 0x6d3c560
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @↱check_axis_valid_735 parent: [subgraph @check_axis_valid_706]() {
  %1(CNode_762) = S_Prim_greater_equal(%para158_axis, %para159_ndim)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @↱check_axis_valid_735:CNode_762{[0]: ValueNode<DoSignaturePrimitive> S_Prim_greater_equal, [1]: param_axis, [2]: param_ndim}
#   2: @↱check_axis_valid_735:CNode_763{[0]: ValueNode<Primitive> Return, [1]: CNode_762}


subgraph attr:
subgraph instance: ✓5↓flatten_751 : 0x6d2f450
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✓5↓flatten_751 parent: [subgraph @3↓flatten_586]() {
  Return(%para152_фinput)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1763/        return input/
}
# Order:
#   1: @✓5↓flatten_751:CNode_764{[0]: ValueNode<Primitive> Return, [1]: param_фinput}


subgraph attr:
subgraph instance: ✗5↓flatten_752 : 0x6d26140
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✗5↓flatten_752 parent: [subgraph @4↓flatten_657]() {
  %1(CNode_766) = call @6↓flatten_765()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1762/    if start_dim == end_dim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1762/    if start_dim == end_dim:/
}
# Order:
#   1: @✗5↓flatten_752:CNode_766{[0]: ValueNode<FuncGraph> 6↓flatten_765}
#   2: @✗5↓flatten_752:CNode_767{[0]: ValueNode<Primitive> Return, [1]: CNode_766}


subgraph attr:
after_block : 1
subgraph instance: ↓check_axis_valid_758 : 0x6d3f760
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @↓check_axis_valid_758() {
  Return(None)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
}
# Order:
#   1: @↓check_axis_valid_758:CNode_768{[0]: ValueNode<Primitive> Return, [1]: ValueNode<None> None}


subgraph attr:
after_block : 1
subgraph instance: 6↓flatten_765 : 0x6d27450
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @6↓flatten_765 parent: [subgraph @4↓flatten_657]() {
  %1(x_rank) = $(3↓flatten_586):S_Prim_Rank(%para152_фinput)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1748/    x_rank = rank_(input)/
  %2(idx) = $(4↓flatten_657):call @canonicalize_axis_674(%para129_start_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1755/    start_dim = canonicalize_axis(start_dim, x_rank)/
  %3(CNode_770) = call @↵6↓flatten_769(%2, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1767/    while idx <= end_dim:/
}
# Order:
#   1: @6↓flatten_765:CNode_771{[0]: ValueNode<Primitive> Return, [1]: CNode_770}
#   2: @6↓flatten_765:CNode_770{[0]: ValueNode<FuncGraph> ↵6↓flatten_769, [1]: idx, [2]: ValueNode<Int64Imm> 1}


subgraph attr:
is_while_header : 1
subgraph instance: ↵6↓flatten_769 : 0x6d288a0
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↵6↓flatten_769 parent: [subgraph @4↓flatten_657](%para160_, %para161_) {
  %1(x_rank) = $(3↓flatten_586):S_Prim_Rank(%para152_фinput)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1748/    x_rank = rank_(input)/
  %2(end_dim) = $(4↓flatten_657):call @canonicalize_axis_674(%para130_end_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1756/    end_dim = canonicalize_axis(end_dim, x_rank)/
  %3(CNode_772) = S_Prim_less_equal(%para160_фidx, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1767/    while idx <= end_dim:/
  %4(force_while_cond_CNode_772) = Cond(%3, Bool(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1767/    while idx <= end_dim:/
  %5(CNode_773) = Switch(%4, @↻6↓flatten_774, @7↓flatten_775)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1767/    while idx <= end_dim:/
  %6(CNode_776) = %5()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1767/    while idx <= end_dim:/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1767/    while idx <= end_dim:/
}
# Order:
#   1: @↵6↓flatten_769:CNode_772{[0]: ValueNode<DoSignaturePrimitive> S_Prim_less_equal, [1]: param_фidx, [2]: end_dim}
#   2: @↵6↓flatten_769:force_while_cond_CNode_772{[0]: ValueNode<Primitive> Cond, [1]: CNode_772, [2]: ValueNode<BoolImm> true}
#   3: @↵6↓flatten_769:CNode_773{[0]: ValueNode<Primitive> Switch, [1]: force_while_cond_CNode_772, [2]: ValueNode<FuncGraph> ↻6↓flatten_774, [3]: ValueNode<FuncGraph> 7↓flatten_775}
#   4: @↵6↓flatten_769:CNode_776{[0]: CNode_773}
#   5: @↵6↓flatten_769:CNode_777{[0]: ValueNode<Primitive> Return, [1]: CNode_776}


subgraph attr:
subgraph instance: ↻6↓flatten_774 : 0x6d2d770
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↻6↓flatten_774 parent: [subgraph @↵6↓flatten_769]() {
  %1(idx) = S_Prim_add(%para160_фidx, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1769/        idx += 1/
  %2(x_shape) = $(3↓flatten_586):S_Prim_Shape(%para152_фinput)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1747/    x_shape = shape_(input)/
  %3(CNode_778) = S_Prim_getitem(%2, %para160_фidx)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1768/        dim_length *= x_shape[idx]/
  %4(dim_length) = S_Prim_mul(%para161_фdim_length, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1768/        dim_length *= x_shape[idx]/
  %5(CNode_779) = call @↵6↓flatten_769(%1, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1767/    while idx <= end_dim:/
}
# Order:
#   1: @↻6↓flatten_774:CNode_778{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: param_фidx}
#   2: @↻6↓flatten_774:dim_length{[0]: ValueNode<DoSignaturePrimitive> S_Prim_mul, [1]: param_фdim_length, [2]: CNode_778}
#   3: @↻6↓flatten_774:idx{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: param_фidx, [2]: ValueNode<Int64Imm> 1}
#   4: @↻6↓flatten_774:CNode_780{[0]: ValueNode<Primitive> Return, [1]: CNode_779}
#   5: @↻6↓flatten_774:CNode_779{[0]: ValueNode<FuncGraph> ↵6↓flatten_769, [1]: idx, [2]: dim_length}


subgraph attr:
subgraph instance: 7↓flatten_775 : 0x6d2ad20
# In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @7↓flatten_775 parent: [subgraph @↵6↓flatten_769]() {
  %1(x_shape) = $(3↓flatten_586):S_Prim_Shape(%para152_фinput)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1747/    x_shape = shape_(input)/
  %2(x_rank) = $(3↓flatten_586):S_Prim_Rank(%para152_фinput)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1748/    x_rank = rank_(input)/
  %3(idx) = $(4↓flatten_657):call @canonicalize_axis_674(%para129_start_dim, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1755/    start_dim = canonicalize_axis(start_dim, x_rank)/
  %4(CNode_781) = S_Prim_make_slice(None, %3, None)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %5(CNode_782) = S_Prim_getitem(%1, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %6(CNode_783) = S_Prim_MakeTuple(%para161_фdim_length)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %7(CNode_784) = S_Prim_add(%5, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %8(end_dim) = $(4↓flatten_657):call @canonicalize_axis_674(%para130_end_dim, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1756/    end_dim = canonicalize_axis(end_dim, x_rank)/
  %9(CNode_785) = S_Prim_add(%8, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %10(CNode_786) = S_Prim_make_slice(%9, None, None)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %11(CNode_787) = S_Prim_getitem(%1, %10)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %12(new_shape) = S_Prim_add(%7, %11)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %13(CNode_788) = S_Prim_Reshape[output_names: ["output"], input_names: ["tensor", "shape"]](%para152_фinput, %12)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1771/    return reshape_(input, new_shape)/
  Return(%13)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /home/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1771/    return reshape_(input, new_shape)/
}
# Order:
#   1: @7↓flatten_775:CNode_781{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_slice, [1]: ValueNode<None> None, [2]: idx, [3]: ValueNode<None> None}
#   2: @7↓flatten_775:CNode_782{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: CNode_781}
#   3: @7↓flatten_775:CNode_783{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: param_фdim_length}
#   4: @7↓flatten_775:CNode_784{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: CNode_782, [2]: CNode_783}
#   5: @7↓flatten_775:CNode_785{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: end_dim, [2]: ValueNode<Int64Imm> 1}
#   6: @7↓flatten_775:CNode_786{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_slice, [1]: CNode_785, [2]: ValueNode<None> None, [3]: ValueNode<None> None}
#   7: @7↓flatten_775:CNode_787{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: CNode_786}
#   8: @7↓flatten_775:new_shape{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: CNode_784, [2]: CNode_787}
#   9: @7↓flatten_775:CNode_788{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_фinput, [2]: new_shape}
#  10: @7↓flatten_775:CNode_789{[0]: ValueNode<Primitive> Return, [1]: CNode_788}


