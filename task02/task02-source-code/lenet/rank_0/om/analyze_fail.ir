# ===============================================================================================
# The following shows the last analyze fail log message.
# ===============================================================================================

----------------------------------------------------
- Caught exception:
----------------------------------------------------
For 'Conv2D', 'C_in' of input 'x' shape divide by parameter 'group' must be equal to 'C_in' of input 'weight' shape: 64, but got 'C_in' of input 'x' shape: 32, and 'group': 1.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore/core/ops/conv2d.cc:240 Conv2dInferShape

----------------------------------------------------
- The Traceback of Net Construct Code:
----------------------------------------------------
# 0 In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:417
        if not self.sense_flag:
# 1 In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:418
            return self._no_sens_impl(*inputs)
                   ^
# 2 In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:437
        if self.return_grad:
# 3 In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:433
        loss = self.network(*inputs)
               ^
# 4 In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:121
        out = self._backbone(data)
              ^
# 5 In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:120
        x = self.max_pool2d(self.relu(self.conv2(x)))
                                      ^
# 6 In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362
        if self.has_bias:
# 7 In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:361
        output = self.conv2d(x, self.weight)
                 ^

# ===============================================================================================
# The following shows the IR when the function graphs evaluation fails to help locate the problem.
# You can search the last ------------------------> to the node which is evaluated failure.
# Refer to https://www.mindspore.cn/search?inputValue=analyze_fail.ir to get more instructions.
# ===============================================================================================

# IR entry: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4
# Total subgraphs: 140

# Attrs:
training : 1

# Total params: 21
# Params:
%para1_inputs0 : <null>
%para2_inputs1 : <null>
%para3_conv1.weight : <Ref[Tensor[Float32]], (32, 1, 5, 5), ref_key=:conv1.weight>  :  has_default
%para4_conv2.weight : <Ref[Tensor[Float32]], (16, 64, 5, 5), ref_key=:conv2.weight>  :  has_default
%para5_fc1.weight : <Ref[Tensor[Float32]], (128, 1600), ref_key=:fc1.weight>  :  has_default
%para6_fc1.bias : <Ref[Tensor[Float32]], (128), ref_key=:fc1.bias>  :  has_default
%para7_fc2.weight : <Ref[Tensor[Float32]], (256, 128), ref_key=:fc2.weight>  :  has_default
%para8_fc2.bias : <Ref[Tensor[Float32]], (256), ref_key=:fc2.bias>  :  has_default
%para9_fc3.weight : <Ref[Tensor[Float32]], (10, 256), ref_key=:fc3.weight>  :  has_default
%para10_fc3.bias : <Ref[Tensor[Float32]], (10), ref_key=:fc3.bias>  :  has_default
%para11_global_step : <Ref[Tensor[Int32]], (1), ref_key=:global_step>  :  has_default
%para12_moments.conv1.weight : <Ref[Tensor[Float32]], (32, 1, 5, 5), ref_key=:moments.conv1.weight>  :  has_default
%para13_moments.conv2.weight : <Ref[Tensor[Float32]], (16, 64, 5, 5), ref_key=:moments.conv2.weight>  :  has_default
%para14_moments.fc1.weight : <Ref[Tensor[Float32]], (128, 1600), ref_key=:moments.fc1.weight>  :  has_default
%para15_moments.fc1.bias : <Ref[Tensor[Float32]], (128), ref_key=:moments.fc1.bias>  :  has_default
%para16_moments.fc2.weight : <Ref[Tensor[Float32]], (256, 128), ref_key=:moments.fc2.weight>  :  has_default
%para17_moments.fc2.bias : <Ref[Tensor[Float32]], (256), ref_key=:moments.fc2.bias>  :  has_default
%para18_moments.fc3.weight : <Ref[Tensor[Float32]], (10, 256), ref_key=:moments.fc3.weight>  :  has_default
%para19_moments.fc3.bias : <Ref[Tensor[Float32]], (10), ref_key=:moments.fc3.bias>  :  has_default
%para20_momentum : <Ref[Tensor[Float32]], (), ref_key=:momentum>  :  has_default
%para21_learning_rate : <Ref[Tensor[Float32]], (), ref_key=:learning_rate>  :  has_default

subgraph attr:
training : 1
subgraph instance: mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4 : 0x140ece618
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:416/    def construct(self, *inputs):/
subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4(%para1_inputs0, %para2_inputs1, %para3_conv1.weight, %para4_conv2.weight, %para5_fc1.weight, %para6_fc1.bias, %para7_fc2.weight, %para8_fc2.bias, %para9_fc3.weight, %para10_fc3.bias, %para11_global_step, %para12_moments.conv1.weight, %para13_moments.conv2.weight, %para14_moments.fc1.weight, %para15_moments.fc1.bias, %para16_moments.fc2.weight, %para17_moments.fc2.bias, %para18_moments.fc3.weight, %para19_moments.fc3.bias, %para20_momentum, %para21_learning_rate) {

#------------------------> 0
  %1(CNode_20) = call @✓mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_5()
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:417/        if not self.sense_flag:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:417/        if not self.sense_flag:/
}
# Order:
#   1: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4:CNode_20{[0]: ValueNode<FuncGraph> ✓mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_5}
#   2: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4:CNode_21{[0]: ValueNode<Primitive> Return, [1]: CNode_20}


subgraph attr:
training : 1
subgraph instance: ✓mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_5 : 0x140ecde18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:416/    def construct(self, *inputs):/
subgraph @✓mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_5 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4]() {
  %1(CNode_22) = $(mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4):MakeTuple(%para1_inputs0, %para2_inputs1)
      : (<Tensor[Float32], (32, 1, 32, 32)>, <Tensor[Int32], (32)>) -> (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:416/    def construct(self, *inputs):/

#------------------------> 1
  %2(CNode_23) = UnpackCall_unpack_call(@_no_sens_impl_24, %1)
      : (<Func, NoShape>, <Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>) -> (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
  Return(%2)
      : (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
}
# Order:
#   1: @✓mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_5:CNode_23{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.25, [1]: ValueNode<FuncGraph> _no_sens_impl_24, [2]: CNode_22}
#   2: @✓mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_5:CNode_26{[0]: ValueNode<Primitive> Return, [1]: CNode_23}


subgraph attr:
core : 1
subgraph instance: UnpackCall_6 : 0x140f12018
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
subgraph @UnpackCall_6(%para22_, %para23_) {
  %1(CNode_23) = TupleGetItem(%para23_8, I64(0))
      : (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>, <Int64, NoShape>) -> (<Tensor[Float32], (32, 1, 32, 32)>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
  %2(CNode_23) = TupleGetItem(%para23_8, I64(1))
      : (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>, <Int64, NoShape>) -> (<Tensor[Int32], (32)>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/

#------------------------> 2
  %3(CNode_23) = %para22_7(%1, %2)
      : (<Tensor[Float32], (32, 1, 32, 32)>, <Tensor[Int32], (32)>) -> (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
  Return(%3)
      : (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
}
# Order:
#   1: @UnpackCall_6:CNode_23{[0]: param_7, [1]: CNode_23, [2]: CNode_23}
#   2: @UnpackCall_6:CNode_23{[0]: ValueNode<Primitive> Return, [1]: CNode_23}


subgraph attr:
training : 1
subgraph instance: _no_sens_impl_9 : 0x140f1c418
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @_no_sens_impl_9 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4](%para24_inputs0, %para25_inputs1) {

#------------------------> 3
  %1(CNode_27) = call @✗_no_sens_impl_10()
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:437/        if self.return_grad:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:437/        if self.return_grad:/
}
# Order:
#   1: @_no_sens_impl_9:loss{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.28, [1]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_15, [2]: CNode_29}
#   2: @_no_sens_impl_9:grads{[0]: ValueNode<UnpackGraphPrimitive> UnpackGraph, [1]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_15, [2]: CNode_29}
#   3: @_no_sens_impl_9:grads{[0]: ValueNode<DoSignaturePrimitive> S_Prim_grad, [1]: grads, [2]: CNode_30}
#   4: @_no_sens_impl_9:grads{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.31, [1]: grads, [2]: CNode_29}
#   5: @_no_sens_impl_9:grads{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Identity_construct_32, [1]: grads}
#   6: @_no_sens_impl_9:CNode_33{[0]: ValueNode<FuncGraph> mindspore_nn_optim_momentum_Momentum_construct_34, [1]: grads}
#   7: @_no_sens_impl_9:loss{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Depend, [1]: loss, [2]: CNode_33}
#   8: @_no_sens_impl_9:CNode_27{[0]: ValueNode<FuncGraph> ✗_no_sens_impl_10}
#   9: @_no_sens_impl_9:CNode_35{[0]: ValueNode<Primitive> Return, [1]: CNode_27}


subgraph attr:
training : 1
subgraph instance: ✗_no_sens_impl_10 : 0x140eebe18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @✗_no_sens_impl_10 parent: [subgraph @_no_sens_impl_9]() {

#------------------------> 4
  %1(CNode_36) = call @↓_no_sens_impl_11()
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:437/        if self.return_grad:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:437/        if self.return_grad:/
}
# Order:
#   1: @✗_no_sens_impl_10:CNode_36{[0]: ValueNode<FuncGraph> ↓_no_sens_impl_11}
#   2: @✗_no_sens_impl_10:CNode_37{[0]: ValueNode<Primitive> Return, [1]: CNode_36}


subgraph attr:
training : 1
subgraph instance: ↓_no_sens_impl_11 : 0x140f10418
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @↓_no_sens_impl_11 parent: [subgraph @_no_sens_impl_9]() {
  %1(CNode_29) = $(_no_sens_impl_9):MakeTuple(%para24_inputs0, %para25_inputs1)
      : (<Tensor[Float32], (32, 1, 32, 32)>, <Tensor[Int32], (32)>) -> (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/

#------------------------> 5
  %2(loss) = $(_no_sens_impl_9):UnpackCall_unpack_call(@mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_15, %1)
      : (<Func, NoShape>, <Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>) -> (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:433/        loss = self.network(*inputs)/
  %3(grads) = $(_no_sens_impl_9):UnpackGraph(@mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_15, %1)
      : (<null>, <Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>) -> (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %4(CNode_30) = $(_no_sens_impl_9):MakeTuple(%para3_conv1.weight, %para4_conv2.weight, %para5_fc1.weight, %para6_fc1.bias, %para7_fc2.weight, %para8_fc2.bias, %para9_fc3.weight, %para10_fc3.bias)
      : (<Ref[Tensor[Float32]], (32, 1, 5, 5)>, <Ref[Tensor[Float32]], (16, 64, 5, 5)>, <Ref[Tensor[Float32]], (128, 1600)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (256, 128)>, <Ref[Tensor[Float32]], (256)>, <Ref[Tensor[Float32]], (10, 256)>, <Ref[Tensor[Float32]], (10)>) -> (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %5(grads) = $(_no_sens_impl_9):S_Prim_grad(%3, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %6(grads) = $(_no_sens_impl_9):UnpackCall_unpack_call(%5, %1)
      : (<null>, <Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>) -> (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %7(grads) = $(_no_sens_impl_9):call @mindspore_nn_layer_basic_Identity_construct_32(%6)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:435/        grads = self.grad_reducer(grads)/
  %8(CNode_33) = $(_no_sens_impl_9):call @mindspore_nn_optim_momentum_Momentum_construct_34(%7)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:436/        loss = F.depend(loss, self.optimizer(grads))/
  %9(loss) = $(_no_sens_impl_9):S_Prim_Depend[side_effect_propagate: I64(1)](%2, %8)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:436/        loss = F.depend(loss, self.optimizer(grads))/
  Return(%9)
      : (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:442/        return loss/
}
# Order:
#   1: @↓_no_sens_impl_11:CNode_38{[0]: ValueNode<Primitive> Return, [1]: loss}


subgraph attr:
core : 1
subgraph instance: UnpackCall_12 : 0x140eb7018
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:433/        loss = self.network(*inputs)/
subgraph @UnpackCall_12(%para26_, %para27_) {
  %1(loss) = TupleGetItem(%para27_14, I64(0))
      : (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>, <Int64, NoShape>) -> (<Tensor[Float32], (32, 1, 32, 32)>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:433/        loss = self.network(*inputs)/
  %2(loss) = TupleGetItem(%para27_14, I64(1))
      : (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>, <Int64, NoShape>) -> (<Tensor[Int32], (32)>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:433/        loss = self.network(*inputs)/

#------------------------> 6
  %3(loss) = %para26_13(%1, %2)
      : (<Tensor[Float32], (32, 1, 32, 32)>, <Tensor[Int32], (32)>) -> (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:433/        loss = self.network(*inputs)/
  Return(%3)
      : (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:433/        loss = self.network(*inputs)/
}
# Order:
#   1: @UnpackCall_12:loss{[0]: param_13, [1]: loss, [2]: loss}
#   2: @UnpackCall_12:loss{[0]: ValueNode<Primitive> Return, [1]: loss}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_15 : 0x140ef6a18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:120/    def construct(self, data, label):/
subgraph @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_15 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4](%para28_data, %para29_label) {

#------------------------> 7
  %1(out) = call @__main___LeNet5_construct_16(%para28_data)
      : (<Tensor[Float32], (32, 1, 32, 32)>) -> (<null>)
      #scope: (Default/network-WithLossCell)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:121/        out = self._backbone(data)/
  %2(CNode_40) = call @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_39(%1, %para29_label)
      : (<null>, <Tensor[Int32], (32)>) -> (<null>)
      #scope: (Default/network-WithLossCell)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:122/        return self._loss_fn(out, label)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:122/        return self._loss_fn(out, label)/
}
# Order:
#   1: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_15:out{[0]: ValueNode<FuncGraph> __main___LeNet5_construct_16, [1]: param_data}
#   2: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_15:CNode_40{[0]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_39, [1]: out, [2]: param_label}
#   3: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_15:CNode_41{[0]: ValueNode<Primitive> Return, [1]: CNode_40}


subgraph attr:
training : 1
subgraph instance: __main___LeNet5_construct_16 : 0x140ef1418
# In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:118/    def construct(self, x):/
subgraph @__main___LeNet5_construct_16 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4](%para30_x) {
  %1(CNode_43) = call @mindspore_nn_layer_conv_Conv2d_construct_42(%para30_x)
      : (<Tensor[Float32], (32, 1, 32, 32)>) -> (<Tensor[Float32], (32, 32, 28, 28)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:119/        x = self.max_pool2d(self.relu(self.conv1(x)))/
  %2(CNode_45) = call @mindspore_nn_layer_activation_ReLU_construct_44(%1)
      : (<Tensor[Float32], (32, 32, 28, 28)>) -> (<Tensor[Float32], (32, 32, 28, 28)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:119/        x = self.max_pool2d(self.relu(self.conv1(x)))/
  %3(x) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_46(%2)
      : (<Tensor[Float32], (32, 32, 28, 28)>) -> (<Tensor[Float32], (32, 32, 14, 14)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:119/        x = self.max_pool2d(self.relu(self.conv1(x)))/

#------------------------> 8
  %4(CNode_47) = call @mindspore_nn_layer_conv_Conv2d_construct_17(%3)
      : (<Tensor[Float32], (32, 32, 14, 14)>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:120/        x = self.max_pool2d(self.relu(self.conv2(x)))/
  %5(CNode_48) = call @mindspore_nn_layer_activation_ReLU_construct_44(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:120/        x = self.max_pool2d(self.relu(self.conv2(x)))/
  %6(x) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_46(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:120/        x = self.max_pool2d(self.relu(self.conv2(x)))/
  %7(x) = call @mindspore_nn_layer_basic_Flatten_construct_49(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:121/        x = self.flatten(x)/
  %8(CNode_51) = call @mindspore_nn_layer_basic_Dense_construct_50(%7)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:122/        x = self.relu(self.fc1(x))/
  %9(x) = call @mindspore_nn_layer_activation_ReLU_construct_44(%8)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:122/        x = self.relu(self.fc1(x))/
  %10(CNode_53) = call @mindspore_nn_layer_basic_Dense_construct_52(%9)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:123/        x = self.relu(self.fc2(x))/
  %11(x) = call @mindspore_nn_layer_activation_ReLU_construct_44(%10)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:123/        x = self.relu(self.fc2(x))/
  %12(x) = call @mindspore_nn_layer_basic_Dense_construct_54(%11)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:124/        x = self.fc3(x)/
  Return(%12)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:125/        return x/
}
# Order:
#   1: @__main___LeNet5_construct_16:CNode_43{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_42, [1]: param_x}
#   2: @__main___LeNet5_construct_16:CNode_45{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_44, [1]: CNode_43}
#   3: @__main___LeNet5_construct_16:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_46, [1]: CNode_45}
#   4: @__main___LeNet5_construct_16:CNode_47{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_17, [1]: x}
#   5: @__main___LeNet5_construct_16:CNode_48{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_44, [1]: CNode_47}
#   6: @__main___LeNet5_construct_16:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_46, [1]: CNode_48}
#   7: @__main___LeNet5_construct_16:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Flatten_construct_49, [1]: x}
#   8: @__main___LeNet5_construct_16:CNode_51{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_50, [1]: x}
#   9: @__main___LeNet5_construct_16:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_44, [1]: CNode_51}
#  10: @__main___LeNet5_construct_16:CNode_53{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_52, [1]: x}
#  11: @__main___LeNet5_construct_16:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_44, [1]: CNode_53}
#  12: @__main___LeNet5_construct_16:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_54, [1]: x}
#  13: @__main___LeNet5_construct_16:CNode_55{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_17 : 0x140ef1a18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_17 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4](%para31_x) {

#------------------------> 9
  %1(CNode_56) = call @✗mindspore_nn_layer_conv_Conv2d_construct_18()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv2-Conv2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv2-Conv2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_17:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_conv2.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_17:CNode_56{[0]: ValueNode<FuncGraph> ✗mindspore_nn_layer_conv_Conv2d_construct_18}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_17:CNode_57{[0]: ValueNode<Primitive> Return, [1]: CNode_56}


subgraph attr:
training : 1
subgraph instance: ✗mindspore_nn_layer_conv_Conv2d_construct_18 : 0x140ef2018
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:360/    def construct(self, x):/
subgraph @✗mindspore_nn_layer_conv_Conv2d_construct_18 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_17]() {

#------------------------> 10
  %1(CNode_58) = call @↓mindspore_nn_layer_conv_Conv2d_construct_19()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv2-Conv2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv2-Conv2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @✗mindspore_nn_layer_conv_Conv2d_construct_18:CNode_58{[0]: ValueNode<FuncGraph> ↓mindspore_nn_layer_conv_Conv2d_construct_19}
#   2: @✗mindspore_nn_layer_conv_Conv2d_construct_18:CNode_59{[0]: ValueNode<Primitive> Return, [1]: CNode_58}


subgraph attr:
training : 1
subgraph instance: ↓mindspore_nn_layer_conv_Conv2d_construct_19 : 0x140f0a218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:360/    def construct(self, x):/
subgraph @↓mindspore_nn_layer_conv_Conv2d_construct_19 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_17]() {

#------------------------> 11
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_17):S_Prim_Conv2D[kernel_size: (I64(5), I64(5)), stride: (I64(1), I64(1), I64(1), I64(1)), mode: I64(1), out_channel: I64(16), group: I64(1), input_names: ["x", "w"], pad: (I64(0), I64(0), I64(0), I64(0)), dilation: (I64(1), I64(1), I64(1), I64(1)), pad_mode: I64(2), output_names: ["output"], format: "NCHW", groups: I64(1)](%para31_x, %para4_conv2.weight)
      : (<Tensor[Float32], (32, 32, 14, 14)>, <Ref[Tensor[Float32]], (16, 64, 5, 5)>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv2-Conv2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv2-Conv2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:364/        return output/
}
# Order:
#   1: @↓mindspore_nn_layer_conv_Conv2d_construct_19:CNode_60{[0]: ValueNode<Primitive> Return, [1]: output}


# ===============================================================================================
# The total of function graphs in evaluation stack: 12/13 (Ignored 1 internal frames).
# ===============================================================================================


# ===============================================================================================
# The rest function graphs are the following:
# ===============================================================================================
subgraph attr:
training : 1
subgraph instance: _no_sens_impl_24 : 0x140ecfa18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @_no_sens_impl_24 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4](%para32_inputs) {
  %1(CNode_27) = call @✗_no_sens_impl_61()
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:437/        if self.return_grad:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:437/        if self.return_grad:/
}
# Order:
#   1: @_no_sens_impl_24:loss{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.28, [1]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_62, [2]: param_inputs}
#   2: @_no_sens_impl_24:grads{[0]: ValueNode<UnpackGraphPrimitive> UnpackGraph, [1]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_62, [2]: param_inputs}
#   3: @_no_sens_impl_24:grads{[0]: ValueNode<DoSignaturePrimitive> S_Prim_grad, [1]: grads, [2]: CNode_30}
#   4: @_no_sens_impl_24:grads{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.31, [1]: grads, [2]: param_inputs}
#   5: @_no_sens_impl_24:grads{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Identity_construct_63, [1]: grads}
#   6: @_no_sens_impl_24:CNode_33{[0]: ValueNode<FuncGraph> mindspore_nn_optim_momentum_Momentum_construct_64, [1]: grads}
#   7: @_no_sens_impl_24:loss{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Depend, [1]: loss, [2]: CNode_33}
#   8: @_no_sens_impl_24:CNode_27{[0]: ValueNode<FuncGraph> ✗_no_sens_impl_61}
#   9: @_no_sens_impl_24:CNode_35{[0]: ValueNode<Primitive> Return, [1]: CNode_27}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: mindspore_nn_optim_momentum_Momentum_construct_64 : 0x140ed3018
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:215/    def construct(self, gradients):/
subgraph @mindspore_nn_optim_momentum_Momentum_construct_64 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4](%para33_gradients) {
  %1(CNode_65) = S_Prim_AssignAdd[output_names: ["ref"], side_effect_mem: Bool(1), input_names: ["ref", "value"]](%para11_global_step, Tensor(shape=[1], dtype=Int32, value=[1]))
      : (<Ref[Tensor[Int32]], (1), ref_key=:global_step>, <null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:223/        self.assignadd(self.global_step, self.global_step_increase_tensor)/
  %2(CNode_66) = StopGradient(%1)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:214/    @jit/
  %3(CNode_68) = call @✗mindspore_nn_optim_momentum_Momentum_construct_67()
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:224/        if self.use_dist_optimizer:/
  %4(CNode_69) = Depend[side_effect_propagate: I64(1)](%3, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:224/        if self.use_dist_optimizer:/
  Return(%4)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:224/        if self.use_dist_optimizer:/
}
# Order:
#   1: @mindspore_nn_optim_momentum_Momentum_construct_64:gradients{[0]: ValueNode<FuncGraph> flatten_gradients_70, [1]: param_gradients}
#   2: @mindspore_nn_optim_momentum_Momentum_construct_64:gradients{[0]: ValueNode<FuncGraph> decay_weight_71, [1]: gradients}
#   3: @mindspore_nn_optim_momentum_Momentum_construct_64:gradients{[0]: ValueNode<FuncGraph> gradients_centralization_72, [1]: gradients}
#   4: @mindspore_nn_optim_momentum_Momentum_construct_64:gradients{[0]: ValueNode<FuncGraph> scale_grad_73, [1]: gradients}
#   5: @mindspore_nn_optim_momentum_Momentum_construct_64:lr{[0]: ValueNode<FuncGraph> get_lr_74}
#   6: @mindspore_nn_optim_momentum_Momentum_construct_64:CNode_65{[0]: ValueNode<DoSignaturePrimitive> S_Prim_AssignAdd, [1]: param_global_step, [2]: ValueNode<Tensor> Tensor(shape=[1], dtype=Int32, value=[1])}
#   7: @mindspore_nn_optim_momentum_Momentum_construct_64:CNode_68{[0]: ValueNode<FuncGraph> ✗mindspore_nn_optim_momentum_Momentum_construct_67}
#   8: @mindspore_nn_optim_momentum_Momentum_construct_64:CNode_75{[0]: ValueNode<Primitive> Return, [1]: CNode_69}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Identity_construct_63 : 0x140edc818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:505/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Identity_construct_63(%para34_x) {
  Return(%para34_x)
      : (<null>)
      #scope: (Default/grad_reducer-Identity)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:506/        return x/
}
# Order:
#   1: @mindspore_nn_layer_basic_Identity_construct_63:CNode_76{[0]: ValueNode<Primitive> Return, [1]: param_x}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_62 : 0x140eefa18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:120/    def construct(self, data, label):/
subgraph @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_62 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4](%para35_data, %para36_label) {
  %1(out) = call @__main___LeNet5_construct_77(%para35_data)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:121/        out = self._backbone(data)/
  %2(CNode_40) = call @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_78(%1, %para36_label)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:122/        return self._loss_fn(out, label)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:122/        return self._loss_fn(out, label)/
}
# Order:
#   1: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_62:out{[0]: ValueNode<FuncGraph> __main___LeNet5_construct_77, [1]: param_data}
#   2: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_62:CNode_40{[0]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_78, [1]: out, [2]: param_label}
#   3: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_62:CNode_41{[0]: ValueNode<Primitive> Return, [1]: CNode_40}


subgraph attr:
training : 1
subgraph instance: ✗_no_sens_impl_61 : 0x140f35418
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @✗_no_sens_impl_61 parent: [subgraph @_no_sens_impl_24]() {
  %1(CNode_36) = call @↓_no_sens_impl_79()
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:437/        if self.return_grad:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:437/        if self.return_grad:/
}
# Order:
#   1: @✗_no_sens_impl_61:CNode_36{[0]: ValueNode<FuncGraph> ↓_no_sens_impl_79}
#   2: @✗_no_sens_impl_61:CNode_37{[0]: ValueNode<Primitive> Return, [1]: CNode_36}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: scale_grad_73 : 0x140eea018
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:471/    def scale_grad(self, gradients):/
subgraph @scale_grad_73(%para37_gradients) {
  %1(CNode_81) = call @✗scale_grad_80()
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:485/        if self.need_scale:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:485/        if self.need_scale:/
}
# Order:
#   1: @scale_grad_73:CNode_81{[0]: ValueNode<FuncGraph> ✗scale_grad_80}
#   2: @scale_grad_73:CNode_82{[0]: ValueNode<Primitive> Return, [1]: CNode_81}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: gradients_centralization_72 : 0x140ee6e18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:452/    def gradients_centralization(self, gradients):/
subgraph @gradients_centralization_72(%para38_gradients) {
  %1(CNode_84) = call @✗gradients_centralization_83()
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:466/        if self.is_group:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:466/        if self.is_group:/
}
# Order:
#   1: @gradients_centralization_72:CNode_84{[0]: ValueNode<FuncGraph> ✗gradients_centralization_83}
#   2: @gradients_centralization_72:CNode_85{[0]: ValueNode<Primitive> Return, [1]: CNode_84}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: decay_weight_71 : 0x140ee3c18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:429/    def decay_weight(self, gradients):/
subgraph @decay_weight_71(%para39_gradients) {
  %1(CNode_87) = call @✗decay_weight_86()
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:442/        if self.exec_weight_decay:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:442/        if self.exec_weight_decay:/
}
# Order:
#   1: @decay_weight_71:CNode_87{[0]: ValueNode<FuncGraph> ✗decay_weight_86}
#   2: @decay_weight_71:CNode_88{[0]: ValueNode<Primitive> Return, [1]: CNode_87}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: flatten_gradients_70 : 0x140ed3618
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:410/    def flatten_gradients(self, gradients):/
subgraph @flatten_gradients_70(%para40_gradients) {
  %1(CNode_90) = call @✗flatten_gradients_89()
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:424/        if self._use_flattened_params:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:424/        if self._use_flattened_params:/
}
# Order:
#   1: @flatten_gradients_70:CNode_90{[0]: ValueNode<FuncGraph> ✗flatten_gradients_89}
#   2: @flatten_gradients_70:CNode_91{[0]: ValueNode<Primitive> Return, [1]: CNode_90}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: get_lr_74 : 0x140ee8e18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:739/    def get_lr(self):/
subgraph @get_lr_74 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4]() {
  %1(CNode_93) = call @✗get_lr_92()
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:748/        if self.dynamic_lr:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:748/        if self.dynamic_lr:/
}
# Order:
#   1: @get_lr_74:CNode_93{[0]: ValueNode<FuncGraph> ✗get_lr_92}
#   2: @get_lr_74:CNode_94{[0]: ValueNode<Primitive> Return, [1]: CNode_93}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ✗mindspore_nn_optim_momentum_Momentum_construct_67 : 0x140eeb218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:215/    def construct(self, gradients):/
subgraph @✗mindspore_nn_optim_momentum_Momentum_construct_67 parent: [subgraph @mindspore_nn_optim_momentum_Momentum_construct_64]() {
  %1(CNode_96) = call @2✗mindspore_nn_optim_momentum_Momentum_construct_95()
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:234/            if self.is_group_lr:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:234/            if self.is_group_lr:/
}
# Order:
#   1: @✗mindspore_nn_optim_momentum_Momentum_construct_67:CNode_96{[0]: ValueNode<FuncGraph> 2✗mindspore_nn_optim_momentum_Momentum_construct_95}
#   2: @✗mindspore_nn_optim_momentum_Momentum_construct_67:CNode_97{[0]: ValueNode<Primitive> Return, [1]: CNode_96}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_78 : 0x140f30c18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_78(%para41_logits, %para42_labels) {
  %1(CNode_98) = S_Prim__check_is_tensor[constexpr_prim: Bool(1)]("logits", %para41_logits, "SoftmaxCrossEntropyWithLogits")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:778/        _check_is_tensor('logits', logits, self.cls_name)/
  %2(CNode_99) = S_Prim__check_is_tensor[constexpr_prim: Bool(1)]("labels", %para42_labels, "SoftmaxCrossEntropyWithLogits")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:779/        _check_is_tensor('labels', labels, self.cls_name)/
  %3(CNode_100) = MakeTuple(%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:777/    def construct(self, logits, labels):/
  %4(CNode_101) = StopGradient(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:777/    def construct(self, logits, labels):/
  %5(CNode_103) = call @✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_102()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:780/        if self.sparse:/
  %6(CNode_104) = Depend[side_effect_propagate: I64(1)](%5, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:780/        if self.sparse:/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:780/        if self.sparse:/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_78:CNode_98{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_is_tensor, [1]: ValueNode<StringImm> logits, [2]: param_logits, [3]: ValueNode<StringImm> SoftmaxCrossEntropyWithLogits}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_78:CNode_99{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_is_tensor, [1]: ValueNode<StringImm> labels, [2]: param_labels, [3]: ValueNode<StringImm> SoftmaxCrossEntropyWithLogits}
#   3: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_78:CNode_103{[0]: ValueNode<FuncGraph> ✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_102}
#   4: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_78:CNode_105{[0]: ValueNode<Primitive> Return, [1]: CNode_104}


subgraph attr:
training : 1
subgraph instance: __main___LeNet5_construct_77 : 0x140ef0818
# In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:118/    def construct(self, x):/
subgraph @__main___LeNet5_construct_77 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4](%para43_x) {
  %1(CNode_43) = call @mindspore_nn_layer_conv_Conv2d_construct_106(%para43_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:119/        x = self.max_pool2d(self.relu(self.conv1(x)))/
  %2(CNode_45) = call @mindspore_nn_layer_activation_ReLU_construct_107(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:119/        x = self.max_pool2d(self.relu(self.conv1(x)))/
  %3(x) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_108(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:119/        x = self.max_pool2d(self.relu(self.conv1(x)))/
  %4(CNode_47) = call @mindspore_nn_layer_conv_Conv2d_construct_109(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:120/        x = self.max_pool2d(self.relu(self.conv2(x)))/
  %5(CNode_48) = call @mindspore_nn_layer_activation_ReLU_construct_107(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:120/        x = self.max_pool2d(self.relu(self.conv2(x)))/
  %6(x) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_108(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:120/        x = self.max_pool2d(self.relu(self.conv2(x)))/
  %7(x) = call @mindspore_nn_layer_basic_Flatten_construct_110(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:121/        x = self.flatten(x)/
  %8(CNode_51) = call @mindspore_nn_layer_basic_Dense_construct_111(%7)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:122/        x = self.relu(self.fc1(x))/
  %9(x) = call @mindspore_nn_layer_activation_ReLU_construct_107(%8)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:122/        x = self.relu(self.fc1(x))/
  %10(CNode_53) = call @mindspore_nn_layer_basic_Dense_construct_112(%9)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:123/        x = self.relu(self.fc2(x))/
  %11(x) = call @mindspore_nn_layer_activation_ReLU_construct_107(%10)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:123/        x = self.relu(self.fc2(x))/
  %12(x) = call @mindspore_nn_layer_basic_Dense_construct_113(%11)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:124/        x = self.fc3(x)/
  Return(%12)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:125/        return x/
}
# Order:
#   1: @__main___LeNet5_construct_77:CNode_43{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_106, [1]: param_x}
#   2: @__main___LeNet5_construct_77:CNode_45{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_107, [1]: CNode_43}
#   3: @__main___LeNet5_construct_77:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_108, [1]: CNode_45}
#   4: @__main___LeNet5_construct_77:CNode_47{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_109, [1]: x}
#   5: @__main___LeNet5_construct_77:CNode_48{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_107, [1]: CNode_47}
#   6: @__main___LeNet5_construct_77:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_108, [1]: CNode_48}
#   7: @__main___LeNet5_construct_77:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Flatten_construct_110, [1]: x}
#   8: @__main___LeNet5_construct_77:CNode_51{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_111, [1]: x}
#   9: @__main___LeNet5_construct_77:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_107, [1]: CNode_51}
#  10: @__main___LeNet5_construct_77:CNode_53{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_112, [1]: x}
#  11: @__main___LeNet5_construct_77:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_107, [1]: CNode_53}
#  12: @__main___LeNet5_construct_77:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_113, [1]: x}
#  13: @__main___LeNet5_construct_77:CNode_55{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: ↓_no_sens_impl_79 : 0x140f35a18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @↓_no_sens_impl_79 parent: [subgraph @_no_sens_impl_24]() {
  %1(loss) = $(_no_sens_impl_24):UnpackCall_unpack_call(@mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_62, %para32_inputs)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:433/        loss = self.network(*inputs)/
  %2(grads) = $(_no_sens_impl_24):UnpackGraph(@mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_62, %para32_inputs)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %3(CNode_30) = $(_no_sens_impl_24):MakeTuple(%para3_conv1.weight, %para4_conv2.weight, %para5_fc1.weight, %para6_fc1.bias, %para7_fc2.weight, %para8_fc2.bias, %para9_fc3.weight, %para10_fc3.bias)
      : (<Ref[Tensor[Float32]], (32, 1, 5, 5), ref_key=:conv1.weight>, <Ref[Tensor[Float32]], (16, 64, 5, 5), ref_key=:conv2.weight>, <Ref[Tensor[Float32]], (128, 1600), ref_key=:fc1.weight>, <Ref[Tensor[Float32]], (128), ref_key=:fc1.bias>, <Ref[Tensor[Float32]], (256, 128), ref_key=:fc2.weight>, <Ref[Tensor[Float32]], (256), ref_key=:fc2.bias>, <Ref[Tensor[Float32]], (10, 256), ref_key=:fc3.weight>, <Ref[Tensor[Float32]], (10), ref_key=:fc3.bias>) -> (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %4(grads) = $(_no_sens_impl_24):S_Prim_grad(%2, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %5(grads) = $(_no_sens_impl_24):UnpackCall_unpack_call(%4, %para32_inputs)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %6(grads) = $(_no_sens_impl_24):call @mindspore_nn_layer_basic_Identity_construct_63(%5)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:435/        grads = self.grad_reducer(grads)/
  %7(CNode_33) = $(_no_sens_impl_24):call @mindspore_nn_optim_momentum_Momentum_construct_64(%6)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:436/        loss = F.depend(loss, self.optimizer(grads))/
  %8(loss) = $(_no_sens_impl_24):S_Prim_Depend[side_effect_propagate: I64(1)](%1, %7)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:436/        loss = F.depend(loss, self.optimizer(grads))/
  Return(%8)
      : (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:442/        return loss/
}
# Order:
#   1: @↓_no_sens_impl_79:CNode_38{[0]: ValueNode<Primitive> Return, [1]: loss}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ✗scale_grad_80 : 0x140eea618
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:471/    def scale_grad(self, gradients):/
subgraph @✗scale_grad_80 parent: [subgraph @scale_grad_73]() {
  %1(CNode_115) = call @↓scale_grad_114()
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:485/        if self.need_scale:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:485/        if self.need_scale:/
}
# Order:
#   1: @✗scale_grad_80:CNode_115{[0]: ValueNode<FuncGraph> ↓scale_grad_114}
#   2: @✗scale_grad_80:CNode_116{[0]: ValueNode<Primitive> Return, [1]: CNode_115}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ✗gradients_centralization_83 : 0x140ee7418
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:452/    def gradients_centralization(self, gradients):/
subgraph @✗gradients_centralization_83 parent: [subgraph @gradients_centralization_72]() {
  %1(CNode_118) = call @↓gradients_centralization_117()
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:466/        if self.is_group:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:466/        if self.is_group:/
}
# Order:
#   1: @✗gradients_centralization_83:CNode_118{[0]: ValueNode<FuncGraph> ↓gradients_centralization_117}
#   2: @✗gradients_centralization_83:CNode_119{[0]: ValueNode<Primitive> Return, [1]: CNode_118}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ✗decay_weight_86 : 0x140ee4218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:429/    def decay_weight(self, gradients):/
subgraph @✗decay_weight_86 parent: [subgraph @decay_weight_71]() {
  %1(CNode_121) = call @↓decay_weight_120()
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:442/        if self.exec_weight_decay:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:442/        if self.exec_weight_decay:/
}
# Order:
#   1: @✗decay_weight_86:CNode_121{[0]: ValueNode<FuncGraph> ↓decay_weight_120}
#   2: @✗decay_weight_86:CNode_122{[0]: ValueNode<Primitive> Return, [1]: CNode_121}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ✗flatten_gradients_89 : 0x140ee3018
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:410/    def flatten_gradients(self, gradients):/
subgraph @✗flatten_gradients_89 parent: [subgraph @flatten_gradients_70]() {
  %1(CNode_124) = call @↓flatten_gradients_123()
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:424/        if self._use_flattened_params:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:424/        if self._use_flattened_params:/
}
# Order:
#   1: @✗flatten_gradients_89:CNode_124{[0]: ValueNode<FuncGraph> ↓flatten_gradients_123}
#   2: @✗flatten_gradients_89:CNode_125{[0]: ValueNode<Primitive> Return, [1]: CNode_124}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ✗get_lr_92 : 0x140ee9418
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:739/    def get_lr(self):/
subgraph @✗get_lr_92 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4]() {
  %1(CNode_127) = call @↓get_lr_126()
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:748/        if self.dynamic_lr:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:748/        if self.dynamic_lr:/
}
# Order:
#   1: @✗get_lr_92:CNode_127{[0]: ValueNode<FuncGraph> ↓get_lr_126}
#   2: @✗get_lr_92:CNode_128{[0]: ValueNode<Primitive> Return, [1]: CNode_127}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: 2✗mindspore_nn_optim_momentum_Momentum_construct_95 : 0x140eeb818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:215/    def construct(self, gradients):/
subgraph @2✗mindspore_nn_optim_momentum_Momentum_construct_95 parent: [subgraph @mindspore_nn_optim_momentum_Momentum_construct_64]() {
  %1(CNode_130) = call @↓✗mindspore_nn_optim_momentum_Momentum_construct_129()
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:238/                success = self.hyper_map_reverse(F.partial(_momentum_opt, self.opt, self.momentum, lr),/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:238/                success = self.hyper_map_reverse(F.partial(_momentum_opt, self.opt, self.momentum, lr),/
}
# Order:
#   1: @2✗mindspore_nn_optim_momentum_Momentum_construct_95:CNode_131{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Partial, [1]: ValueNode<DoSignaturePrimitive> S_Prim_momentum_opt, [2]: ValueNode<DoSignaturePrimitive> S_Prim_ApplyMomentum, [3]: param_momentum, [4]: lr}
#   2: @2✗mindspore_nn_optim_momentum_Momentum_construct_95:success{[0]: ValueNode<DoSignaturePrimitive> S_Prim_hyper_map, [1]: CNode_131, [2]: gradients, [3]: CNode_132, [4]: CNode_133, [5]: ValueNode<ValueTuple> (false, false, false, false, false, false, false, false), [6]: ValueNode<ValueTuple> (false, false, false, false, false, false, false, false)}
#   3: @2✗mindspore_nn_optim_momentum_Momentum_construct_95:CNode_130{[0]: ValueNode<FuncGraph> ↓✗mindspore_nn_optim_momentum_Momentum_construct_129}
#   4: @2✗mindspore_nn_optim_momentum_Momentum_construct_95:CNode_134{[0]: ValueNode<Primitive> Return, [1]: CNode_130}


subgraph attr:
training : 1
subgraph instance: ✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_102 : 0x140f31218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:777/    def construct(self, logits, labels):/
subgraph @✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_102 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_78]() {
  %1(CNode_135) = S_Prim_equal("mean", "mean")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:781/            if self.reduction == 'mean':/
  %2(CNode_136) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:781/            if self.reduction == 'mean':/
  %3(CNode_137) = Switch(%2, @2✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_138, @✗✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_139)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:781/            if self.reduction == 'mean':/
  %4(CNode_140) = %3()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:781/            if self.reduction == 'mean':/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:781/            if self.reduction == 'mean':/
}
# Order:
#   1: @✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_102:CNode_135{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: ValueNode<StringImm> mean, [2]: ValueNode<StringImm> mean}
#   2: @✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_102:CNode_136{[0]: ValueNode<Primitive> Cond, [1]: CNode_135, [2]: ValueNode<BoolImm> false}
#   3: @✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_102:CNode_137{[0]: ValueNode<Primitive> Switch, [1]: CNode_136, [2]: ValueNode<FuncGraph> 2✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_138, [3]: ValueNode<FuncGraph> ✗✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_139}
#   4: @✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_102:CNode_140{[0]: CNode_137}
#   5: @✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_102:CNode_141{[0]: ValueNode<Primitive> Return, [1]: CNode_140}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct_113 : 0x140f30618
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct_113 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4](%para44_x) {
  %1(CNode_143) = call @L_mindspore_nn_layer_basic_Dense_construct_142(%para44_x, %para10_fc3.bias, %para9_fc3.weight)
      : (<null>, <Ref[Tensor[Float32]], (10), ref_key=:fc3.bias>, <Ref[Tensor[Float32]], (10, 256), ref_key=:fc3.weight>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct_113:CNode_143{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_142, [1]: param_x, [2]: param_fc3.bias, [3]: param_fc3.weight}
#   2: @mindspore_nn_layer_basic_Dense_construct_113:CNode_144{[0]: ValueNode<Primitive> Return, [1]: CNode_143}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_activation_ReLU_construct_107 : 0x140f30018
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/activation.py:437/    def construct(self, x):/
subgraph @mindspore_nn_layer_activation_ReLU_construct_107(%para45_x) {
  %1(CNode_145) = S_Prim_ReLU[output_names: ["output"], input_names: ["x"]](%para45_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/relu-ReLU)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/activation.py:438/        return self.relu(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/relu-ReLU)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/activation.py:438/        return self.relu(x)/
}
# Order:
#   1: @mindspore_nn_layer_activation_ReLU_construct_107:CNode_145{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: param_x}
#   2: @mindspore_nn_layer_activation_ReLU_construct_107:CNode_146{[0]: ValueNode<Primitive> Return, [1]: CNode_145}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct_112 : 0x140f2fa18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct_112 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4](%para46_x) {
  %1(CNode_147) = call @L_mindspore_nn_layer_basic_Dense_construct_142(%para46_x, %para8_fc2.bias, %para7_fc2.weight)
      : (<null>, <Ref[Tensor[Float32]], (256), ref_key=:fc2.bias>, <Ref[Tensor[Float32]], (256, 128), ref_key=:fc2.weight>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc2-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct_112:CNode_147{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_142, [1]: param_x, [2]: param_fc2.bias, [3]: param_fc2.weight}
#   2: @mindspore_nn_layer_basic_Dense_construct_112:CNode_148{[0]: ValueNode<Primitive> Return, [1]: CNode_147}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct_111 : 0x140f0c418
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct_111 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4](%para47_x) {
  %1(CNode_149) = call @L_mindspore_nn_layer_basic_Dense_construct_142(%para47_x, %para6_fc1.bias, %para5_fc1.weight)
      : (<null>, <Ref[Tensor[Float32]], (128), ref_key=:fc1.bias>, <Ref[Tensor[Float32]], (128, 1600), ref_key=:fc1.weight>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc1-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct_111:CNode_149{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_142, [1]: param_x, [2]: param_fc1.bias, [3]: param_fc1.weight}
#   2: @mindspore_nn_layer_basic_Dense_construct_111:CNode_150{[0]: ValueNode<Primitive> Return, [1]: CNode_149}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Flatten_construct_110 : 0x140f04618
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:461/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Flatten_construct_110(%para48_x) {
  %1(x_rank) = call @rank_151(%para48_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:462/        x_rank = F.rank(x)/
  %2(CNode_152) = S_Prim_not_equal(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
  %3(CNode_153) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
  %4(CNode_154) = Switch(%3, @↰mindspore_nn_layer_basic_Flatten_construct_155, @↱mindspore_nn_layer_basic_Flatten_construct_156)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
  %5(ndim) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
  %6(CNode_158) = call @check_axis_valid_157(I64(1), %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:464/        self.check_axis_valid(self.start_dim, ndim)/
  %7(CNode_159) = call @check_axis_valid_157(I64(-1), %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:465/        self.check_axis_valid(self.end_dim, ndim)/
  %8(CNode_160) = MakeTuple(%6, %7)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:461/    def construct(self, x):/
  %9(CNode_161) = StopGradient(%8)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:461/    def construct(self, x):/
  %10(CNode_162) = S_Prim_MakeTuple(%para48_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %11(CNode_163) = S_Prim_MakeTuple("start_dim", "end_dim")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %12(CNode_164) = S_Prim_MakeTuple(I64(1), I64(-1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %13(CNode_165) = S_Prim_make_dict(%11, %12)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %14(CNode_166) = UnpackCall_unpack_call(@flatten_167, %10, %13)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %15(CNode_168) = Depend[side_effect_propagate: I64(1)](%14, %9)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%15)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
}
# Order:
#   1: @mindspore_nn_layer_basic_Flatten_construct_110:x_rank{[0]: ValueNode<FuncGraph> rank_151, [1]: param_x}
#   2: @mindspore_nn_layer_basic_Flatten_construct_110:CNode_152{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: x_rank, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_basic_Flatten_construct_110:CNode_153{[0]: ValueNode<Primitive> Cond, [1]: CNode_152, [2]: ValueNode<BoolImm> false}
#   4: @mindspore_nn_layer_basic_Flatten_construct_110:CNode_154{[0]: ValueNode<Primitive> Switch, [1]: CNode_153, [2]: ValueNode<FuncGraph> ↰mindspore_nn_layer_basic_Flatten_construct_155, [3]: ValueNode<FuncGraph> ↱mindspore_nn_layer_basic_Flatten_construct_156}
#   5: @mindspore_nn_layer_basic_Flatten_construct_110:ndim{[0]: CNode_154}
#   6: @mindspore_nn_layer_basic_Flatten_construct_110:CNode_158{[0]: ValueNode<FuncGraph> check_axis_valid_157, [1]: ValueNode<Int64Imm> 1, [2]: ndim}
#   7: @mindspore_nn_layer_basic_Flatten_construct_110:CNode_159{[0]: ValueNode<FuncGraph> check_axis_valid_157, [1]: ValueNode<Int64Imm> -1, [2]: ndim}
#   8: @mindspore_nn_layer_basic_Flatten_construct_110:CNode_162{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: param_x}
#   9: @mindspore_nn_layer_basic_Flatten_construct_110:CNode_163{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<StringImm> start_dim, [2]: ValueNode<StringImm> end_dim}
#  10: @mindspore_nn_layer_basic_Flatten_construct_110:CNode_164{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<Int64Imm> 1, [2]: ValueNode<Int64Imm> -1}
#  11: @mindspore_nn_layer_basic_Flatten_construct_110:CNode_165{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_dict, [1]: CNode_163, [2]: CNode_164}
#  12: @mindspore_nn_layer_basic_Flatten_construct_110:CNode_166{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.169, [1]: ValueNode<FuncGraph> flatten_167, [2]: CNode_162, [3]: CNode_165}
#  13: @mindspore_nn_layer_basic_Flatten_construct_110:CNode_170{[0]: ValueNode<Primitive> Return, [1]: CNode_168}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_108 : 0x140ef8618
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_108(%para49_x) {
  %1(CNode_171) = getattr(%para49_x, "ndim")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:570/        if x.ndim == 3:/
  %2(CNode_172) = S_Prim_equal(%1, I64(3))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:570/        if x.ndim == 3:/
  %3(CNode_173) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:570/        if x.ndim == 3:/
  %4(CNode_174) = Switch(%3, @✓mindspore_nn_layer_pooling_MaxPool2d_construct_175, @✗mindspore_nn_layer_pooling_MaxPool2d_construct_176)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:570/        if x.ndim == 3:/
  %5(CNode_177) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:570/        if x.ndim == 3:/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:570/        if x.ndim == 3:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_108:CNode_171{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> ndim}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_108:CNode_172{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: CNode_171, [2]: ValueNode<Int64Imm> 3}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_108:CNode_173{[0]: ValueNode<Primitive> Cond, [1]: CNode_172, [2]: ValueNode<BoolImm> false}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_108:CNode_174{[0]: ValueNode<Primitive> Switch, [1]: CNode_173, [2]: ValueNode<FuncGraph> ✓mindspore_nn_layer_pooling_MaxPool2d_construct_175, [3]: ValueNode<FuncGraph> ✗mindspore_nn_layer_pooling_MaxPool2d_construct_176}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_108:CNode_177{[0]: CNode_174}
#   6: @mindspore_nn_layer_pooling_MaxPool2d_construct_108:CNode_178{[0]: ValueNode<Primitive> Return, [1]: CNode_177}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_109 : 0x140ef7418
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_109 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4](%para50_x) {
  %1(CNode_56) = call @✗mindspore_nn_layer_conv_Conv2d_construct_179()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv2-Conv2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv2-Conv2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_109:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_conv2.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_109:CNode_56{[0]: ValueNode<FuncGraph> ✗mindspore_nn_layer_conv_Conv2d_construct_179}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_109:CNode_57{[0]: ValueNode<Primitive> Return, [1]: CNode_56}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_106 : 0x140ee8218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_106 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4](%para51_x) {
  %1(CNode_181) = call @✗mindspore_nn_layer_conv_Conv2d_construct_180()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv1-Conv2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv1-Conv2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_106:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_conv1.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_106:CNode_181{[0]: ValueNode<FuncGraph> ✗mindspore_nn_layer_conv_Conv2d_construct_180}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_106:CNode_182{[0]: ValueNode<Primitive> Return, [1]: CNode_181}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ↓scale_grad_114 : 0x140eeac18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:471/    def scale_grad(self, gradients):/
subgraph @↓scale_grad_114 parent: [subgraph @scale_grad_73]() {
  Return(%para37_gradients)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:488/        return gradients/
}
# Order:
#   1: @↓scale_grad_114:CNode_183{[0]: ValueNode<Primitive> Return, [1]: param_gradients}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ↓gradients_centralization_117 : 0x140ee8818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:452/    def gradients_centralization(self, gradients):/
subgraph @↓gradients_centralization_117 parent: [subgraph @gradients_centralization_72]() {
  Return(%para38_gradients)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:469/        return gradients/
}
# Order:
#   1: @↓gradients_centralization_117:CNode_184{[0]: ValueNode<Primitive> Return, [1]: param_gradients}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ↓decay_weight_120 : 0x140ee4818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:429/    def decay_weight(self, gradients):/
subgraph @↓decay_weight_120 parent: [subgraph @decay_weight_71]() {
  Return(%para39_gradients)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:450/        return gradients/
}
# Order:
#   1: @↓decay_weight_120:CNode_185{[0]: ValueNode<Primitive> Return, [1]: param_gradients}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ↓flatten_gradients_123 : 0x140ee3618
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:410/    def flatten_gradients(self, gradients):/
subgraph @↓flatten_gradients_123 parent: [subgraph @flatten_gradients_70]() {
  Return(%para40_gradients)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:427/        return gradients/
}
# Order:
#   1: @↓flatten_gradients_123:CNode_186{[0]: ValueNode<Primitive> Return, [1]: param_gradients}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ↓get_lr_126 : 0x140ee9a18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:739/    def get_lr(self):/
subgraph @↓get_lr_126 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4]() {
  Return(%para21_learning_rate)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:756/        return lr/
}
# Order:
#   1: @↓get_lr_126:CNode_187{[0]: ValueNode<Primitive> Return, [1]: param_learning_rate}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ↓✗mindspore_nn_optim_momentum_Momentum_construct_129 : 0x140ed2018
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:215/    def construct(self, gradients):/
subgraph @↓✗mindspore_nn_optim_momentum_Momentum_construct_129 parent: [subgraph @2✗mindspore_nn_optim_momentum_Momentum_construct_95]() {
  %1(CNode_189) = call @↓mindspore_nn_optim_momentum_Momentum_construct_188()
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:234/            if self.is_group_lr:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:234/            if self.is_group_lr:/
}
# Order:
#   1: @↓✗mindspore_nn_optim_momentum_Momentum_construct_129:CNode_189{[0]: ValueNode<FuncGraph> ↓mindspore_nn_optim_momentum_Momentum_construct_188}
#   2: @↓✗mindspore_nn_optim_momentum_Momentum_construct_129:CNode_190{[0]: ValueNode<Primitive> Return, [1]: CNode_189}


subgraph attr:
training : 1
subgraph instance: 2✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_138 : 0x140f34e18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:777/    def construct(self, logits, labels):/
subgraph @2✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_138 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_78]() {
  %1(x) = S_Prim_SparseSoftmaxCrossEntropyWithLogits[output_names: ["output"], input_names: ["features", "labels"], sens: F32(1), is_grad: Bool(0)](%para41_logits, %para42_labels)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:782/                x = self.sparse_softmax_cross_entropy(logits, labels)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:783/                return x/
}
# Order:
#   1: @2✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_138:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_SparseSoftmaxCrossEntropyWithLogits, [1]: param_logits, [2]: param_labels}
#   2: @2✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_138:CNode_191{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: ✗✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_139 : 0x140f31818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:777/    def construct(self, logits, labels):/
subgraph @✗✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_139 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_78]() {
  %1(CNode_193) = call @↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_192()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:781/            if self.reduction == 'mean':/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:781/            if self.reduction == 'mean':/
}
# Order:
#   1: @✗✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_139:CNode_193{[0]: ValueNode<FuncGraph> ↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_192}
#   2: @✗✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_139:CNode_194{[0]: ValueNode<Primitive> Return, [1]: CNode_193}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_142 : 0x140ef2618
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_142(%para52_x, %para53_, %para54_) {
  %1(x_shape) = S_Prim_Shape(%para52_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:623/        x_shape = self.shape_op(x)/
  %2(CNode_195) = S_Prim_check_dense_input_shape[constexpr_prim: Bool(1)](%1, "Dense")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:624/        check_dense_input_shape(x_shape, self.cls_name)/
  %3(CNode_196) = StopGradient(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
  %4(CNode_197) = S_Prim_inner_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
  %5(CNode_198) = S_Prim_not_equal(%4, I64(2))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
  %6(CNode_199) = Cond(%5, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
  %7(CNode_200) = Switch(%6, @L_✓mindspore_nn_layer_basic_Dense_construct_201, @L_✗mindspore_nn_layer_basic_Dense_construct_202)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
  %8(CNode_203) = %7()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
  %9(CNode_205) = call @L_↓mindspore_nn_layer_basic_Dense_construct_204(%8)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:124/        x = self.fc3(x)/
  %10(CNode_206) = Depend[side_effect_propagate: I64(1)](%9, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:124/        x = self.fc3(x)/
  Return(%10)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_142:x_shape{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_x}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_142:CNode_195{[0]: ValueNode<DoSignaturePrimitive> S_Prim_check_dense_input_shape, [1]: x_shape, [2]: ValueNode<StringImm> Dense}
#   3: @L_mindspore_nn_layer_basic_Dense_construct_142:CNode_197{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: x_shape}
#   4: @L_mindspore_nn_layer_basic_Dense_construct_142:CNode_198{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: CNode_197, [2]: ValueNode<Int64Imm> 2}
#   5: @L_mindspore_nn_layer_basic_Dense_construct_142:CNode_199{[0]: ValueNode<Primitive> Cond, [1]: CNode_198, [2]: ValueNode<BoolImm> false}
#   6: @L_mindspore_nn_layer_basic_Dense_construct_142:CNode_200{[0]: ValueNode<Primitive> Switch, [1]: CNode_199, [2]: ValueNode<FuncGraph> L_✓mindspore_nn_layer_basic_Dense_construct_201, [3]: ValueNode<FuncGraph> L_✗mindspore_nn_layer_basic_Dense_construct_202}
#   7: @L_mindspore_nn_layer_basic_Dense_construct_142:CNode_203{[0]: CNode_200}
#   8: @L_mindspore_nn_layer_basic_Dense_construct_142:CNode_205{[0]: ValueNode<FuncGraph> L_↓mindspore_nn_layer_basic_Dense_construct_204, [1]: CNode_203}
#   9: @L_mindspore_nn_layer_basic_Dense_construct_142:CNode_206{[0]: ValueNode<Primitive> Depend, [1]: CNode_205, [2]: CNode_196}
#  10: @L_mindspore_nn_layer_basic_Dense_construct_142:CNode_144{[0]: ValueNode<Primitive> Return, [1]: CNode_206}


subgraph attr:
training : 1
subgraph instance: check_axis_valid_157 : 0x140f04c18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @check_axis_valid_157(%para55_axis, %para56_ndim) {
  %1(CNode_207) = S_Prim_negative(%para56_ndim)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  %2(CNode_208) = S_Prim_less(%para55_axis, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  %3(CNode_209) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  %4(CNode_210) = Switch(%3, @↰check_axis_valid_211, @↱check_axis_valid_212)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  %5(CNode_213) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  %6(CNode_214) = Cond(%5, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  %7(CNode_215) = Switch(%6, @✓check_axis_valid_216, @✗check_axis_valid_217)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  %8(CNode_218) = %7()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_157:CNode_207{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: param_ndim}
#   2: @check_axis_valid_157:CNode_208{[0]: ValueNode<DoSignaturePrimitive> S_Prim_less, [1]: param_axis, [2]: CNode_207}
#   3: @check_axis_valid_157:CNode_209{[0]: ValueNode<Primitive> Cond, [1]: CNode_208, [2]: ValueNode<BoolImm> false}
#   4: @check_axis_valid_157:CNode_210{[0]: ValueNode<Primitive> Switch, [1]: CNode_209, [2]: ValueNode<FuncGraph> ↰check_axis_valid_211, [3]: ValueNode<FuncGraph> ↱check_axis_valid_212}
#   5: @check_axis_valid_157:CNode_213{[0]: CNode_210}
#   6: @check_axis_valid_157:CNode_214{[0]: ValueNode<Primitive> Cond, [1]: CNode_213, [2]: ValueNode<BoolImm> false}
#   7: @check_axis_valid_157:CNode_215{[0]: ValueNode<Primitive> Switch, [1]: CNode_214, [2]: ValueNode<FuncGraph> ✓check_axis_valid_216, [3]: ValueNode<FuncGraph> ✗check_axis_valid_217}
#   8: @check_axis_valid_157:CNode_218{[0]: CNode_215}
#   9: @check_axis_valid_157:CNode_219{[0]: ValueNode<Primitive> Return, [1]: CNode_218}


subgraph attr:
subgraph instance: rank_151 : 0x140f0b218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1541/def rank(input_x):/
subgraph @rank_151(%para57_input_x) {
  %1(CNode_220) = S_Prim_Rank(%para57_input_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1571/    return rank_(input_x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1571/    return rank_(input_x)/
}
# Order:
#   1: @rank_151:CNode_220{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Rank, [1]: param_input_x}
#   2: @rank_151:CNode_221{[0]: ValueNode<Primitive> Return, [1]: CNode_220}


subgraph attr:
training : 1
subgraph instance: ↰mindspore_nn_layer_basic_Flatten_construct_155 : 0x140f0be18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
subgraph @↰mindspore_nn_layer_basic_Flatten_construct_155 parent: [subgraph @mindspore_nn_layer_basic_Flatten_construct_110]() {
  %1(x_rank) = $(mindspore_nn_layer_basic_Flatten_construct_110):call @rank_151(%para48_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:462/        x_rank = F.rank(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
}
# Order:
#   1: @↰mindspore_nn_layer_basic_Flatten_construct_155:CNode_222{[0]: ValueNode<Primitive> Return, [1]: x_rank}


subgraph attr:
training : 1
subgraph instance: ↱mindspore_nn_layer_basic_Flatten_construct_156 : 0x140f0b818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
subgraph @↱mindspore_nn_layer_basic_Flatten_construct_156() {
  Return(I64(1))
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
}
# Order:
#   1: @↱mindspore_nn_layer_basic_Flatten_construct_156:CNode_223{[0]: ValueNode<Primitive> Return, [1]: ValueNode<Int64Imm> 1}


subgraph attr:
subgraph instance: flatten_167 : 0x140ef0e18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_167(%para58_input, %para59_order, %para60_start_dim, %para61_end_dim) {
  %1(CNode_224) = S_Prim_isinstance(%para58_input, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1731/    if not isinstance(input, Tensor):/
  %2(CNode_225) = S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1731/    if not isinstance(input, Tensor):/
  %3(CNode_226) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1731/    if not isinstance(input, Tensor):/
  %4(CNode_227) = Switch(%3, @✓flatten_228, @✗flatten_229)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1731/    if not isinstance(input, Tensor):/
  %5(CNode_230) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1731/    if not isinstance(input, Tensor):/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1731/    if not isinstance(input, Tensor):/
}
# Order:
#   1: @flatten_167:CNode_224{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_input, [2]: ValueNode<ClassType> class 'mindspore.common.tensor.Tensor'}
#   2: @flatten_167:CNode_225{[0]: ValueNode<DoSignaturePrimitive> S_Prim_logical_not, [1]: CNode_224}
#   3: @flatten_167:CNode_226{[0]: ValueNode<Primitive> Cond, [1]: CNode_225, [2]: ValueNode<BoolImm> false}
#   4: @flatten_167:CNode_227{[0]: ValueNode<Primitive> Switch, [1]: CNode_226, [2]: ValueNode<FuncGraph> ✓flatten_228, [3]: ValueNode<FuncGraph> ✗flatten_229}
#   5: @flatten_167:CNode_230{[0]: CNode_227}
#   6: @flatten_167:CNode_231{[0]: ValueNode<Primitive> Return, [1]: CNode_230}


subgraph attr:
training : 1
subgraph instance: ✓mindspore_nn_layer_pooling_MaxPool2d_construct_175 : 0x140f04018
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @✓mindspore_nn_layer_pooling_MaxPool2d_construct_175 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_108]() {
  %1(CNode_232) = getattr(%para49_x, "unsqueeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:571/            x = x.unsqueeze(0)/
  %2(x) = %1(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:571/            x = x.unsqueeze(0)/
  %3(CNode_234) = call @↓mindspore_nn_layer_pooling_MaxPool2d_construct_233(%2, Bool(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:120/        x = self.max_pool2d(self.relu(self.conv2(x)))/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:571/            x = x.unsqueeze(0)/
}
# Order:
#   1: @✓mindspore_nn_layer_pooling_MaxPool2d_construct_175:CNode_232{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> unsqueeze}
#   2: @✓mindspore_nn_layer_pooling_MaxPool2d_construct_175:x{[0]: CNode_232, [1]: ValueNode<Int64Imm> 0}
#   3: @✓mindspore_nn_layer_pooling_MaxPool2d_construct_175:CNode_235{[0]: ValueNode<Primitive> Return, [1]: CNode_234}
#   4: @✓mindspore_nn_layer_pooling_MaxPool2d_construct_175:CNode_234{[0]: ValueNode<FuncGraph> ↓mindspore_nn_layer_pooling_MaxPool2d_construct_233, [1]: x, [2]: ValueNode<BoolImm> true}


subgraph attr:
training : 1
subgraph instance: ✗mindspore_nn_layer_pooling_MaxPool2d_construct_176 : 0x140ef8c18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @✗mindspore_nn_layer_pooling_MaxPool2d_construct_176 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_108]() {
  %1(CNode_236) = call @↓mindspore_nn_layer_pooling_MaxPool2d_construct_233(%para49_x, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:120/        x = self.max_pool2d(self.relu(self.conv2(x)))/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:570/        if x.ndim == 3:/
}
# Order:
#   1: @✗mindspore_nn_layer_pooling_MaxPool2d_construct_176:CNode_237{[0]: ValueNode<Primitive> Return, [1]: CNode_236}
#   2: @✗mindspore_nn_layer_pooling_MaxPool2d_construct_176:CNode_236{[0]: ValueNode<FuncGraph> ↓mindspore_nn_layer_pooling_MaxPool2d_construct_233, [1]: param_x, [2]: ValueNode<BoolImm> false}


subgraph attr:
training : 1
subgraph instance: ✗mindspore_nn_layer_conv_Conv2d_construct_179 : 0x140ef7a18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:360/    def construct(self, x):/
subgraph @✗mindspore_nn_layer_conv_Conv2d_construct_179 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_109]() {
  %1(CNode_58) = call @↓mindspore_nn_layer_conv_Conv2d_construct_238()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv2-Conv2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv2-Conv2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @✗mindspore_nn_layer_conv_Conv2d_construct_179:CNode_58{[0]: ValueNode<FuncGraph> ↓mindspore_nn_layer_conv_Conv2d_construct_238}
#   2: @✗mindspore_nn_layer_conv_Conv2d_construct_179:CNode_59{[0]: ValueNode<Primitive> Return, [1]: CNode_58}


subgraph attr:
training : 1
subgraph instance: ✗mindspore_nn_layer_conv_Conv2d_construct_180 : 0x140ef4818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:360/    def construct(self, x):/
subgraph @✗mindspore_nn_layer_conv_Conv2d_construct_180 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_106]() {
  %1(CNode_240) = call @↓mindspore_nn_layer_conv_Conv2d_construct_239()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv1-Conv2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv1-Conv2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @✗mindspore_nn_layer_conv_Conv2d_construct_180:CNode_240{[0]: ValueNode<FuncGraph> ↓mindspore_nn_layer_conv_Conv2d_construct_239}
#   2: @✗mindspore_nn_layer_conv_Conv2d_construct_180:CNode_241{[0]: ValueNode<Primitive> Return, [1]: CNode_240}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ↓mindspore_nn_optim_momentum_Momentum_construct_188 : 0x140ed2618
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:215/    def construct(self, gradients):/
subgraph @↓mindspore_nn_optim_momentum_Momentum_construct_188 parent: [subgraph @2✗mindspore_nn_optim_momentum_Momentum_construct_95]() {
  %1(lr) = $(mindspore_nn_optim_momentum_Momentum_construct_64):call @get_lr_74()
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:222/        lr = self.get_lr()/
  %2(CNode_131) = $(2✗mindspore_nn_optim_momentum_Momentum_construct_95):S_Prim_Partial[side_effect_propagate: I64(1)](S_Prim_momentum_opt, S_Prim_ApplyMomentum[output_names: ["output"], side_effect_mem: Bool(1), use_nesterov: Bool(0), input_names: ["variable", "accumulation", "learning_rate", "gradient", "momentum"], use_locking: Bool(0), gradient_scale: F32(1)], %para20_momentum, %1)
      : (<null>, <null>, <Ref[Tensor[Float32]], (), ref_key=:momentum>, <null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:238/                success = self.hyper_map_reverse(F.partial(_momentum_opt, self.opt, self.momentum, lr),/
  %3(gradients) = $(mindspore_nn_optim_momentum_Momentum_construct_64):call @flatten_gradients_70(%para33_gradients)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:218/        gradients = self.flatten_gradients(gradients)/
  %4(gradients) = $(mindspore_nn_optim_momentum_Momentum_construct_64):call @decay_weight_71(%3)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:219/        gradients = self.decay_weight(gradients)/
  %5(gradients) = $(mindspore_nn_optim_momentum_Momentum_construct_64):call @gradients_centralization_72(%4)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:220/        gradients = self.gradients_centralization(gradients)/
  %6(gradients) = $(mindspore_nn_optim_momentum_Momentum_construct_64):call @scale_grad_73(%5)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:221/        gradients = self.scale_grad(gradients)/
  %7(CNode_132) = $(mindspore_nn_optim_momentum_Momentum_construct_64):MakeTuple(%para3_conv1.weight, %para4_conv2.weight, %para5_fc1.weight, %para6_fc1.bias, %para7_fc2.weight, %para8_fc2.bias, %para9_fc3.weight, %para10_fc3.bias)
      : (<Ref[Tensor[Float32]], (32, 1, 5, 5), ref_key=:conv1.weight>, <Ref[Tensor[Float32]], (16, 64, 5, 5), ref_key=:conv2.weight>, <Ref[Tensor[Float32]], (128, 1600), ref_key=:fc1.weight>, <Ref[Tensor[Float32]], (128), ref_key=:fc1.bias>, <Ref[Tensor[Float32]], (256, 128), ref_key=:fc2.weight>, <Ref[Tensor[Float32]], (256), ref_key=:fc2.bias>, <Ref[Tensor[Float32]], (10, 256), ref_key=:fc3.weight>, <Ref[Tensor[Float32]], (10), ref_key=:fc3.bias>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:216/        params = self.params/
  %8(CNode_133) = $(mindspore_nn_optim_momentum_Momentum_construct_64):MakeTuple(%para12_moments.conv1.weight, %para13_moments.conv2.weight, %para14_moments.fc1.weight, %para15_moments.fc1.bias, %para16_moments.fc2.weight, %para17_moments.fc2.bias, %para18_moments.fc3.weight, %para19_moments.fc3.bias)
      : (<Ref[Tensor[Float32]], (32, 1, 5, 5), ref_key=:moments.conv1.weight>, <Ref[Tensor[Float32]], (16, 64, 5, 5), ref_key=:moments.conv2.weight>, <Ref[Tensor[Float32]], (128, 1600), ref_key=:moments.fc1.weight>, <Ref[Tensor[Float32]], (128), ref_key=:moments.fc1.bias>, <Ref[Tensor[Float32]], (256, 128), ref_key=:moments.fc2.weight>, <Ref[Tensor[Float32]], (256), ref_key=:moments.fc2.bias>, <Ref[Tensor[Float32]], (10, 256), ref_key=:moments.fc3.weight>, <Ref[Tensor[Float32]], (10), ref_key=:moments.fc3.bias>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:217/        moments = self.moments/
  %9(success) = $(2✗mindspore_nn_optim_momentum_Momentum_construct_95):S_Prim_hyper_map(%2, %6, %7, %8, (Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0)), (Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0)))
      : (<null>, <null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:238/                success = self.hyper_map_reverse(F.partial(_momentum_opt, self.opt, self.momentum, lr),/
  Return(%9)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:240/        return success/
}
# Order:
#   1: @↓mindspore_nn_optim_momentum_Momentum_construct_188:CNode_242{[0]: ValueNode<Primitive> Return, [1]: success}


subgraph attr:
after_block : 1
training : 1
subgraph instance: ↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_192 : 0x140f31e18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:777/    def construct(self, logits, labels):/
subgraph @↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_192 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_78]() {
  %1(CNode_244) = call @↓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_243()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:781/            if self.reduction == 'mean':/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:781/            if self.reduction == 'mean':/
}
# Order:
#   1: @↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_192:CNode_245{[0]: ValueNode<FuncGraph> shape_246, [1]: param_logits}
#   2: @↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_192:CNode_247{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   3: @↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_192:CNode_248{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_245, [2]: CNode_247}
#   4: @↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_192:labels{[0]: ValueNode<DoSignaturePrimitive> S_Prim_OneHot, [1]: param_labels, [2]: CNode_248, [3]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=1), [4]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=0)}
#   5: @↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_192:CNode_244{[0]: ValueNode<FuncGraph> ↓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_243}
#   6: @↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_192:CNode_249{[0]: ValueNode<Primitive> Return, [1]: CNode_244}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_↓mindspore_nn_layer_basic_Dense_construct_204 : 0x140ef3818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_↓mindspore_nn_layer_basic_Dense_construct_204 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_142](%para62_) {
  %1(CNode_251) = call @L_✓↓mindspore_nn_layer_basic_Dense_construct_250()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:628/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:628/        if self.has_bias:/
}
# Order:
#   1: @L_↓mindspore_nn_layer_basic_Dense_construct_204:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MatMul, [1]: param_фx, [2]: param_L_fc3.weight}
#   2: @L_↓mindspore_nn_layer_basic_Dense_construct_204:CNode_251{[0]: ValueNode<FuncGraph> L_✓↓mindspore_nn_layer_basic_Dense_construct_250}
#   3: @L_↓mindspore_nn_layer_basic_Dense_construct_204:CNode_252{[0]: ValueNode<Primitive> Return, [1]: CNode_251}


subgraph attr:
training : 1
subgraph instance: L_✓mindspore_nn_layer_basic_Dense_construct_201 : 0x140ef3218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_✓mindspore_nn_layer_basic_Dense_construct_201 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_142]() {
  %1(CNode_253) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  %2(x_shape) = $(L_mindspore_nn_layer_basic_Dense_construct_142):S_Prim_Shape(%para52_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:623/        x_shape = self.shape_op(x)/
  %3(CNode_254) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  %4(CNode_255) = S_Prim_getitem(%2, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  %5(CNode_256) = S_Prim_MakeTuple(%1, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  %6(x) = S_Prim_Reshape[output_names: ["output"], input_names: ["tensor", "shape"]](%para52_x, %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
}
# Order:
#   1: @L_✓mindspore_nn_layer_basic_Dense_construct_201:CNode_253{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @L_✓mindspore_nn_layer_basic_Dense_construct_201:CNode_254{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   3: @L_✓mindspore_nn_layer_basic_Dense_construct_201:CNode_255{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: CNode_254}
#   4: @L_✓mindspore_nn_layer_basic_Dense_construct_201:CNode_256{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_253, [2]: CNode_255}
#   5: @L_✓mindspore_nn_layer_basic_Dense_construct_201:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_x, [2]: CNode_256}
#   6: @L_✓mindspore_nn_layer_basic_Dense_construct_201:CNode_257{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: L_✗mindspore_nn_layer_basic_Dense_construct_202 : 0x140ef2c18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_✗mindspore_nn_layer_basic_Dense_construct_202 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_142]() {
  Return(%para52_x)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_✗mindspore_nn_layer_basic_Dense_construct_202:CNode_258{[0]: ValueNode<Primitive> Return, [1]: param_x}


subgraph attr:
training : 1
subgraph instance: ✓check_axis_valid_216 : 0x140ee7a18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @✓check_axis_valid_216() {
  %1(CNode_259) = raise[side_effect_io: Bool(1)]("ValueError", "'start_dim' or 'end_dim' out of range.", "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:459/            raise ValueError("'start_dim' or 'end_dim' out of range.")/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:459/            raise ValueError("'start_dim' or 'end_dim' out of range.")/
}
# Order:
#   1: @✓check_axis_valid_216:CNode_259{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> ValueError, [2]: ValueNode<StringImm> 'start_dim' or 'end_dim' out of range., [3]: ValueNode<StringImm> None}
#   2: @✓check_axis_valid_216:CNode_260{[0]: ValueNode<Primitive> Return, [1]: CNode_259}


subgraph attr:
training : 1
subgraph instance: ✗check_axis_valid_217 : 0x140eeda18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @✗check_axis_valid_217() {
  %1(CNode_262) = call @↓check_axis_valid_261()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @✗check_axis_valid_217:CNode_262{[0]: ValueNode<FuncGraph> ↓check_axis_valid_261}
#   2: @✗check_axis_valid_217:CNode_263{[0]: ValueNode<Primitive> Return, [1]: CNode_262}


subgraph attr:
training : 1
subgraph instance: ↰check_axis_valid_211 : 0x140eed418
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @↰check_axis_valid_211 parent: [subgraph @check_axis_valid_157]() {
  %1(CNode_207) = $(check_axis_valid_157):S_Prim_negative(%para56_ndim)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  %2(CNode_208) = $(check_axis_valid_157):S_Prim_less(%para55_axis, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @↰check_axis_valid_211:CNode_264{[0]: ValueNode<Primitive> Return, [1]: CNode_208}


subgraph attr:
training : 1
subgraph instance: ↱check_axis_valid_212 : 0x140eece18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @↱check_axis_valid_212 parent: [subgraph @check_axis_valid_157]() {
  %1(CNode_265) = S_Prim_greater_equal(%para55_axis, %para56_ndim)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @↱check_axis_valid_212:CNode_265{[0]: ValueNode<DoSignaturePrimitive> S_Prim_greater_equal, [1]: param_axis, [2]: param_ndim}
#   2: @↱check_axis_valid_212:CNode_266{[0]: ValueNode<Primitive> Return, [1]: CNode_265}


subgraph attr:
subgraph instance: ✓flatten_228 : 0x140eef218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✓flatten_228() {
  %1(CNode_267) = JoinedStr("For 'flatten', argument 'input' must be Tensor.")
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1732/        raise TypeError(f"For 'flatten', argument 'input' must be Tensor.")/
  %2(CNode_268) = raise[side_effect_io: Bool(1)]("TypeError", %1, "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1732/        raise TypeError(f"For 'flatten', argument 'input' must be Tensor.")/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1732/        raise TypeError(f"For 'flatten', argument 'input' must be Tensor.")/
}
# Order:
#   1: @✓flatten_228:CNode_267{[0]: ValueNode<Primitive> JoinedStr, [1]: ValueNode<StringImm> For 'flatten', argument 'input' must be Tensor.}
#   2: @✓flatten_228:CNode_268{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> TypeError, [2]: CNode_267, [3]: ValueNode<StringImm> None}
#   3: @✓flatten_228:CNode_269{[0]: ValueNode<Primitive> Return, [1]: CNode_268}


subgraph attr:
subgraph instance: ✗flatten_229 : 0x140ef0018
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✗flatten_229 parent: [subgraph @flatten_167]() {
  %1(CNode_271) = call @↓flatten_270()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1731/    if not isinstance(input, Tensor):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1731/    if not isinstance(input, Tensor):/
}
# Order:
#   1: @✗flatten_229:CNode_271{[0]: ValueNode<FuncGraph> ↓flatten_270}
#   2: @✗flatten_229:CNode_272{[0]: ValueNode<Primitive> Return, [1]: CNode_271}


subgraph attr:
after_block : 1
training : 1
subgraph instance: ↓mindspore_nn_layer_pooling_MaxPool2d_construct_233 : 0x140ef9218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @↓mindspore_nn_layer_pooling_MaxPool2d_construct_233(%para63_, %para64_) {
  %1(CNode_274) = call @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_273()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:573/        if self.use_pad:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:573/        if self.use_pad:/
}
# Order:
#   1: @↓mindspore_nn_layer_pooling_MaxPool2d_construct_233:CNode_274{[0]: ValueNode<FuncGraph> ✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_273}
#   2: @↓mindspore_nn_layer_pooling_MaxPool2d_construct_233:CNode_275{[0]: ValueNode<Primitive> Return, [1]: CNode_274}


subgraph attr:
training : 1
subgraph instance: ↓mindspore_nn_layer_conv_Conv2d_construct_238 : 0x140ef8018
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:360/    def construct(self, x):/
subgraph @↓mindspore_nn_layer_conv_Conv2d_construct_238 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_109]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_109):S_Prim_Conv2D[kernel_size: (I64(5), I64(5)), stride: (I64(1), I64(1), I64(1), I64(1)), mode: I64(1), out_channel: I64(16), group: I64(1), input_names: ["x", "w"], pad: (I64(0), I64(0), I64(0), I64(0)), dilation: (I64(1), I64(1), I64(1), I64(1)), pad_mode: I64(2), output_names: ["output"], format: "NCHW", groups: I64(1)](%para50_x, %para4_conv2.weight)
      : (<null>, <Ref[Tensor[Float32]], (16, 64, 5, 5), ref_key=:conv2.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv2-Conv2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv2-Conv2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:364/        return output/
}
# Order:
#   1: @↓mindspore_nn_layer_conv_Conv2d_construct_238:CNode_60{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: ↓mindspore_nn_layer_conv_Conv2d_construct_239 : 0x140ef4e18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:360/    def construct(self, x):/
subgraph @↓mindspore_nn_layer_conv_Conv2d_construct_239 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_106]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_106):S_Prim_Conv2D[kernel_size: (I64(5), I64(5)), mode: I64(1), out_channel: I64(32), input_names: ["x", "w"], pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(2), format: "NCHW", pad_list: (I64(0), I64(0), I64(0), I64(0)), groups: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), group: I64(1), dilation: (I64(1), I64(1), I64(1), I64(1)), output_names: ["output"]](%para51_x, %para3_conv1.weight)
      : (<null>, <Ref[Tensor[Float32]], (32, 1, 5, 5), ref_key=:conv1.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv1-Conv2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv1-Conv2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:364/        return output/
}
# Order:
#   1: @↓mindspore_nn_layer_conv_Conv2d_construct_239:CNode_276{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
subgraph instance: shape_246 : 0x140f32418
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1484/def shape(input_x):/
subgraph @shape_246(%para65_input_x) {
  %1(CNode_277) = S_Prim_Shape(%para65_input_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1510/    return shape_(input_x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1510/    return shape_(input_x)/
}
# Order:
#   1: @shape_246:CNode_277{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_input_x}
#   2: @shape_246:CNode_278{[0]: ValueNode<Primitive> Return, [1]: CNode_277}


subgraph attr:
training : 1
subgraph instance: ↓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_243 : 0x140f32a18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:777/    def construct(self, logits, labels):/
subgraph @↓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_243 parent: [subgraph @↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_192]() {
  %1(CNode_245) = $(↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_192):call @shape_246(%para41_logits)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:784/            labels = self.one_hot(labels, F.shape(logits)[-1], self.on_value, self.off_value)/
  %2(CNode_247) = $(↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_192):S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:784/            labels = self.one_hot(labels, F.shape(logits)[-1], self.on_value, self.off_value)/
  %3(CNode_248) = $(↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_192):S_Prim_getitem(%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:784/            labels = self.one_hot(labels, F.shape(logits)[-1], self.on_value, self.off_value)/
  %4(labels) = $(↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_192):S_Prim_OneHot[output_names: ["output"], input_names: ["indices", "depth", "on_value", "off_value"], axis: I64(-1)](%para42_labels, %3, Tensor(shape=[], dtype=Float32, value=1), Tensor(shape=[], dtype=Float32, value=0))
      : (<null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:784/            labels = self.one_hot(labels, F.shape(logits)[-1], self.on_value, self.off_value)/
  %5(CNode_279) = S_Prim_SoftmaxCrossEntropyWithLogits(%para41_logits, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:785/        x = self.softmax_cross_entropy(logits, labels)[0]/
  %6(x) = S_Prim_getitem(%5, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:785/        x = self.softmax_cross_entropy(logits, labels)[0]/
  %7(CNode_281) = call @get_loss_280(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:786/        return self.get_loss(x)/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:786/        return self.get_loss(x)/
}
# Order:
#   1: @↓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_243:CNode_279{[0]: ValueNode<DoSignaturePrimitive> S_Prim_SoftmaxCrossEntropyWithLogits, [1]: param_logits, [2]: labels}
#   2: @↓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_243:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_279, [2]: ValueNode<Int64Imm> 0}
#   3: @↓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_243:CNode_281{[0]: ValueNode<FuncGraph> get_loss_280, [1]: x}
#   4: @↓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_243:CNode_282{[0]: ValueNode<Primitive> Return, [1]: CNode_281}


subgraph attr:
training : 1
subgraph instance: L_✓↓mindspore_nn_layer_basic_Dense_construct_250 : 0x140ef3e18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_250 parent: [subgraph @L_↓mindspore_nn_layer_basic_Dense_construct_204]() {
  %1(CNode_284) = call @L_2↓mindspore_nn_layer_basic_Dense_construct_283()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:629/            x = self.bias_add(x, self.bias)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:629/            x = self.bias_add(x, self.bias)/
}
# Order:
#   1: @L_✓↓mindspore_nn_layer_basic_Dense_construct_250:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BiasAdd, [1]: x, [2]: param_L_fc3.bias}
#   2: @L_✓↓mindspore_nn_layer_basic_Dense_construct_250:CNode_284{[0]: ValueNode<FuncGraph> L_2↓mindspore_nn_layer_basic_Dense_construct_283}
#   3: @L_✓↓mindspore_nn_layer_basic_Dense_construct_250:CNode_285{[0]: ValueNode<Primitive> Return, [1]: CNode_284}


subgraph attr:
after_block : 1
training : 1
subgraph instance: ↓check_axis_valid_261 : 0x140eee018
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @↓check_axis_valid_261() {
  Return(None)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:457/    def check_axis_valid(self, axis, ndim):/
}
# Order:
#   1: @↓check_axis_valid_261:CNode_286{[0]: ValueNode<Primitive> Return, [1]: ValueNode<None> None}


subgraph attr:
after_block : 1
subgraph instance: ↓flatten_270 : 0x140ef6418
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↓flatten_270 parent: [subgraph @flatten_167]() {
  %1(CNode_287) = S_Prim_isinstance(%para60_start_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %2(CNode_288) = S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %3(CNode_289) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %4(CNode_290) = Switch(%3, @↰↓flatten_291, @↱↓flatten_292)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %5(CNode_293) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %6(CNode_294) = Cond(%5, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %7(CNode_295) = Switch(%6, @✓↓flatten_296, @✗↓flatten_297)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %8(CNode_298) = %7()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @↓flatten_270:CNode_287{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_start_dim, [2]: ValueNode<ClassType> class 'int'}
#   2: @↓flatten_270:CNode_288{[0]: ValueNode<DoSignaturePrimitive> S_Prim_logical_not, [1]: CNode_287}
#   3: @↓flatten_270:CNode_289{[0]: ValueNode<Primitive> Cond, [1]: CNode_288, [2]: ValueNode<BoolImm> false}
#   4: @↓flatten_270:CNode_290{[0]: ValueNode<Primitive> Switch, [1]: CNode_289, [2]: ValueNode<FuncGraph> ↰↓flatten_291, [3]: ValueNode<FuncGraph> ↱↓flatten_292}
#   5: @↓flatten_270:CNode_293{[0]: CNode_290}
#   6: @↓flatten_270:CNode_294{[0]: ValueNode<Primitive> Cond, [1]: CNode_293, [2]: ValueNode<BoolImm> false}
#   7: @↓flatten_270:CNode_295{[0]: ValueNode<Primitive> Switch, [1]: CNode_294, [2]: ValueNode<FuncGraph> ✓↓flatten_296, [3]: ValueNode<FuncGraph> ✗↓flatten_297}
#   8: @↓flatten_270:CNode_298{[0]: CNode_295}
#   9: @↓flatten_270:CNode_299{[0]: ValueNode<Primitive> Return, [1]: CNode_298}


subgraph attr:
training : 1
subgraph instance: ✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_273 : 0x140ef9818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_273 parent: [subgraph @↓mindspore_nn_layer_pooling_MaxPool2d_construct_233]() {
  %1(CNode_301) = call @2↓mindspore_nn_layer_pooling_MaxPool2d_construct_300()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:581/            out = self.max_pool(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:581/            out = self.max_pool(x)/
}
# Order:
#   1: @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_273:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MaxPool, [1]: param_фx}
#   2: @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_273:CNode_301{[0]: ValueNode<FuncGraph> 2↓mindspore_nn_layer_pooling_MaxPool2d_construct_300}
#   3: @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_273:CNode_302{[0]: ValueNode<Primitive> Return, [1]: CNode_301}


subgraph attr:
training : 1
subgraph instance: get_loss_280 : 0x140f33018
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @get_loss_280(%para66_x, %para67_weights) {
  %1(CNode_304) = call @✓get_loss_303()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:143/        if self.reduce and self.average:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:143/        if self.reduce and self.average:/
}
# Order:
#   1: @get_loss_280:input_dtype{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> dtype}
#   2: @get_loss_280:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Cast, [1]: param_x, [2]: ValueNode<Float> Float32}
#   3: @get_loss_280:weights{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Cast, [1]: param_weights, [2]: ValueNode<Float> Float32}
#   4: @get_loss_280:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Mul, [1]: weights, [2]: x}
#   5: @get_loss_280:CNode_304{[0]: ValueNode<FuncGraph> ✓get_loss_303}
#   6: @get_loss_280:CNode_305{[0]: ValueNode<Primitive> Return, [1]: CNode_304}


subgraph attr:
training : 1
subgraph instance: L_2↓mindspore_nn_layer_basic_Dense_construct_283 : 0x140f2d018
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_2↓mindspore_nn_layer_basic_Dense_construct_283 parent: [subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_250]() {
  %1(CNode_307) = call @L_✗2↓mindspore_nn_layer_basic_Dense_construct_306()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:630/        if self.activation_flag:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:630/        if self.activation_flag:/
}
# Order:
#   1: @L_2↓mindspore_nn_layer_basic_Dense_construct_283:CNode_307{[0]: ValueNode<FuncGraph> L_✗2↓mindspore_nn_layer_basic_Dense_construct_306}
#   2: @L_2↓mindspore_nn_layer_basic_Dense_construct_283:CNode_308{[0]: ValueNode<Primitive> Return, [1]: CNode_307}


subgraph attr:
subgraph instance: ✓↓flatten_296 : 0x140eeec18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✓↓flatten_296() {
  %1(CNode_309) = JoinedStr("For 'flatten', both 'start_dim' and 'end_dim' must be int.")
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1735/        raise TypeError(f"For 'flatten', both 'start_dim' and 'end_dim' must be int.")/
  %2(CNode_310) = raise[side_effect_io: Bool(1)]("TypeError", %1, "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1735/        raise TypeError(f"For 'flatten', both 'start_dim' and 'end_dim' must be int.")/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1735/        raise TypeError(f"For 'flatten', both 'start_dim' and 'end_dim' must be int.")/
}
# Order:
#   1: @✓↓flatten_296:CNode_309{[0]: ValueNode<Primitive> JoinedStr, [1]: ValueNode<StringImm> For 'flatten', both 'start_dim' and 'end_dim' must be int.}
#   2: @✓↓flatten_296:CNode_310{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> TypeError, [2]: CNode_309, [3]: ValueNode<StringImm> None}
#   3: @✓↓flatten_296:CNode_311{[0]: ValueNode<Primitive> Return, [1]: CNode_310}


subgraph attr:
subgraph instance: ✗↓flatten_297 : 0x140f0fe18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✗↓flatten_297 parent: [subgraph @flatten_167]() {
  %1(CNode_313) = call @2↓flatten_312()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @✗↓flatten_297:CNode_313{[0]: ValueNode<FuncGraph> 2↓flatten_312}
#   2: @✗↓flatten_297:CNode_314{[0]: ValueNode<Primitive> Return, [1]: CNode_313}


subgraph attr:
subgraph instance: ↰↓flatten_291 : 0x140f0f818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↰↓flatten_291 parent: [subgraph @↓flatten_270]() {
  %1(CNode_287) = $(↓flatten_270):S_Prim_isinstance(%para60_start_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %2(CNode_288) = $(↓flatten_270):S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @↰↓flatten_291:CNode_315{[0]: ValueNode<Primitive> Return, [1]: CNode_288}


subgraph attr:
subgraph instance: ↱↓flatten_292 : 0x140f0da18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↱↓flatten_292 parent: [subgraph @flatten_167]() {
  %1(CNode_316) = S_Prim_isinstance(%para61_end_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %2(CNode_317) = S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %3(CNode_318) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %4(CNode_319) = Switch(%3, @↰↱↓flatten_320, @2↱↓flatten_321)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %5(CNode_322) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @↱↓flatten_292:CNode_316{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_end_dim, [2]: ValueNode<ClassType> class 'int'}
#   2: @↱↓flatten_292:CNode_317{[0]: ValueNode<DoSignaturePrimitive> S_Prim_logical_not, [1]: CNode_316}
#   3: @↱↓flatten_292:CNode_318{[0]: ValueNode<Primitive> Cond, [1]: CNode_317, [2]: ValueNode<BoolImm> false}
#   4: @↱↓flatten_292:CNode_319{[0]: ValueNode<Primitive> Switch, [1]: CNode_318, [2]: ValueNode<FuncGraph> ↰↱↓flatten_320, [3]: ValueNode<FuncGraph> 2↱↓flatten_321}
#   5: @↱↓flatten_292:CNode_322{[0]: CNode_319}
#   6: @↱↓flatten_292:CNode_323{[0]: ValueNode<Primitive> Return, [1]: CNode_322}


subgraph attr:
training : 1
subgraph instance: 2↓mindspore_nn_layer_pooling_MaxPool2d_construct_300 : 0x140ef9e18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @2↓mindspore_nn_layer_pooling_MaxPool2d_construct_300 parent: [subgraph @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_273]() {
  %1(CNode_324) = Cond(%para64_фexpand_batch, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:582/        if expand_batch:/
  %2(CNode_325) = Switch(%1, @✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_326, @✗2↓mindspore_nn_layer_pooling_MaxPool2d_construct_327)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:582/        if expand_batch:/
  %3(CNode_328) = %2()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:582/        if expand_batch:/
  %4(CNode_330) = call @3↓mindspore_nn_layer_pooling_MaxPool2d_construct_329(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:120/        x = self.max_pool2d(self.relu(self.conv2(x)))/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:582/        if expand_batch:/
}
# Order:
#   1: @2↓mindspore_nn_layer_pooling_MaxPool2d_construct_300:CNode_324{[0]: ValueNode<Primitive> Cond, [1]: param_фexpand_batch, [2]: ValueNode<BoolImm> false}
#   2: @2↓mindspore_nn_layer_pooling_MaxPool2d_construct_300:CNode_325{[0]: ValueNode<Primitive> Switch, [1]: CNode_324, [2]: ValueNode<FuncGraph> ✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_326, [3]: ValueNode<FuncGraph> ✗2↓mindspore_nn_layer_pooling_MaxPool2d_construct_327}
#   3: @2↓mindspore_nn_layer_pooling_MaxPool2d_construct_300:CNode_328{[0]: CNode_325}
#   4: @2↓mindspore_nn_layer_pooling_MaxPool2d_construct_300:CNode_330{[0]: ValueNode<FuncGraph> 3↓mindspore_nn_layer_pooling_MaxPool2d_construct_329, [1]: CNode_328}
#   5: @2↓mindspore_nn_layer_pooling_MaxPool2d_construct_300:CNode_331{[0]: ValueNode<Primitive> Return, [1]: CNode_330}


subgraph attr:
training : 1
subgraph instance: ✓get_loss_303 : 0x140eec618
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @✓get_loss_303 parent: [subgraph @get_loss_280]() {
  %1(CNode_333) = call @↓get_loss_332()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:144/            x = self.reduce_mean(x, self.get_axis(x))/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:144/            x = self.reduce_mean(x, self.get_axis(x))/
}
# Order:
#   1: @✓get_loss_303:CNode_334{[0]: ValueNode<FuncGraph> get_axis_335, [1]: x}
#   2: @✓get_loss_303:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReduceMean, [1]: x, [2]: CNode_334}
#   3: @✓get_loss_303:CNode_333{[0]: ValueNode<FuncGraph> ↓get_loss_332}
#   4: @✓get_loss_303:CNode_336{[0]: ValueNode<Primitive> Return, [1]: CNode_333}


subgraph attr:
training : 1
subgraph instance: L_✗2↓mindspore_nn_layer_basic_Dense_construct_306 : 0x140f2d618
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_✗2↓mindspore_nn_layer_basic_Dense_construct_306 parent: [subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_250]() {
  %1(CNode_338) = call @L_3↓mindspore_nn_layer_basic_Dense_construct_337()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:630/        if self.activation_flag:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:630/        if self.activation_flag:/
}
# Order:
#   1: @L_✗2↓mindspore_nn_layer_basic_Dense_construct_306:CNode_338{[0]: ValueNode<FuncGraph> L_3↓mindspore_nn_layer_basic_Dense_construct_337}
#   2: @L_✗2↓mindspore_nn_layer_basic_Dense_construct_306:CNode_339{[0]: ValueNode<Primitive> Return, [1]: CNode_338}


subgraph attr:
after_block : 1
subgraph instance: 2↓flatten_312 : 0x140f10c18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @2↓flatten_312 parent: [subgraph @flatten_167]() {
  %1(CNode_340) = S_Prim_check_flatten_order[constexpr_prim: Bool(1)](%para59_order)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1736/    check_flatten_order_const(order)/
  %2(CNode_341) = StopGradient(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
  %3(CNode_342) = S_Prim_equal(%para59_order, "F")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1737/    if order == 'F':/
  %4(CNode_343) = Cond(%3, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1737/    if order == 'F':/
  %5(CNode_344) = Switch(%4, @✓2↓flatten_345, @✗2↓flatten_346)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1737/    if order == 'F':/
  %6(CNode_347) = %5()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1737/    if order == 'F':/
  %7(CNode_348) = Depend[side_effect_propagate: I64(1)](%6, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1737/    if order == 'F':/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1737/    if order == 'F':/
}
# Order:
#   1: @2↓flatten_312:CNode_340{[0]: ValueNode<DoSignaturePrimitive> S_Prim_check_flatten_order, [1]: param_order}
#   2: @2↓flatten_312:CNode_342{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: param_order, [2]: ValueNode<StringImm> F}
#   3: @2↓flatten_312:CNode_343{[0]: ValueNode<Primitive> Cond, [1]: CNode_342, [2]: ValueNode<BoolImm> false}
#   4: @2↓flatten_312:CNode_344{[0]: ValueNode<Primitive> Switch, [1]: CNode_343, [2]: ValueNode<FuncGraph> ✓2↓flatten_345, [3]: ValueNode<FuncGraph> ✗2↓flatten_346}
#   5: @2↓flatten_312:CNode_347{[0]: CNode_344}
#   6: @2↓flatten_312:CNode_349{[0]: ValueNode<Primitive> Return, [1]: CNode_348}


subgraph attr:
subgraph instance: ↰↱↓flatten_320 : 0x140f0f218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↰↱↓flatten_320 parent: [subgraph @↱↓flatten_292]() {
  %1(CNode_316) = $(↱↓flatten_292):S_Prim_isinstance(%para61_end_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %2(CNode_317) = $(↱↓flatten_292):S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
}
# Order:
#   1: @↰↱↓flatten_320:CNode_350{[0]: ValueNode<Primitive> Return, [1]: CNode_317}


subgraph attr:
subgraph instance: 2↱↓flatten_321 : 0x140f0e018
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @2↱↓flatten_321 parent: [subgraph @flatten_167]() {
  %1(CNode_351) = S_Prim_isinstance(%para60_start_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %2(CNode_352) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %3(CNode_353) = Switch(%2, @↰2↱↓flatten_354, @3↱↓flatten_355)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %4(CNode_356) = %3()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @2↱↓flatten_321:CNode_351{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_start_dim, [2]: ValueNode<ClassType> class 'bool'}
#   2: @2↱↓flatten_321:CNode_352{[0]: ValueNode<Primitive> Cond, [1]: CNode_351, [2]: ValueNode<BoolImm> false}
#   3: @2↱↓flatten_321:CNode_353{[0]: ValueNode<Primitive> Switch, [1]: CNode_352, [2]: ValueNode<FuncGraph> ↰2↱↓flatten_354, [3]: ValueNode<FuncGraph> 3↱↓flatten_355}
#   4: @2↱↓flatten_321:CNode_356{[0]: CNode_353}
#   5: @2↱↓flatten_321:CNode_357{[0]: ValueNode<Primitive> Return, [1]: CNode_356}


subgraph attr:
after_block : 1
training : 1
subgraph instance: 3↓mindspore_nn_layer_pooling_MaxPool2d_construct_329 : 0x140edc018
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @3↓mindspore_nn_layer_pooling_MaxPool2d_construct_329(%para68_) {
  %1(CNode_359) = call @✗3↓mindspore_nn_layer_pooling_MaxPool2d_construct_358()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:587/        if self.use_pad and not self.return_indices:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:587/        if self.use_pad and not self.return_indices:/
}
# Order:
#   1: @3↓mindspore_nn_layer_pooling_MaxPool2d_construct_329:CNode_359{[0]: ValueNode<FuncGraph> ✗3↓mindspore_nn_layer_pooling_MaxPool2d_construct_358}
#   2: @3↓mindspore_nn_layer_pooling_MaxPool2d_construct_329:CNode_360{[0]: ValueNode<Primitive> Return, [1]: CNode_359}


subgraph attr:
training : 1
subgraph instance: ✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_326 : 0x140efb818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_326 parent: [subgraph @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_273]() {
  %1(out) = $(✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_273):S_Prim_MaxPool[pad_mode: I64(2), output_names: ["output"], kernel_size: (I64(1), I64(1), I64(2), I64(2)), format: "NCHW", strides: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"]](%para63_фx)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_361) = S_Prim_isinstance(%1, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:583/            if isinstance(out, tuple):/
  %3(CNode_362) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:583/            if isinstance(out, tuple):/
  %4(CNode_363) = Switch(%3, @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_364, @✗✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_365)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:583/            if isinstance(out, tuple):/
  %5(CNode_366) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:583/            if isinstance(out, tuple):/
  %6(CNode_368) = call @↓✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_367(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:120/        x = self.max_pool2d(self.relu(self.conv2(x)))/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:583/            if isinstance(out, tuple):/
}
# Order:
#   1: @✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_326:CNode_361{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: out, [2]: ValueNode<ClassType> class 'tuple'}
#   2: @✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_326:CNode_362{[0]: ValueNode<Primitive> Cond, [1]: CNode_361, [2]: ValueNode<BoolImm> false}
#   3: @✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_326:CNode_363{[0]: ValueNode<Primitive> Switch, [1]: CNode_362, [2]: ValueNode<FuncGraph> 2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_364, [3]: ValueNode<FuncGraph> ✗✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_365}
#   4: @✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_326:CNode_366{[0]: CNode_363}
#   5: @✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_326:CNode_368{[0]: ValueNode<FuncGraph> ↓✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_367, [1]: CNode_366}
#   6: @✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_326:CNode_369{[0]: ValueNode<Primitive> Return, [1]: CNode_368}


subgraph attr:
training : 1
subgraph instance: ✗2↓mindspore_nn_layer_pooling_MaxPool2d_construct_327 : 0x140efb218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @✗2↓mindspore_nn_layer_pooling_MaxPool2d_construct_327 parent: [subgraph @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_273]() {
  %1(out) = $(✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_273):S_Prim_MaxPool[pad_mode: I64(2), output_names: ["output"], kernel_size: (I64(1), I64(1), I64(2), I64(2)), format: "NCHW", strides: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"]](%para63_фx)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:581/            out = self.max_pool(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:582/        if expand_batch:/
}
# Order:
#   1: @✗2↓mindspore_nn_layer_pooling_MaxPool2d_construct_327:CNode_370{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: get_axis_335 : 0x140f33618
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:113/    def get_axis(self, x):/
subgraph @get_axis_335(%para69_x) {
  %1(shape) = call @shape_246(%para69_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:120/        shape = F.shape(x)/
  %2(length) = S_Prim_sequence_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:121/        length = F.tuple_len(shape)/
  %3(perm) = S_Prim_make_range(I64(0), %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:122/        perm = F.make_range(0, length)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:123/        return perm/
}
# Order:
#   1: @get_axis_335:shape{[0]: ValueNode<FuncGraph> shape_246, [1]: param_x}
#   2: @get_axis_335:length{[0]: ValueNode<DoSignaturePrimitive> S_Prim_sequence_len, [1]: shape}
#   3: @get_axis_335:perm{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_range, [1]: ValueNode<Int64Imm> 0, [2]: length}
#   4: @get_axis_335:CNode_371{[0]: ValueNode<Primitive> Return, [1]: perm}


subgraph attr:
training : 1
subgraph instance: ↓get_loss_332 : 0x140f33c18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @↓get_loss_332 parent: [subgraph @✓get_loss_303]() {
  %1(CNode_373) = call @✗↓get_loss_372()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:145/        if self.reduce and not self.average:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:145/        if self.reduce and not self.average:/
}
# Order:
#   1: @↓get_loss_332:CNode_373{[0]: ValueNode<FuncGraph> ✗↓get_loss_372}
#   2: @↓get_loss_332:CNode_374{[0]: ValueNode<Primitive> Return, [1]: CNode_373}


subgraph attr:
training : 1
subgraph instance: L_3↓mindspore_nn_layer_basic_Dense_construct_337 : 0x140f2dc18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_3↓mindspore_nn_layer_basic_Dense_construct_337 parent: [subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_250]() {
  %1(x_shape) = $(L_mindspore_nn_layer_basic_Dense_construct_142):S_Prim_Shape(%para52_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:623/        x_shape = self.shape_op(x)/
  %2(CNode_375) = S_Prim_inner_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
  %3(CNode_376) = S_Prim_not_equal(%2, I64(2))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
  %4(CNode_377) = Cond(%3, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
  %5(CNode_378) = Switch(%4, @L_✓3↓mindspore_nn_layer_basic_Dense_construct_379, @L_✗3↓mindspore_nn_layer_basic_Dense_construct_380)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
  %6(CNode_381) = %5()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
  %7(CNode_383) = call @L_4↓mindspore_nn_layer_basic_Dense_construct_382(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenetImproved.py:124/        x = self.fc3(x)/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_3↓mindspore_nn_layer_basic_Dense_construct_337:CNode_375{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: x_shape}
#   2: @L_3↓mindspore_nn_layer_basic_Dense_construct_337:CNode_376{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: CNode_375, [2]: ValueNode<Int64Imm> 2}
#   3: @L_3↓mindspore_nn_layer_basic_Dense_construct_337:CNode_377{[0]: ValueNode<Primitive> Cond, [1]: CNode_376, [2]: ValueNode<BoolImm> false}
#   4: @L_3↓mindspore_nn_layer_basic_Dense_construct_337:CNode_378{[0]: ValueNode<Primitive> Switch, [1]: CNode_377, [2]: ValueNode<FuncGraph> L_✓3↓mindspore_nn_layer_basic_Dense_construct_379, [3]: ValueNode<FuncGraph> L_✗3↓mindspore_nn_layer_basic_Dense_construct_380}
#   5: @L_3↓mindspore_nn_layer_basic_Dense_construct_337:CNode_381{[0]: CNode_378}
#   6: @L_3↓mindspore_nn_layer_basic_Dense_construct_337:CNode_383{[0]: ValueNode<FuncGraph> L_4↓mindspore_nn_layer_basic_Dense_construct_382, [1]: CNode_381}
#   7: @L_3↓mindspore_nn_layer_basic_Dense_construct_337:CNode_384{[0]: ValueNode<Primitive> Return, [1]: CNode_383}


subgraph attr:
subgraph instance: ✓2↓flatten_345 : 0x140f1be18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✓2↓flatten_345 parent: [subgraph @flatten_167]() {
  %1(x_rank) = S_Prim_Rank(%para58_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1738/        x_rank = rank_(input)/
  %2(CNode_385) = S_Prim_MakeTuple(I64(0), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1740/        if x_rank in (0, 1):/
  %3(CNode_386) = S_Prim_in(%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1740/        if x_rank in (0, 1):/
  %4(CNode_387) = Cond(%3, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1740/        if x_rank in (0, 1):/
  %5(CNode_388) = Switch(%4, @2✓2↓flatten_389, @✗✓2↓flatten_390)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1740/        if x_rank in (0, 1):/
  %6(CNode_391) = %5()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1740/        if x_rank in (0, 1):/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1740/        if x_rank in (0, 1):/
}
# Order:
#   1: @✓2↓flatten_345:x_rank{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Rank, [1]: param_input}
#   2: @✓2↓flatten_345:CNode_385{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<Int64Imm> 0, [2]: ValueNode<Int64Imm> 1}
#   3: @✓2↓flatten_345:CNode_386{[0]: ValueNode<DoSignaturePrimitive> S_Prim_in, [1]: x_rank, [2]: CNode_385}
#   4: @✓2↓flatten_345:CNode_387{[0]: ValueNode<Primitive> Cond, [1]: CNode_386, [2]: ValueNode<BoolImm> false}
#   5: @✓2↓flatten_345:CNode_388{[0]: ValueNode<Primitive> Switch, [1]: CNode_387, [2]: ValueNode<FuncGraph> 2✓2↓flatten_389, [3]: ValueNode<FuncGraph> ✗✓2↓flatten_390}
#   6: @✓2↓flatten_345:CNode_391{[0]: CNode_388}
#   7: @✓2↓flatten_345:CNode_392{[0]: ValueNode<Primitive> Return, [1]: CNode_391}


subgraph attr:
subgraph instance: ✗2↓flatten_346 : 0x140f11a18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✗2↓flatten_346 parent: [subgraph @flatten_167]() {
  %1(CNode_394) = call @3↓flatten_393(%para58_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1737/    if order == 'F':/
}
# Order:
#   1: @✗2↓flatten_346:CNode_395{[0]: ValueNode<Primitive> Return, [1]: CNode_394}
#   2: @✗2↓flatten_346:CNode_394{[0]: ValueNode<FuncGraph> 3↓flatten_393, [1]: param_input}


subgraph attr:
subgraph instance: ↰2↱↓flatten_354 : 0x140f0ec18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↰2↱↓flatten_354 parent: [subgraph @2↱↓flatten_321]() {
  %1(CNode_351) = $(2↱↓flatten_321):S_Prim_isinstance(%para60_start_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
}
# Order:
#   1: @↰2↱↓flatten_354:CNode_396{[0]: ValueNode<Primitive> Return, [1]: CNode_351}


subgraph attr:
subgraph instance: 3↱↓flatten_355 : 0x140f0e618
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @3↱↓flatten_355 parent: [subgraph @flatten_167]() {
  %1(CNode_397) = S_Prim_isinstance(%para61_end_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @3↱↓flatten_355:CNode_397{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_end_dim, [2]: ValueNode<ClassType> class 'bool'}
#   2: @3↱↓flatten_355:CNode_398{[0]: ValueNode<Primitive> Return, [1]: CNode_397}


subgraph attr:
training : 1
subgraph instance: ✗3↓mindspore_nn_layer_pooling_MaxPool2d_construct_358 : 0x140f03418
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @✗3↓mindspore_nn_layer_pooling_MaxPool2d_construct_358 parent: [subgraph @3↓mindspore_nn_layer_pooling_MaxPool2d_construct_329]() {
  %1(CNode_400) = call @4↓mindspore_nn_layer_pooling_MaxPool2d_construct_399()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:587/        if self.use_pad and not self.return_indices:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:587/        if self.use_pad and not self.return_indices:/
}
# Order:
#   1: @✗3↓mindspore_nn_layer_pooling_MaxPool2d_construct_358:CNode_400{[0]: ValueNode<FuncGraph> 4↓mindspore_nn_layer_pooling_MaxPool2d_construct_399}
#   2: @✗3↓mindspore_nn_layer_pooling_MaxPool2d_construct_358:CNode_401{[0]: ValueNode<Primitive> Return, [1]: CNode_400}


subgraph attr:
after_block : 1
training : 1
subgraph instance: ↓✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_367 : 0x140efca18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @↓✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_367(%para70_) {
  Return(%para70_фout)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:583/            if isinstance(out, tuple):/
}
# Order:
#   1: @↓✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_367:CNode_402{[0]: ValueNode<Primitive> Return, [1]: param_фout}


subgraph attr:
training : 1
subgraph instance: 2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_364 : 0x140efc418
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_364 parent: [subgraph @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_273]() {
  %1(out) = $(✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_273):S_Prim_MaxPool[pad_mode: I64(2), output_names: ["output"], kernel_size: (I64(1), I64(1), I64(2), I64(2)), format: "NCHW", strides: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"]](%para63_фx)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_403) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %3(CNode_404) = getattr(%2, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %4(CNode_405) = %3(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %5(CNode_406) = S_Prim_getitem(%1, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %6(CNode_407) = getattr(%5, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %7(CNode_408) = %6(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %8(out) = S_Prim_MakeTuple(%4, %7)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
}
# Order:
#   1: @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_364:CNode_403{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: out, [2]: ValueNode<Int64Imm> 0}
#   2: @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_364:CNode_404{[0]: ValueNode<Primitive> getattr, [1]: CNode_403, [2]: ValueNode<StringImm> squeeze}
#   3: @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_364:CNode_405{[0]: CNode_404, [1]: ValueNode<Int64Imm> 0}
#   4: @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_364:CNode_406{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: out, [2]: ValueNode<Int64Imm> 1}
#   5: @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_364:CNode_407{[0]: ValueNode<Primitive> getattr, [1]: CNode_406, [2]: ValueNode<StringImm> squeeze}
#   6: @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_364:CNode_408{[0]: CNode_407, [1]: ValueNode<Int64Imm> 0}
#   7: @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_364:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_405, [2]: CNode_408}
#   8: @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_364:CNode_409{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: ✗✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_365 : 0x140efbe18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @✗✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_365 parent: [subgraph @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_273]() {
  %1(out) = $(✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_273):S_Prim_MaxPool[pad_mode: I64(2), output_names: ["output"], kernel_size: (I64(1), I64(1), I64(2), I64(2)), format: "NCHW", strides: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"]](%para63_фx)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_410) = getattr(%1, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:586/                out = out.squeeze(0)/
  %3(out) = %2(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:586/                out = out.squeeze(0)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:586/                out = out.squeeze(0)/
}
# Order:
#   1: @✗✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_365:CNode_410{[0]: ValueNode<Primitive> getattr, [1]: out, [2]: ValueNode<StringImm> squeeze}
#   2: @✗✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_365:out{[0]: CNode_410, [1]: ValueNode<Int64Imm> 0}
#   3: @✗✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_365:CNode_411{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: ✗↓get_loss_372 : 0x140f34218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @✗↓get_loss_372 parent: [subgraph @✓get_loss_303]() {
  %1(CNode_413) = call @2↓get_loss_412()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:145/        if self.reduce and not self.average:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:145/        if self.reduce and not self.average:/
}
# Order:
#   1: @✗↓get_loss_372:CNode_413{[0]: ValueNode<FuncGraph> 2↓get_loss_412}
#   2: @✗↓get_loss_372:CNode_414{[0]: ValueNode<Primitive> Return, [1]: CNode_413}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_4↓mindspore_nn_layer_basic_Dense_construct_382 : 0x140f2f418
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_4↓mindspore_nn_layer_basic_Dense_construct_382(%para71_) {
  Return(%para71_фx)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:635/        return x/
}
# Order:
#   1: @L_4↓mindspore_nn_layer_basic_Dense_construct_382:CNode_415{[0]: ValueNode<Primitive> Return, [1]: param_фx}


subgraph attr:
training : 1
subgraph instance: L_✓3↓mindspore_nn_layer_basic_Dense_construct_379 : 0x140f2e818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_✓3↓mindspore_nn_layer_basic_Dense_construct_379 parent: [subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_250]() {
  %1(x) = $(L_↓mindspore_nn_layer_basic_Dense_construct_204):S_Prim_MatMul[output_names: ["output"], transpose_a: Bool(0), input_names: ["x1", "x2"], transpose_x2: Bool(1), transpose_x1: Bool(0), transpose_b: Bool(1)](%para62_фx, %para54_L_fc3.weight)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:627/        x = self.matmul(x, self.weight)/
  %2(x) = $(L_✓↓mindspore_nn_layer_basic_Dense_construct_250):S_Prim_BiasAdd[output_names: ["output"], format: "NCHW", input_names: ["x", "b"]](%1, %para53_L_fc3.bias)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:629/            x = self.bias_add(x, self.bias)/
  %3(x_shape) = $(L_mindspore_nn_layer_basic_Dense_construct_142):S_Prim_Shape(%para52_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:623/        x_shape = self.shape_op(x)/
  %4(CNode_416) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %5(CNode_417) = S_Prim_make_slice(None, %4, None)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %6(CNode_418) = S_Prim_getitem(%3, %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %7(CNode_420) = call @L_shape_419(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %8(CNode_421) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %9(CNode_422) = S_Prim_getitem(%7, %8)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %10(CNode_423) = S_Prim_MakeTuple(%9)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %11(out_shape) = S_Prim_add(%6, %10)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %12(x) = S_Prim_Reshape[output_names: ["output"], input_names: ["tensor", "shape"]](%2, %11)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:634/            x = self.reshape(x, out_shape)/
  Return(%12)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
}
# Order:
#   1: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_379:CNode_416{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_379:CNode_417{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_slice, [1]: ValueNode<None> None, [2]: CNode_416, [3]: ValueNode<None> None}
#   3: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_379:CNode_418{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: CNode_417}
#   4: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_379:CNode_420{[0]: ValueNode<FuncGraph> L_shape_419, [1]: x}
#   5: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_379:CNode_421{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   6: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_379:CNode_422{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_420, [2]: CNode_421}
#   7: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_379:CNode_423{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_422}
#   8: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_379:out_shape{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: CNode_418, [2]: CNode_423}
#   9: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_379:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: x, [2]: out_shape}
#  10: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_379:CNode_424{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: L_✗3↓mindspore_nn_layer_basic_Dense_construct_380 : 0x140f2e218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_✗3↓mindspore_nn_layer_basic_Dense_construct_380 parent: [subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_250]() {
  %1(x) = $(L_↓mindspore_nn_layer_basic_Dense_construct_204):S_Prim_MatMul[output_names: ["output"], transpose_a: Bool(0), input_names: ["x1", "x2"], transpose_x2: Bool(1), transpose_x1: Bool(0), transpose_b: Bool(1)](%para62_фx, %para54_L_fc3.weight)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:627/        x = self.matmul(x, self.weight)/
  %2(x) = $(L_✓↓mindspore_nn_layer_basic_Dense_construct_250):S_Prim_BiasAdd[output_names: ["output"], format: "NCHW", input_names: ["x", "b"]](%1, %para53_L_fc3.bias)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:629/            x = self.bias_add(x, self.bias)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_✗3↓mindspore_nn_layer_basic_Dense_construct_380:CNode_425{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
subgraph instance: 2✓2↓flatten_389 : 0x140eee618
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @2✓2↓flatten_389 parent: [subgraph @flatten_167]() {
  %1(CNode_426) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1741/            return reshape_(input, (-1,))/
  %2(CNode_427) = S_Prim_MakeTuple(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1741/            return reshape_(input, (-1,))/
  %3(CNode_428) = S_Prim_Reshape[output_names: ["output"], input_names: ["tensor", "shape"]](%para58_input, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1741/            return reshape_(input, (-1,))/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1741/            return reshape_(input, (-1,))/
}
# Order:
#   1: @2✓2↓flatten_389:CNode_426{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @2✓2↓flatten_389:CNode_427{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_426}
#   3: @2✓2↓flatten_389:CNode_428{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_input, [2]: CNode_427}
#   4: @2✓2↓flatten_389:CNode_429{[0]: ValueNode<Primitive> Return, [1]: CNode_428}


subgraph attr:
subgraph instance: ✗✓2↓flatten_390 : 0x140f1cc18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✗✓2↓flatten_390 parent: [subgraph @✓2↓flatten_345]() {
  %1(CNode_431) = call @↓✓2↓flatten_430()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1740/        if x_rank in (0, 1):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1740/        if x_rank in (0, 1):/
}
# Order:
#   1: @✗✓2↓flatten_390:CNode_431{[0]: ValueNode<FuncGraph> ↓✓2↓flatten_430}
#   2: @✗✓2↓flatten_390:CNode_432{[0]: ValueNode<Primitive> Return, [1]: CNode_431}


subgraph attr:
after_block : 1
subgraph instance: 3↓flatten_393 : 0x140f12818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @3↓flatten_393 parent: [subgraph @flatten_167](%para72_) {
  %1(CNode_433) = S_Prim_equal(%para60_start_dim, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %2(CNode_434) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %3(CNode_435) = Switch(%2, @↰3↓flatten_436, @↱3↓flatten_437)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %4(CNode_438) = %3()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %5(CNode_439) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %6(CNode_440) = Switch(%5, @✓3↓flatten_441, @✗3↓flatten_442)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %7(CNode_443) = %6()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
}
# Order:
#   1: @3↓flatten_393:x_shape{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_фinput}
#   2: @3↓flatten_393:x_rank{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Rank, [1]: param_фinput}
#   3: @3↓flatten_393:CNode_433{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: param_start_dim, [2]: ValueNode<Int64Imm> 1}
#   4: @3↓flatten_393:CNode_434{[0]: ValueNode<Primitive> Cond, [1]: CNode_433, [2]: ValueNode<BoolImm> false}
#   5: @3↓flatten_393:CNode_435{[0]: ValueNode<Primitive> Switch, [1]: CNode_434, [2]: ValueNode<FuncGraph> ↰3↓flatten_436, [3]: ValueNode<FuncGraph> ↱3↓flatten_437}
#   6: @3↓flatten_393:CNode_438{[0]: CNode_435}
#   7: @3↓flatten_393:CNode_439{[0]: ValueNode<Primitive> Cond, [1]: CNode_438, [2]: ValueNode<BoolImm> false}
#   8: @3↓flatten_393:CNode_440{[0]: ValueNode<Primitive> Switch, [1]: CNode_439, [2]: ValueNode<FuncGraph> ✓3↓flatten_441, [3]: ValueNode<FuncGraph> ✗3↓flatten_442}
#   9: @3↓flatten_393:CNode_443{[0]: CNode_440}
#  10: @3↓flatten_393:CNode_444{[0]: ValueNode<Primitive> Return, [1]: CNode_443}


subgraph attr:
training : 1
subgraph instance: 4↓mindspore_nn_layer_pooling_MaxPool2d_construct_399 : 0x140f03a18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @4↓mindspore_nn_layer_pooling_MaxPool2d_construct_399 parent: [subgraph @3↓mindspore_nn_layer_pooling_MaxPool2d_construct_329]() {
  Return(%para68_фout)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:589/        return out/
}
# Order:
#   1: @4↓mindspore_nn_layer_pooling_MaxPool2d_construct_399:CNode_445{[0]: ValueNode<Primitive> Return, [1]: param_фout}


subgraph attr:
training : 1
subgraph instance: 2↓get_loss_412 : 0x140f34818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @2↓get_loss_412 parent: [subgraph @✓get_loss_303]() {
  %1(weights) = $(get_loss_280):S_Prim_Cast[output_names: ["output"], input_names: ["x", "dst_type"]](%para67_weights, F32)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:141/        weights = self.cast(weights, mstype.float32)/
  %2(x) = $(get_loss_280):S_Prim_Cast[output_names: ["output"], input_names: ["x", "dst_type"]](%para66_x, F32)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:140/        x = self.cast(x, mstype.float32)/
  %3(x) = $(get_loss_280):S_Prim_Mul[output_names: ["output"], input_names: ["x", "y"]](%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:142/        x = self.mul(weights, x)/
  %4(CNode_334) = $(✓get_loss_303):call @get_axis_335(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:144/            x = self.reduce_mean(x, self.get_axis(x))/
  %5(x) = $(✓get_loss_303):S_Prim_ReduceMean[output_names: ["y"], keep_dims: Bool(0), input_names: ["input_x", "axis"]](%3, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:144/            x = self.reduce_mean(x, self.get_axis(x))/
  %6(input_dtype) = $(get_loss_280):getattr(%para66_x, "dtype")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:139/        input_dtype = x.dtype/
  %7(x) = S_Prim_Cast[output_names: ["output"], input_names: ["x", "dst_type"]](%5, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:147/        x = self.cast(x, input_dtype)/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:148/        return x/
}
# Order:
#   1: @2↓get_loss_412:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Cast, [1]: x, [2]: input_dtype}
#   2: @2↓get_loss_412:CNode_446{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
subgraph instance: L_shape_419 : 0x140f2ee18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1484/def shape(input_x):/
subgraph @L_shape_419(%para73_input_x) {
  %1(CNode_277) = S_Prim_Shape(%para73_input_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1510/    return shape_(input_x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1510/    return shape_(input_x)/
}
# Order:
#   1: @L_shape_419:CNode_277{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_input_x}
#   2: @L_shape_419:CNode_278{[0]: ValueNode<Primitive> Return, [1]: CNode_277}


subgraph attr:
after_block : 1
subgraph instance: ↓✓2↓flatten_430 : 0x140efac18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↓✓2↓flatten_430 parent: [subgraph @✓2↓flatten_345]() {
  %1(CNode_448) = call @_get_cache_prim_447(ClassType)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1744/        input = _get_cache_prim(P.Transpose)()(input, new_order)/
  %2(CNode_449) = %1()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1744/        input = _get_cache_prim(P.Transpose)()(input, new_order)/
  %3(x_rank) = $(✓2↓flatten_345):S_Prim_Rank(%para58_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1738/        x_rank = rank_(input)/
  %4(perm) = S_Prim_make_range(I64(0), %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1742/        perm = ops.make_range(0, x_rank)/
  %5(new_order) = S_Prim_tuple_reversed(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1743/        new_order = ops.tuple_reversed(perm)/
  %6(input) = %2(%para58_input, %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1744/        input = _get_cache_prim(P.Transpose)()(input, new_order)/
  %7(CNode_450) = call @3↓flatten_393(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1738/        x_rank = rank_(input)/
}
# Order:
#   1: @↓✓2↓flatten_430:perm{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_range, [1]: ValueNode<Int64Imm> 0, [2]: x_rank}
#   2: @↓✓2↓flatten_430:new_order{[0]: ValueNode<DoSignaturePrimitive> S_Prim_tuple_reversed, [1]: perm}
#   3: @↓✓2↓flatten_430:CNode_448{[0]: ValueNode<FuncGraph> _get_cache_prim_447, [1]: ValueNode<ClassType> class 'mindspore.ops.operations.array_ops.Transpose'}
#   4: @↓✓2↓flatten_430:CNode_449{[0]: CNode_448}
#   5: @↓✓2↓flatten_430:input{[0]: CNode_449, [1]: param_input, [2]: new_order}
#   6: @↓✓2↓flatten_430:CNode_451{[0]: ValueNode<Primitive> Return, [1]: CNode_450}
#   7: @↓✓2↓flatten_430:CNode_450{[0]: ValueNode<FuncGraph> 3↓flatten_393, [1]: input}


subgraph attr:
subgraph instance: ✓3↓flatten_441 : 0x140f02418
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✓3↓flatten_441 parent: [subgraph @3↓flatten_393]() {
  %1(x_rank) = $(3↓flatten_393):S_Prim_Rank(%para72_фinput)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1748/    x_rank = rank_(input)/
  %2(CNode_452) = S_Prim_MakeTuple(I64(0), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1750/        if x_rank in (0, 1):/
  %3(CNode_453) = S_Prim_in(%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1750/        if x_rank in (0, 1):/
  %4(CNode_454) = Cond(%3, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1750/        if x_rank in (0, 1):/
  %5(CNode_455) = Switch(%4, @2✓3↓flatten_456, @✗✓3↓flatten_457)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1750/        if x_rank in (0, 1):/
  %6(CNode_458) = %5()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1750/        if x_rank in (0, 1):/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1750/        if x_rank in (0, 1):/
}
# Order:
#   1: @✓3↓flatten_441:CNode_452{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<Int64Imm> 0, [2]: ValueNode<Int64Imm> 1}
#   2: @✓3↓flatten_441:CNode_453{[0]: ValueNode<DoSignaturePrimitive> S_Prim_in, [1]: x_rank, [2]: CNode_452}
#   3: @✓3↓flatten_441:CNode_454{[0]: ValueNode<Primitive> Cond, [1]: CNode_453, [2]: ValueNode<BoolImm> false}
#   4: @✓3↓flatten_441:CNode_455{[0]: ValueNode<Primitive> Switch, [1]: CNode_454, [2]: ValueNode<FuncGraph> 2✓3↓flatten_456, [3]: ValueNode<FuncGraph> ✗✓3↓flatten_457}
#   5: @✓3↓flatten_441:CNode_458{[0]: CNode_455}
#   6: @✓3↓flatten_441:CNode_459{[0]: ValueNode<Primitive> Return, [1]: CNode_458}


subgraph attr:
subgraph instance: ✗3↓flatten_442 : 0x140f14218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✗3↓flatten_442 parent: [subgraph @3↓flatten_393]() {
  %1(CNode_461) = call @4↓flatten_460()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
}
# Order:
#   1: @✗3↓flatten_442:CNode_461{[0]: ValueNode<FuncGraph> 4↓flatten_460}
#   2: @✗3↓flatten_442:CNode_462{[0]: ValueNode<Primitive> Return, [1]: CNode_461}


subgraph attr:
subgraph instance: ↰3↓flatten_436 : 0x140f13c18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↰3↓flatten_436 parent: [subgraph @flatten_167]() {
  %1(CNode_463) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %2(CNode_464) = S_Prim_equal(%para61_end_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
}
# Order:
#   1: @↰3↓flatten_436:CNode_463{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @↰3↓flatten_436:CNode_464{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: param_end_dim, [2]: CNode_463}
#   3: @↰3↓flatten_436:CNode_465{[0]: ValueNode<Primitive> Return, [1]: CNode_464}


subgraph attr:
subgraph instance: ↱3↓flatten_437 : 0x140f13618
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↱3↓flatten_437 parent: [subgraph @3↓flatten_393]() {
  %1(CNode_433) = $(3↓flatten_393):S_Prim_equal(%para60_start_dim, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
}
# Order:
#   1: @↱3↓flatten_437:CNode_466{[0]: ValueNode<Primitive> Return, [1]: CNode_433}


subgraph attr:
subgraph instance: _get_cache_prim_447 : 0x140f1a618
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/_primitive_cache.py:35/def _get_cache_prim(cls: Primitive) -> Primitive:/
subgraph @_get_cache_prim_447(%para74_cls) {
  %1(CNode_468) = call @✓_get_cache_prim_467()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/_primitive_cache.py:88/    if _is_need_compile(_temp_func): # @jit.cond: True/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/_primitive_cache.py:88/    if _is_need_compile(_temp_func): # @jit.cond: True/
}
# Order:
#   1: @_get_cache_prim_447:CNode_468{[0]: ValueNode<FuncGraph> ✓_get_cache_prim_467}
#   2: @_get_cache_prim_447:CNode_469{[0]: ValueNode<Primitive> Return, [1]: CNode_468}


subgraph attr:
subgraph instance: 2✓3↓flatten_456 : 0x140f1b818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @2✓3↓flatten_456 parent: [subgraph @3↓flatten_393]() {
  %1(CNode_470) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1751/            return reshape_(input, (-1,))/
  %2(CNode_471) = S_Prim_MakeTuple(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1751/            return reshape_(input, (-1,))/
  %3(CNode_472) = S_Prim_Reshape[output_names: ["output"], input_names: ["tensor", "shape"]](%para72_фinput, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1751/            return reshape_(input, (-1,))/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1751/            return reshape_(input, (-1,))/
}
# Order:
#   1: @2✓3↓flatten_456:CNode_470{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @2✓3↓flatten_456:CNode_471{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_470}
#   3: @2✓3↓flatten_456:CNode_472{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_фinput, [2]: CNode_471}
#   4: @2✓3↓flatten_456:CNode_473{[0]: ValueNode<Primitive> Return, [1]: CNode_472}


subgraph attr:
subgraph instance: ✗✓3↓flatten_457 : 0x140f02a18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✗✓3↓flatten_457 parent: [subgraph @3↓flatten_393]() {
  %1(CNode_475) = call @↓✓3↓flatten_474()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1750/        if x_rank in (0, 1):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1750/        if x_rank in (0, 1):/
}
# Order:
#   1: @✗✓3↓flatten_457:CNode_475{[0]: ValueNode<FuncGraph> ↓✓3↓flatten_474}
#   2: @✗✓3↓flatten_457:CNode_476{[0]: ValueNode<Primitive> Return, [1]: CNode_475}


subgraph attr:
after_block : 1
subgraph instance: 4↓flatten_460 : 0x140f14818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @4↓flatten_460 parent: [subgraph @3↓flatten_393]() {
  %1(x_rank) = $(3↓flatten_393):S_Prim_Rank(%para72_фinput)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1748/    x_rank = rank_(input)/
  %2(idx) = call @canonicalize_axis_477(%para60_start_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1755/    start_dim = canonicalize_axis(start_dim, x_rank)/
  %3(end_dim) = call @canonicalize_axis_477(%para61_end_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1756/    end_dim = canonicalize_axis(end_dim, x_rank)/
  %4(CNode_479) = call @check_dim_valid_478(%2, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1757/    check_dim_valid(start_dim, end_dim)/
  %5(CNode_480) = StopGradient(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
  %6(CNode_481) = S_Prim_MakeTuple(I64(0), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1759/    if x_rank in (0, 1):/
  %7(CNode_482) = S_Prim_in(%1, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1759/    if x_rank in (0, 1):/
  %8(CNode_483) = Cond(%7, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1759/    if x_rank in (0, 1):/
  %9(CNode_484) = Switch(%8, @✓4↓flatten_485, @✗4↓flatten_486)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1759/    if x_rank in (0, 1):/
  %10(CNode_487) = %9()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1759/    if x_rank in (0, 1):/
  %11(CNode_488) = Depend[side_effect_propagate: I64(1)](%10, %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1759/    if x_rank in (0, 1):/
  Return(%11)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1759/    if x_rank in (0, 1):/
}
# Order:
#   1: @4↓flatten_460:idx{[0]: ValueNode<FuncGraph> canonicalize_axis_477, [1]: param_start_dim, [2]: x_rank}
#   2: @4↓flatten_460:end_dim{[0]: ValueNode<FuncGraph> canonicalize_axis_477, [1]: param_end_dim, [2]: x_rank}
#   3: @4↓flatten_460:CNode_479{[0]: ValueNode<FuncGraph> check_dim_valid_478, [1]: idx, [2]: end_dim}
#   4: @4↓flatten_460:CNode_481{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<Int64Imm> 0, [2]: ValueNode<Int64Imm> 1}
#   5: @4↓flatten_460:CNode_482{[0]: ValueNode<DoSignaturePrimitive> S_Prim_in, [1]: x_rank, [2]: CNode_481}
#   6: @4↓flatten_460:CNode_483{[0]: ValueNode<Primitive> Cond, [1]: CNode_482, [2]: ValueNode<BoolImm> false}
#   7: @4↓flatten_460:CNode_484{[0]: ValueNode<Primitive> Switch, [1]: CNode_483, [2]: ValueNode<FuncGraph> ✓4↓flatten_485, [3]: ValueNode<FuncGraph> ✗4↓flatten_486}
#   8: @4↓flatten_460:CNode_487{[0]: CNode_484}
#   9: @4↓flatten_460:CNode_489{[0]: ValueNode<Primitive> Return, [1]: CNode_488}


subgraph attr:
subgraph instance: ✓_get_cache_prim_467 : 0x140f1ac18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/_primitive_cache.py:35/def _get_cache_prim(cls: Primitive) -> Primitive:/
subgraph @✓_get_cache_prim_467 parent: [subgraph @_get_cache_prim_447]() {
  Return(@_new_prim_for_graph_490)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/_primitive_cache.py:89/        return _new_prim_for_graph/
}
# Order:
#   1: @✓_get_cache_prim_467:CNode_491{[0]: ValueNode<Primitive> Return, [1]: ValueNode<FuncGraph> _new_prim_for_graph_490}


subgraph attr:
after_block : 1
subgraph instance: ↓✓3↓flatten_474 : 0x140f1a018
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↓✓3↓flatten_474 parent: [subgraph @3↓flatten_393]() {
  %1(CNode_492) = call @_get_cache_prim_447(ClassType)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1752/        return _get_cache_prim(P.Flatten)()(input)/
  %2(CNode_493) = %1()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1752/        return _get_cache_prim(P.Flatten)()(input)/
  %3(CNode_494) = %2(%para72_фinput)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1752/        return _get_cache_prim(P.Flatten)()(input)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1752/        return _get_cache_prim(P.Flatten)()(input)/
}
# Order:
#   1: @↓✓3↓flatten_474:CNode_492{[0]: ValueNode<FuncGraph> _get_cache_prim_447, [1]: ValueNode<ClassType> class 'mindspore.ops.operations.nn_ops.Flatten'}
#   2: @↓✓3↓flatten_474:CNode_493{[0]: CNode_492}
#   3: @↓✓3↓flatten_474:CNode_494{[0]: CNode_493, [1]: param_фinput}
#   4: @↓✓3↓flatten_474:CNode_495{[0]: ValueNode<Primitive> Return, [1]: CNode_494}


subgraph attr:
subgraph instance: check_dim_valid_478 : 0x140f00c18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1721/    def check_dim_valid(start_dim, end_dim):/
subgraph @check_dim_valid_478(%para75_start_dim, %para76_end_dim) {
  %1(CNode_496) = S_Prim_greater(%para75_start_dim, %para76_end_dim)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1722/        if start_dim > end_dim:/
  %2(CNode_497) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1722/        if start_dim > end_dim:/
  %3(CNode_498) = Switch(%2, @✓check_dim_valid_499, @✗check_dim_valid_500)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1722/        if start_dim > end_dim:/
  %4(CNode_501) = %3()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1722/        if start_dim > end_dim:/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1722/        if start_dim > end_dim:/
}
# Order:
#   1: @check_dim_valid_478:CNode_496{[0]: ValueNode<DoSignaturePrimitive> S_Prim_greater, [1]: param_start_dim, [2]: param_end_dim}
#   2: @check_dim_valid_478:CNode_497{[0]: ValueNode<Primitive> Cond, [1]: CNode_496, [2]: ValueNode<BoolImm> false}
#   3: @check_dim_valid_478:CNode_498{[0]: ValueNode<Primitive> Switch, [1]: CNode_497, [2]: ValueNode<FuncGraph> ✓check_dim_valid_499, [3]: ValueNode<FuncGraph> ✗check_dim_valid_500}
#   4: @check_dim_valid_478:CNode_501{[0]: CNode_498}
#   5: @check_dim_valid_478:CNode_502{[0]: ValueNode<Primitive> Return, [1]: CNode_501}


subgraph attr:
subgraph instance: canonicalize_axis_477 : 0x140f17e18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1725/    def canonicalize_axis(axis, x_rank):/
subgraph @canonicalize_axis_477(%para77_axis, %para78_x_rank) {
  %1(CNode_503) = S_Prim_not_equal(%para78_x_rank, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %2(CNode_504) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %3(CNode_505) = Switch(%2, @↰canonicalize_axis_506, @↱canonicalize_axis_507)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %4(ndim) = %3()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %5(CNode_509) = call @check_axis_valid_508(%para77_axis, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1727/        check_axis_valid(axis, ndim)/
  %6(CNode_510) = StopGradient(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1725/    def canonicalize_axis(axis, x_rank):/
  %7(CNode_511) = S_Prim_greater_equal(%para77_axis, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  %8(CNode_512) = Cond(%7, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  %9(CNode_513) = Switch(%8, @↰canonicalize_axis_514, @↱canonicalize_axis_515)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  %10(CNode_516) = %9()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  %11(CNode_517) = Depend[side_effect_propagate: I64(1)](%10, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  Return(%11)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
}
# Order:
#   1: @canonicalize_axis_477:CNode_503{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: param_x_rank, [2]: ValueNode<Int64Imm> 0}
#   2: @canonicalize_axis_477:CNode_504{[0]: ValueNode<Primitive> Cond, [1]: CNode_503, [2]: ValueNode<BoolImm> false}
#   3: @canonicalize_axis_477:CNode_505{[0]: ValueNode<Primitive> Switch, [1]: CNode_504, [2]: ValueNode<FuncGraph> ↰canonicalize_axis_506, [3]: ValueNode<FuncGraph> ↱canonicalize_axis_507}
#   4: @canonicalize_axis_477:ndim{[0]: CNode_505}
#   5: @canonicalize_axis_477:CNode_509{[0]: ValueNode<FuncGraph> check_axis_valid_508, [1]: param_axis, [2]: ndim}
#   6: @canonicalize_axis_477:CNode_511{[0]: ValueNode<DoSignaturePrimitive> S_Prim_greater_equal, [1]: param_axis, [2]: ValueNode<Int64Imm> 0}
#   7: @canonicalize_axis_477:CNode_512{[0]: ValueNode<Primitive> Cond, [1]: CNode_511, [2]: ValueNode<BoolImm> false}
#   8: @canonicalize_axis_477:CNode_513{[0]: ValueNode<Primitive> Switch, [1]: CNode_512, [2]: ValueNode<FuncGraph> ↰canonicalize_axis_514, [3]: ValueNode<FuncGraph> ↱canonicalize_axis_515}
#   9: @canonicalize_axis_477:CNode_516{[0]: CNode_513}
#  10: @canonicalize_axis_477:CNode_518{[0]: ValueNode<Primitive> Return, [1]: CNode_517}


subgraph attr:
subgraph instance: ✓4↓flatten_485 : 0x140f12e18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✓4↓flatten_485 parent: [subgraph @3↓flatten_393]() {
  %1(CNode_519) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1760/        return reshape_(input, (-1,))/
  %2(CNode_520) = S_Prim_MakeTuple(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1760/        return reshape_(input, (-1,))/
  %3(CNode_521) = S_Prim_Reshape[output_names: ["output"], input_names: ["tensor", "shape"]](%para72_фinput, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1760/        return reshape_(input, (-1,))/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1760/        return reshape_(input, (-1,))/
}
# Order:
#   1: @✓4↓flatten_485:CNode_519{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @✓4↓flatten_485:CNode_520{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_519}
#   3: @✓4↓flatten_485:CNode_521{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_фinput, [2]: CNode_520}
#   4: @✓4↓flatten_485:CNode_522{[0]: ValueNode<Primitive> Return, [1]: CNode_521}


subgraph attr:
subgraph instance: ✗4↓flatten_486 : 0x140f14e18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✗4↓flatten_486 parent: [subgraph @4↓flatten_460]() {
  %1(CNode_524) = call @5↓flatten_523()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1759/    if x_rank in (0, 1):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1759/    if x_rank in (0, 1):/
}
# Order:
#   1: @✗4↓flatten_486:CNode_524{[0]: ValueNode<FuncGraph> 5↓flatten_523}
#   2: @✗4↓flatten_486:CNode_525{[0]: ValueNode<Primitive> Return, [1]: CNode_524}


subgraph attr:
subgraph instance: _new_prim_for_graph_490 : 0x140f1b218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/_primitive_cache.py:67/    def _new_prim_for_graph(*args, **kwargs) -> Primitive:/
subgraph @_new_prim_for_graph_490 parent: [subgraph @_get_cache_prim_447](%para79_args, %para80_kwargs) {
  %1(CNode_526) = UnpackCall_unpack_call(%para74_cls, %para79_args, %para80_kwargs)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/_primitive_cache.py:68/        return cls(*args, **kwargs)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/_primitive_cache.py:68/        return cls(*args, **kwargs)/
}
# Order:
#   1: @_new_prim_for_graph_490:CNode_526{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.527, [1]: param_cls, [2]: param_args, [3]: param_kwargs}
#   2: @_new_prim_for_graph_490:CNode_528{[0]: ValueNode<Primitive> Return, [1]: CNode_526}


subgraph attr:
subgraph instance: ✓check_dim_valid_499 : 0x140f01e18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1721/    def check_dim_valid(start_dim, end_dim):/
subgraph @✓check_dim_valid_499() {
  %1(CNode_529) = raise[side_effect_io: Bool(1)]("ValueError", "For 'flatten', 'start_dim' cannot come after 'end_dim'.", "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1723/            raise ValueError("For 'flatten', 'start_dim' cannot come after 'end_dim'.")/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1723/            raise ValueError("For 'flatten', 'start_dim' cannot come after 'end_dim'.")/
}
# Order:
#   1: @✓check_dim_valid_499:CNode_529{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> ValueError, [2]: ValueNode<StringImm> For 'flatten', 'start_dim' cannot come after 'end_dim'., [3]: ValueNode<StringImm> None}
#   2: @✓check_dim_valid_499:CNode_530{[0]: ValueNode<Primitive> Return, [1]: CNode_529}


subgraph attr:
subgraph instance: ✗check_dim_valid_500 : 0x140f01218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1721/    def check_dim_valid(start_dim, end_dim):/
subgraph @✗check_dim_valid_500() {
  %1(CNode_532) = call @↓check_dim_valid_531()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1722/        if start_dim > end_dim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1722/        if start_dim > end_dim:/
}
# Order:
#   1: @✗check_dim_valid_500:CNode_532{[0]: ValueNode<FuncGraph> ↓check_dim_valid_531}
#   2: @✗check_dim_valid_500:CNode_533{[0]: ValueNode<Primitive> Return, [1]: CNode_532}


subgraph attr:
subgraph instance: check_axis_valid_508 : 0x140efe818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @check_axis_valid_508(%para81_axis, %para82_ndim) {
  %1(CNode_534) = S_Prim_negative(%para82_ndim)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %2(CNode_535) = S_Prim_less(%para81_axis, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %3(CNode_536) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %4(CNode_537) = Switch(%3, @↰check_axis_valid_538, @↱check_axis_valid_539)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %5(CNode_540) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %6(CNode_541) = Cond(%5, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %7(CNode_542) = Switch(%6, @✓check_axis_valid_543, @✗check_axis_valid_544)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %8(CNode_545) = %7()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_508:CNode_534{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: param_ndim}
#   2: @check_axis_valid_508:CNode_535{[0]: ValueNode<DoSignaturePrimitive> S_Prim_less, [1]: param_axis, [2]: CNode_534}
#   3: @check_axis_valid_508:CNode_536{[0]: ValueNode<Primitive> Cond, [1]: CNode_535, [2]: ValueNode<BoolImm> false}
#   4: @check_axis_valid_508:CNode_537{[0]: ValueNode<Primitive> Switch, [1]: CNode_536, [2]: ValueNode<FuncGraph> ↰check_axis_valid_538, [3]: ValueNode<FuncGraph> ↱check_axis_valid_539}
#   5: @check_axis_valid_508:CNode_540{[0]: CNode_537}
#   6: @check_axis_valid_508:CNode_541{[0]: ValueNode<Primitive> Cond, [1]: CNode_540, [2]: ValueNode<BoolImm> false}
#   7: @check_axis_valid_508:CNode_542{[0]: ValueNode<Primitive> Switch, [1]: CNode_541, [2]: ValueNode<FuncGraph> ✓check_axis_valid_543, [3]: ValueNode<FuncGraph> ✗check_axis_valid_544}
#   8: @check_axis_valid_508:CNode_545{[0]: CNode_542}
#   9: @check_axis_valid_508:CNode_546{[0]: ValueNode<Primitive> Return, [1]: CNode_545}


subgraph attr:
subgraph instance: ↰canonicalize_axis_506 : 0x140efe218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
subgraph @↰canonicalize_axis_506 parent: [subgraph @canonicalize_axis_477]() {
  Return(%para78_x_rank)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
}
# Order:
#   1: @↰canonicalize_axis_506:CNode_547{[0]: ValueNode<Primitive> Return, [1]: param_x_rank}


subgraph attr:
subgraph instance: ↱canonicalize_axis_507 : 0x140efdc18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
subgraph @↱canonicalize_axis_507() {
  Return(I64(1))
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
}
# Order:
#   1: @↱canonicalize_axis_507:CNode_548{[0]: ValueNode<Primitive> Return, [1]: ValueNode<Int64Imm> 1}


subgraph attr:
subgraph instance: ↰canonicalize_axis_514 : 0x140efd618
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
subgraph @↰canonicalize_axis_514 parent: [subgraph @canonicalize_axis_477]() {
  Return(%para77_axis)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
}
# Order:
#   1: @↰canonicalize_axis_514:CNode_549{[0]: ValueNode<Primitive> Return, [1]: param_axis}


subgraph attr:
subgraph instance: ↱canonicalize_axis_515 : 0x140efd018
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
subgraph @↱canonicalize_axis_515 parent: [subgraph @canonicalize_axis_477]() {
  %1(CNode_503) = $(canonicalize_axis_477):S_Prim_not_equal(%para78_x_rank, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %2(CNode_504) = $(canonicalize_axis_477):Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %3(CNode_505) = $(canonicalize_axis_477):Switch(%2, @↰canonicalize_axis_506, @↱canonicalize_axis_507)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %4(ndim) = $(canonicalize_axis_477):%3()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %5(CNode_550) = S_Prim_add(%para77_axis, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
}
# Order:
#   1: @↱canonicalize_axis_515:CNode_550{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: param_axis, [2]: ndim}
#   2: @↱canonicalize_axis_515:CNode_551{[0]: ValueNode<Primitive> Return, [1]: CNode_550}


subgraph attr:
after_block : 1
subgraph instance: 5↓flatten_523 : 0x140f15418
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @5↓flatten_523 parent: [subgraph @4↓flatten_460]() {
  %1(x_rank) = $(3↓flatten_393):S_Prim_Rank(%para72_фinput)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1748/    x_rank = rank_(input)/
  %2(idx) = $(4↓flatten_460):call @canonicalize_axis_477(%para60_start_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1755/    start_dim = canonicalize_axis(start_dim, x_rank)/
  %3(end_dim) = $(4↓flatten_460):call @canonicalize_axis_477(%para61_end_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1756/    end_dim = canonicalize_axis(end_dim, x_rank)/
  %4(CNode_552) = S_Prim_equal(%2, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1762/    if start_dim == end_dim:/
  %5(CNode_553) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1762/    if start_dim == end_dim:/
  %6(CNode_554) = Switch(%5, @✓5↓flatten_555, @✗5↓flatten_556)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1762/    if start_dim == end_dim:/
  %7(CNode_557) = %6()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1762/    if start_dim == end_dim:/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1762/    if start_dim == end_dim:/
}
# Order:
#   1: @5↓flatten_523:CNode_552{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: idx, [2]: end_dim}
#   2: @5↓flatten_523:CNode_553{[0]: ValueNode<Primitive> Cond, [1]: CNode_552, [2]: ValueNode<BoolImm> false}
#   3: @5↓flatten_523:CNode_554{[0]: ValueNode<Primitive> Switch, [1]: CNode_553, [2]: ValueNode<FuncGraph> ✓5↓flatten_555, [3]: ValueNode<FuncGraph> ✗5↓flatten_556}
#   4: @5↓flatten_523:CNode_557{[0]: CNode_554}
#   5: @5↓flatten_523:CNode_558{[0]: ValueNode<Primitive> Return, [1]: CNode_557}


subgraph attr:
after_block : 1
subgraph instance: ↓check_dim_valid_531 : 0x140f01818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1721/    def check_dim_valid(start_dim, end_dim):/
subgraph @↓check_dim_valid_531() {
  Return(None)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
}
# Order:
#   1: @↓check_dim_valid_531:CNode_559{[0]: ValueNode<Primitive> Return, [1]: ValueNode<None> None}


subgraph attr:
subgraph instance: ✓check_axis_valid_543 : 0x140f00618
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @✓check_axis_valid_543() {
  %1(CNode_560) = raise[side_effect_io: Bool(1)]("ValueError", "'start_dim' or 'end_dim' out of range.", "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1719/            raise ValueError("'start_dim' or 'end_dim' out of range.")/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1719/            raise ValueError("'start_dim' or 'end_dim' out of range.")/
}
# Order:
#   1: @✓check_axis_valid_543:CNode_560{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> ValueError, [2]: ValueNode<StringImm> 'start_dim' or 'end_dim' out of range., [3]: ValueNode<StringImm> None}
#   2: @✓check_axis_valid_543:CNode_561{[0]: ValueNode<Primitive> Return, [1]: CNode_560}


subgraph attr:
subgraph instance: ✗check_axis_valid_544 : 0x140effa18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @✗check_axis_valid_544() {
  %1(CNode_563) = call @↓check_axis_valid_562()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @✗check_axis_valid_544:CNode_563{[0]: ValueNode<FuncGraph> ↓check_axis_valid_562}
#   2: @✗check_axis_valid_544:CNode_564{[0]: ValueNode<Primitive> Return, [1]: CNode_563}


subgraph attr:
subgraph instance: ↰check_axis_valid_538 : 0x140eff418
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @↰check_axis_valid_538 parent: [subgraph @check_axis_valid_508]() {
  %1(CNode_534) = $(check_axis_valid_508):S_Prim_negative(%para82_ndim)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %2(CNode_535) = $(check_axis_valid_508):S_Prim_less(%para81_axis, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @↰check_axis_valid_538:CNode_565{[0]: ValueNode<Primitive> Return, [1]: CNode_535}


subgraph attr:
subgraph instance: ↱check_axis_valid_539 : 0x140efee18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @↱check_axis_valid_539 parent: [subgraph @check_axis_valid_508]() {
  %1(CNode_566) = S_Prim_greater_equal(%para81_axis, %para82_ndim)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @↱check_axis_valid_539:CNode_566{[0]: ValueNode<DoSignaturePrimitive> S_Prim_greater_equal, [1]: param_axis, [2]: param_ndim}
#   2: @↱check_axis_valid_539:CNode_567{[0]: ValueNode<Primitive> Return, [1]: CNode_566}


subgraph attr:
subgraph instance: ✓5↓flatten_555 : 0x140f17818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✓5↓flatten_555 parent: [subgraph @3↓flatten_393]() {
  Return(%para72_фinput)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1763/        return input/
}
# Order:
#   1: @✓5↓flatten_555:CNode_568{[0]: ValueNode<Primitive> Return, [1]: param_фinput}


subgraph attr:
subgraph instance: ✗5↓flatten_556 : 0x140f15a18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✗5↓flatten_556 parent: [subgraph @4↓flatten_460]() {
  %1(CNode_570) = call @6↓flatten_569()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1762/    if start_dim == end_dim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1762/    if start_dim == end_dim:/
}
# Order:
#   1: @✗5↓flatten_556:CNode_570{[0]: ValueNode<FuncGraph> 6↓flatten_569}
#   2: @✗5↓flatten_556:CNode_571{[0]: ValueNode<Primitive> Return, [1]: CNode_570}


subgraph attr:
after_block : 1
subgraph instance: ↓check_axis_valid_562 : 0x140f00018
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @↓check_axis_valid_562() {
  Return(None)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
}
# Order:
#   1: @↓check_axis_valid_562:CNode_572{[0]: ValueNode<Primitive> Return, [1]: ValueNode<None> None}


subgraph attr:
after_block : 1
subgraph instance: 6↓flatten_569 : 0x140f16018
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @6↓flatten_569 parent: [subgraph @4↓flatten_460]() {
  %1(x_rank) = $(3↓flatten_393):S_Prim_Rank(%para72_фinput)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1748/    x_rank = rank_(input)/
  %2(idx) = $(4↓flatten_460):call @canonicalize_axis_477(%para60_start_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1755/    start_dim = canonicalize_axis(start_dim, x_rank)/
  %3(CNode_574) = call @↵6↓flatten_573(%2, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1767/    while idx <= end_dim:/
}
# Order:
#   1: @6↓flatten_569:CNode_575{[0]: ValueNode<Primitive> Return, [1]: CNode_574}
#   2: @6↓flatten_569:CNode_574{[0]: ValueNode<FuncGraph> ↵6↓flatten_573, [1]: idx, [2]: ValueNode<Int64Imm> 1}


subgraph attr:
is_while_header : 1
subgraph instance: ↵6↓flatten_573 : 0x140f16618
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↵6↓flatten_573 parent: [subgraph @4↓flatten_460](%para83_, %para84_) {
  %1(x_rank) = $(3↓flatten_393):S_Prim_Rank(%para72_фinput)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1748/    x_rank = rank_(input)/
  %2(end_dim) = $(4↓flatten_460):call @canonicalize_axis_477(%para61_end_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1756/    end_dim = canonicalize_axis(end_dim, x_rank)/
  %3(CNode_576) = S_Prim_less_equal(%para83_фidx, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1767/    while idx <= end_dim:/
  %4(force_while_cond_CNode_576) = Cond(%3, Bool(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1767/    while idx <= end_dim:/
  %5(CNode_577) = Switch(%4, @↻6↓flatten_578, @7↓flatten_579)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1767/    while idx <= end_dim:/
  %6(CNode_580) = %5()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1767/    while idx <= end_dim:/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1767/    while idx <= end_dim:/
}
# Order:
#   1: @↵6↓flatten_573:CNode_576{[0]: ValueNode<DoSignaturePrimitive> S_Prim_less_equal, [1]: param_фidx, [2]: end_dim}
#   2: @↵6↓flatten_573:force_while_cond_CNode_576{[0]: ValueNode<Primitive> Cond, [1]: CNode_576, [2]: ValueNode<BoolImm> true}
#   3: @↵6↓flatten_573:CNode_577{[0]: ValueNode<Primitive> Switch, [1]: force_while_cond_CNode_576, [2]: ValueNode<FuncGraph> ↻6↓flatten_578, [3]: ValueNode<FuncGraph> 7↓flatten_579}
#   4: @↵6↓flatten_573:CNode_580{[0]: CNode_577}
#   5: @↵6↓flatten_573:CNode_581{[0]: ValueNode<Primitive> Return, [1]: CNode_580}


subgraph attr:
subgraph instance: ↻6↓flatten_578 : 0x140f17218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↻6↓flatten_578 parent: [subgraph @↵6↓flatten_573]() {
  %1(idx) = S_Prim_add(%para83_фidx, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1769/        idx += 1/
  %2(x_shape) = $(3↓flatten_393):S_Prim_Shape(%para72_фinput)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1747/    x_shape = shape_(input)/
  %3(CNode_582) = S_Prim_getitem(%2, %para83_фidx)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1768/        dim_length *= x_shape[idx]/
  %4(dim_length) = S_Prim_mul(%para84_фdim_length, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1768/        dim_length *= x_shape[idx]/
  %5(CNode_583) = call @↵6↓flatten_573(%1, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1767/    while idx <= end_dim:/
}
# Order:
#   1: @↻6↓flatten_578:CNode_582{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: param_фidx}
#   2: @↻6↓flatten_578:dim_length{[0]: ValueNode<DoSignaturePrimitive> S_Prim_mul, [1]: param_фdim_length, [2]: CNode_582}
#   3: @↻6↓flatten_578:idx{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: param_фidx, [2]: ValueNode<Int64Imm> 1}
#   4: @↻6↓flatten_578:CNode_584{[0]: ValueNode<Primitive> Return, [1]: CNode_583}
#   5: @↻6↓flatten_578:CNode_583{[0]: ValueNode<FuncGraph> ↵6↓flatten_573, [1]: idx, [2]: dim_length}


subgraph attr:
subgraph instance: 7↓flatten_579 : 0x140f16c18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @7↓flatten_579 parent: [subgraph @↵6↓flatten_573]() {
  %1(x_shape) = $(3↓flatten_393):S_Prim_Shape(%para72_фinput)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1747/    x_shape = shape_(input)/
  %2(x_rank) = $(3↓flatten_393):S_Prim_Rank(%para72_фinput)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1748/    x_rank = rank_(input)/
  %3(idx) = $(4↓flatten_460):call @canonicalize_axis_477(%para60_start_dim, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1755/    start_dim = canonicalize_axis(start_dim, x_rank)/
  %4(CNode_585) = S_Prim_make_slice(None, %3, None)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %5(CNode_586) = S_Prim_getitem(%1, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %6(CNode_587) = S_Prim_MakeTuple(%para84_фdim_length)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %7(CNode_588) = S_Prim_add(%5, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %8(end_dim) = $(4↓flatten_460):call @canonicalize_axis_477(%para61_end_dim, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1756/    end_dim = canonicalize_axis(end_dim, x_rank)/
  %9(CNode_589) = S_Prim_add(%8, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %10(CNode_590) = S_Prim_make_slice(%9, None, None)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %11(CNode_591) = S_Prim_getitem(%1, %10)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %12(new_shape) = S_Prim_add(%7, %11)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %13(CNode_592) = S_Prim_Reshape[output_names: ["output"], input_names: ["tensor", "shape"]](%para72_фinput, %12)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1771/    return reshape_(input, new_shape)/
  Return(%13)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1771/    return reshape_(input, new_shape)/
}
# Order:
#   1: @7↓flatten_579:CNode_585{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_slice, [1]: ValueNode<None> None, [2]: idx, [3]: ValueNode<None> None}
#   2: @7↓flatten_579:CNode_586{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: CNode_585}
#   3: @7↓flatten_579:CNode_587{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: param_фdim_length}
#   4: @7↓flatten_579:CNode_588{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: CNode_586, [2]: CNode_587}
#   5: @7↓flatten_579:CNode_589{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: end_dim, [2]: ValueNode<Int64Imm> 1}
#   6: @7↓flatten_579:CNode_590{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_slice, [1]: CNode_589, [2]: ValueNode<None> None, [3]: ValueNode<None> None}
#   7: @7↓flatten_579:CNode_591{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: CNode_590}
#   8: @7↓flatten_579:new_shape{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: CNode_588, [2]: CNode_591}
#   9: @7↓flatten_579:CNode_592{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_фinput, [2]: new_shape}
#  10: @7↓flatten_579:CNode_593{[0]: ValueNode<Primitive> Return, [1]: CNode_592}


