# ===============================================================================================
# The following shows the last analyze fail log message.
# ===============================================================================================

----------------------------------------------------
- Caught exception:
----------------------------------------------------
For 'MatMul' the input dimensions must be equal, but got 'x1_col': 576 and 'x2_row': 400.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore/core/ops/mat_mul.cc:107 InferShape

----------------------------------------------------
- The Traceback of Net Construct Code:
----------------------------------------------------
# 0 In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:417
        if not self.sense_flag:
# 1 In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:418
            return self._no_sens_impl(*inputs)
                   ^
# 2 In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:437
        if self.return_grad:
# 3 In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:433
        loss = self.network(*inputs)
               ^
# 4 In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:121
        out = self._backbone(data)
              ^
# 5 In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:124
        x = self.relu(self.fc1(x))
                      ^
# 6 In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:126
        x = self.fc3(x)
            ^
# 7 In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:628
        if self.has_bias:
# 8 In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:629
            x = self.bias_add(x, self.bias)
            ^
# 9 In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:630
        if self.activation_flag:
# 10 In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632
        if len(x_shape) != 2:
# 11 In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:627
        x = self.matmul(x, self.weight)
            ^

# ===============================================================================================
# The following shows the IR when the function graphs evaluation fails to help locate the problem.
# You can search the last ------------------------> to the node which is evaluated failure.
# Refer to https://www.mindspore.cn/search?inputValue=analyze_fail.ir to get more instructions.
# ===============================================================================================

# IR entry: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11
# Total subgraphs: 140

# Attrs:
training : 1

# Total params: 21
# Params:
%para1_inputs0 : <null>
%para2_inputs1 : <null>
%para3_conv1.weight : <Ref[Tensor[Float32]], (6, 1, 3, 3), ref_key=:conv1.weight>  :  has_default
%para4_conv2.weight : <Ref[Tensor[Float32]], (16, 6, 3, 3), ref_key=:conv2.weight>  :  has_default
%para5_fc1.weight : <Ref[Tensor[Float32]], (120, 400), ref_key=:fc1.weight>  :  has_default
%para6_fc1.bias : <Ref[Tensor[Float32]], (120), ref_key=:fc1.bias>  :  has_default
%para7_fc2.weight : <Ref[Tensor[Float32]], (84, 120), ref_key=:fc2.weight>  :  has_default
%para8_fc2.bias : <Ref[Tensor[Float32]], (84), ref_key=:fc2.bias>  :  has_default
%para9_fc3.weight : <Ref[Tensor[Float32]], (10, 84), ref_key=:fc3.weight>  :  has_default
%para10_fc3.bias : <Ref[Tensor[Float32]], (10), ref_key=:fc3.bias>  :  has_default
%para11_global_step : <Ref[Tensor[Int32]], (1), ref_key=:global_step>  :  has_default
%para12_moments.conv1.weight : <Ref[Tensor[Float32]], (6, 1, 3, 3), ref_key=:moments.conv1.weight>  :  has_default
%para13_moments.conv2.weight : <Ref[Tensor[Float32]], (16, 6, 3, 3), ref_key=:moments.conv2.weight>  :  has_default
%para14_moments.fc1.weight : <Ref[Tensor[Float32]], (120, 400), ref_key=:moments.fc1.weight>  :  has_default
%para15_moments.fc1.bias : <Ref[Tensor[Float32]], (120), ref_key=:moments.fc1.bias>  :  has_default
%para16_moments.fc2.weight : <Ref[Tensor[Float32]], (84, 120), ref_key=:moments.fc2.weight>  :  has_default
%para17_moments.fc2.bias : <Ref[Tensor[Float32]], (84), ref_key=:moments.fc2.bias>  :  has_default
%para18_moments.fc3.weight : <Ref[Tensor[Float32]], (10, 84), ref_key=:moments.fc3.weight>  :  has_default
%para19_moments.fc3.bias : <Ref[Tensor[Float32]], (10), ref_key=:moments.fc3.bias>  :  has_default
%para20_momentum : <Ref[Tensor[Float32]], (), ref_key=:momentum>  :  has_default
%para21_learning_rate : <Ref[Tensor[Float32]], (), ref_key=:learning_rate>  :  has_default

subgraph attr:
training : 1
subgraph instance: mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11 : 0x11fb0a218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:416/    def construct(self, *inputs):/
subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11(%para1_inputs0, %para2_inputs1, %para3_conv1.weight, %para4_conv2.weight, %para5_fc1.weight, %para6_fc1.bias, %para7_fc2.weight, %para8_fc2.bias, %para9_fc3.weight, %para10_fc3.bias, %para11_global_step, %para12_moments.conv1.weight, %para13_moments.conv2.weight, %para14_moments.fc1.weight, %para15_moments.fc1.bias, %para16_moments.fc2.weight, %para17_moments.fc2.bias, %para18_moments.fc3.weight, %para19_moments.fc3.bias, %para20_momentum, %para21_learning_rate) {

#------------------------> 0
  %1(CNode_32) = call @✓mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12()
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:417/        if not self.sense_flag:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:417/        if not self.sense_flag:/
}
# Order:
#   1: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11:CNode_32{[0]: ValueNode<FuncGraph> ✓mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12}
#   2: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11:CNode_33{[0]: ValueNode<Primitive> Return, [1]: CNode_32}


subgraph attr:
training : 1
subgraph instance: ✓mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12 : 0x11fad7818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:416/    def construct(self, *inputs):/
subgraph @✓mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11]() {
  %1(CNode_34) = $(mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11):MakeTuple(%para1_inputs0, %para2_inputs1)
      : (<Tensor[Float32], (32, 1, 32, 32)>, <Tensor[Int32], (32)>) -> (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:416/    def construct(self, *inputs):/

#------------------------> 1
  %2(CNode_35) = UnpackCall_unpack_call(@_no_sens_impl_36, %1)
      : (<Func, NoShape>, <Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>) -> (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
  Return(%2)
      : (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
}
# Order:
#   1: @✓mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12:CNode_35{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.37, [1]: ValueNode<FuncGraph> _no_sens_impl_36, [2]: CNode_34}
#   2: @✓mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12:CNode_38{[0]: ValueNode<Primitive> Return, [1]: CNode_35}


subgraph attr:
core : 1
subgraph instance: UnpackCall_13 : 0x13e613e18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
subgraph @UnpackCall_13(%para22_, %para23_) {
  %1(CNode_35) = TupleGetItem(%para23_15, I64(0))
      : (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>, <Int64, NoShape>) -> (<Tensor[Float32], (32, 1, 32, 32)>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
  %2(CNode_35) = TupleGetItem(%para23_15, I64(1))
      : (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>, <Int64, NoShape>) -> (<Tensor[Int32], (32)>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/

#------------------------> 2
  %3(CNode_35) = %para22_14(%1, %2)
      : (<Tensor[Float32], (32, 1, 32, 32)>, <Tensor[Int32], (32)>) -> (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
  Return(%3)
      : (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
}
# Order:
#   1: @UnpackCall_13:CNode_35{[0]: param_14, [1]: CNode_35, [2]: CNode_35}
#   2: @UnpackCall_13:CNode_35{[0]: ValueNode<Primitive> Return, [1]: CNode_35}


subgraph attr:
training : 1
subgraph instance: _no_sens_impl_16 : 0x13e613818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @_no_sens_impl_16 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11](%para24_inputs0, %para25_inputs1) {

#------------------------> 3
  %1(CNode_39) = call @✗_no_sens_impl_17()
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:437/        if self.return_grad:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:437/        if self.return_grad:/
}
# Order:
#   1: @_no_sens_impl_16:loss{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.40, [1]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_22, [2]: CNode_41}
#   2: @_no_sens_impl_16:grads{[0]: ValueNode<UnpackGraphPrimitive> UnpackGraph, [1]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_22, [2]: CNode_41}
#   3: @_no_sens_impl_16:grads{[0]: ValueNode<DoSignaturePrimitive> S_Prim_grad, [1]: grads, [2]: CNode_42}
#   4: @_no_sens_impl_16:grads{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.43, [1]: grads, [2]: CNode_41}
#   5: @_no_sens_impl_16:grads{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Identity_construct_44, [1]: grads}
#   6: @_no_sens_impl_16:CNode_45{[0]: ValueNode<FuncGraph> mindspore_nn_optim_momentum_Momentum_construct_46, [1]: grads}
#   7: @_no_sens_impl_16:loss{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Depend, [1]: loss, [2]: CNode_45}
#   8: @_no_sens_impl_16:CNode_39{[0]: ValueNode<FuncGraph> ✗_no_sens_impl_17}
#   9: @_no_sens_impl_16:CNode_47{[0]: ValueNode<Primitive> Return, [1]: CNode_39}


subgraph attr:
training : 1
subgraph instance: ✗_no_sens_impl_17 : 0x13e618418
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @✗_no_sens_impl_17 parent: [subgraph @_no_sens_impl_16]() {

#------------------------> 4
  %1(CNode_48) = call @↓_no_sens_impl_18()
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:437/        if self.return_grad:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:437/        if self.return_grad:/
}
# Order:
#   1: @✗_no_sens_impl_17:CNode_48{[0]: ValueNode<FuncGraph> ↓_no_sens_impl_18}
#   2: @✗_no_sens_impl_17:CNode_49{[0]: ValueNode<Primitive> Return, [1]: CNode_48}


subgraph attr:
training : 1
subgraph instance: ↓_no_sens_impl_18 : 0x13e60ec18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @↓_no_sens_impl_18 parent: [subgraph @_no_sens_impl_16]() {
  %1(CNode_41) = $(_no_sens_impl_16):MakeTuple(%para24_inputs0, %para25_inputs1)
      : (<Tensor[Float32], (32, 1, 32, 32)>, <Tensor[Int32], (32)>) -> (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/

#------------------------> 5
  %2(loss) = $(_no_sens_impl_16):UnpackCall_unpack_call(@mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_22, %1)
      : (<Func, NoShape>, <Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>) -> (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:433/        loss = self.network(*inputs)/
  %3(grads) = $(_no_sens_impl_16):UnpackGraph(@mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_22, %1)
      : (<null>, <Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>) -> (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %4(CNode_42) = $(_no_sens_impl_16):MakeTuple(%para3_conv1.weight, %para4_conv2.weight, %para5_fc1.weight, %para6_fc1.bias, %para7_fc2.weight, %para8_fc2.bias, %para9_fc3.weight, %para10_fc3.bias)
      : (<Ref[Tensor[Float32]], (6, 1, 3, 3)>, <Ref[Tensor[Float32]], (16, 6, 3, 3)>, <Ref[Tensor[Float32]], (120, 400)>, <Ref[Tensor[Float32]], (120)>, <Ref[Tensor[Float32]], (84, 120)>, <Ref[Tensor[Float32]], (84)>, <Ref[Tensor[Float32]], (10, 84)>, <Ref[Tensor[Float32]], (10)>) -> (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %5(grads) = $(_no_sens_impl_16):S_Prim_grad(%3, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %6(grads) = $(_no_sens_impl_16):UnpackCall_unpack_call(%5, %1)
      : (<null>, <Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>) -> (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %7(grads) = $(_no_sens_impl_16):call @mindspore_nn_layer_basic_Identity_construct_44(%6)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:435/        grads = self.grad_reducer(grads)/
  %8(CNode_45) = $(_no_sens_impl_16):call @mindspore_nn_optim_momentum_Momentum_construct_46(%7)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:436/        loss = F.depend(loss, self.optimizer(grads))/
  %9(loss) = $(_no_sens_impl_16):S_Prim_Depend[side_effect_propagate: I64(1)](%2, %8)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:436/        loss = F.depend(loss, self.optimizer(grads))/
  Return(%9)
      : (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:442/        return loss/
}
# Order:
#   1: @↓_no_sens_impl_18:CNode_50{[0]: ValueNode<Primitive> Return, [1]: loss}


subgraph attr:
core : 1
subgraph instance: UnpackCall_19 : 0x13e630c18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:433/        loss = self.network(*inputs)/
subgraph @UnpackCall_19(%para26_, %para27_) {
  %1(loss) = TupleGetItem(%para27_21, I64(0))
      : (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>, <Int64, NoShape>) -> (<Tensor[Float32], (32, 1, 32, 32)>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:433/        loss = self.network(*inputs)/
  %2(loss) = TupleGetItem(%para27_21, I64(1))
      : (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>, <Int64, NoShape>) -> (<Tensor[Int32], (32)>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:433/        loss = self.network(*inputs)/

#------------------------> 6
  %3(loss) = %para26_20(%1, %2)
      : (<Tensor[Float32], (32, 1, 32, 32)>, <Tensor[Int32], (32)>) -> (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:433/        loss = self.network(*inputs)/
  Return(%3)
      : (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:433/        loss = self.network(*inputs)/
}
# Order:
#   1: @UnpackCall_19:loss{[0]: param_20, [1]: loss, [2]: loss}
#   2: @UnpackCall_19:loss{[0]: ValueNode<Primitive> Return, [1]: loss}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_22 : 0x13e60f218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:120/    def construct(self, data, label):/
subgraph @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_22 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11](%para28_data, %para29_label) {

#------------------------> 7
  %1(out) = call @__main___LeNet5_construct_23(%para28_data)
      : (<Tensor[Float32], (32, 1, 32, 32)>) -> (<null>)
      #scope: (Default/network-WithLossCell)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:121/        out = self._backbone(data)/
  %2(CNode_52) = call @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_51(%1, %para29_label)
      : (<null>, <Tensor[Int32], (32)>) -> (<null>)
      #scope: (Default/network-WithLossCell)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:122/        return self._loss_fn(out, label)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:122/        return self._loss_fn(out, label)/
}
# Order:
#   1: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_22:out{[0]: ValueNode<FuncGraph> __main___LeNet5_construct_23, [1]: param_data}
#   2: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_22:CNode_52{[0]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_51, [1]: out, [2]: param_label}
#   3: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_22:CNode_53{[0]: ValueNode<Primitive> Return, [1]: CNode_52}


subgraph attr:
training : 1
subgraph instance: __main___LeNet5_construct_23 : 0x13e619c18
# In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:120/    def construct(self, x):/
subgraph @__main___LeNet5_construct_23 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11](%para30_x) {
  %1(CNode_55) = call @mindspore_nn_layer_conv_Conv2d_construct_54(%para30_x)
      : (<Tensor[Float32], (32, 1, 32, 32)>) -> (<Tensor[Float32], (32, 6, 30, 30)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:121/        x = self.max_pool2d(self.relu(self.conv1(x)))/
  %2(CNode_57) = call @mindspore_nn_layer_activation_ReLU_construct_56(%1)
      : (<Tensor[Float32], (32, 6, 30, 30)>) -> (<Tensor[Float32], (32, 6, 30, 30)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:121/        x = self.max_pool2d(self.relu(self.conv1(x)))/
  %3(x) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_58(%2)
      : (<Tensor[Float32], (32, 6, 30, 30)>) -> (<Tensor[Float32], (32, 6, 15, 15)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:121/        x = self.max_pool2d(self.relu(self.conv1(x)))/
  %4(CNode_60) = call @mindspore_nn_layer_conv_Conv2d_construct_59(%3)
      : (<Tensor[Float32], (32, 6, 15, 15)>) -> (<Tensor[Float32], (32, 16, 13, 13)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:122/        x = self.max_pool2d(self.relu(self.conv2(x)))/
  %5(CNode_61) = call @mindspore_nn_layer_activation_ReLU_construct_56(%4)
      : (<Tensor[Float32], (32, 16, 13, 13)>) -> (<Tensor[Float32], (32, 16, 13, 13)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:122/        x = self.max_pool2d(self.relu(self.conv2(x)))/
  %6(x) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_58(%5)
      : (<Tensor[Float32], (32, 16, 13, 13)>) -> (<Tensor[Float32], (32, 16, 6, 6)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:122/        x = self.max_pool2d(self.relu(self.conv2(x)))/
  %7(x) = call @mindspore_nn_layer_basic_Flatten_construct_62(%6)
      : (<Tensor[Float32], (32, 16, 6, 6)>) -> (<Tensor[Float32], (32, 576)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:123/        x = self.flatten(x)/

#------------------------> 8
  %8(CNode_63) = call @mindspore_nn_layer_basic_Dense_construct_24(%7)
      : (<Tensor[Float32], (32, 576)>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:124/        x = self.relu(self.fc1(x))/
  %9(x) = call @mindspore_nn_layer_activation_ReLU_construct_56(%8)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:124/        x = self.relu(self.fc1(x))/
  %10(CNode_65) = call @mindspore_nn_layer_basic_Dense_construct_64(%9)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:125/        x = self.relu(self.fc2(x))/
  %11(x) = call @mindspore_nn_layer_activation_ReLU_construct_56(%10)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:125/        x = self.relu(self.fc2(x))/
  %12(x) = call @mindspore_nn_layer_basic_Dense_construct_66(%11)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:126/        x = self.fc3(x)/
  Return(%12)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:127/        return x/
}
# Order:
#   1: @__main___LeNet5_construct_23:CNode_55{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_54, [1]: param_x}
#   2: @__main___LeNet5_construct_23:CNode_57{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_56, [1]: CNode_55}
#   3: @__main___LeNet5_construct_23:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_58, [1]: CNode_57}
#   4: @__main___LeNet5_construct_23:CNode_60{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_59, [1]: x}
#   5: @__main___LeNet5_construct_23:CNode_61{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_56, [1]: CNode_60}
#   6: @__main___LeNet5_construct_23:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_58, [1]: CNode_61}
#   7: @__main___LeNet5_construct_23:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Flatten_construct_62, [1]: x}
#   8: @__main___LeNet5_construct_23:CNode_63{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_24, [1]: x}
#   9: @__main___LeNet5_construct_23:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_56, [1]: CNode_63}
#  10: @__main___LeNet5_construct_23:CNode_65{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_64, [1]: x}
#  11: @__main___LeNet5_construct_23:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_56, [1]: CNode_65}
#  12: @__main___LeNet5_construct_23:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_66, [1]: x}
#  13: @__main___LeNet5_construct_23:CNode_67{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct_24 : 0x13e636218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct_24 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11](%para31_x) {

#------------------------> 9
  %1(CNode_68) = call @L_mindspore_nn_layer_basic_Dense_construct_25(%para31_x, %para6_fc1.bias, %para5_fc1.weight)
      : (<Tensor[Float32], (32, 576)>, <Ref[Tensor[Float32]], (120)>, <Ref[Tensor[Float32]], (120, 400)>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc1-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct_24:CNode_68{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_25, [1]: param_x, [2]: param_fc1.bias, [3]: param_fc1.weight}
#   2: @mindspore_nn_layer_basic_Dense_construct_24:CNode_69{[0]: ValueNode<Primitive> Return, [1]: CNode_68}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_25 : 0x13e636818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_25(%para32_x, %para33_, %para34_) {
  %1(x_shape) = S_Prim_Shape(%para32_x)
      : (<Tensor[Float32], (32, 576)>) -> (<Tuple[Int64*2], TupleShape(NoShape, NoShape)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:623/        x_shape = self.shape_op(x)/
  %2(CNode_70) = S_Prim_check_dense_input_shape[constexpr_prim: Bool(1)](%1, "Dense")
      : (<Tuple[Int64*2], TupleShape(NoShape, NoShape)>, <String, NoShape>) -> (<None, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:624/        check_dense_input_shape(x_shape, self.cls_name)/
  %3(CNode_71) = StopGradient(%2)
      : (<None, NoShape>) -> (<None, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
  %4(CNode_72) = S_Prim_inner_len(%1)
      : (<Tuple[Int64*2], TupleShape(NoShape, NoShape)>) -> (<Int64, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
  %5(CNode_73) = S_Prim_not_equal(%4, I64(2))
      : (<Int64, NoShape>, <Int64, NoShape>) -> (<Bool, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
  %6(CNode_74) = Cond(%5, Bool(0))
      : (<Bool, NoShape>, <Bool, NoShape>) -> (<Bool, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
  %7(CNode_75) = Switch(%6, @L_✓mindspore_nn_layer_basic_Dense_construct_76, @L_✗mindspore_nn_layer_basic_Dense_construct_77)
      : (<Bool, NoShape>, <Func, NoShape>, <Func, NoShape>) -> (<Func, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
  %8(CNode_78) = %7()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/

#------------------------> 10
  %9(CNode_79) = call @L_↓mindspore_nn_layer_basic_Dense_construct_26(%8)
      : (<Tensor[Float32], (32, 576)>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:126/        x = self.fc3(x)/
  %10(CNode_80) = Depend[side_effect_propagate: I64(1)](%9, %3)
      : (<null>, <None, NoShape>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:126/        x = self.fc3(x)/
  Return(%10)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_25:x_shape{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_x}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_25:CNode_70{[0]: ValueNode<DoSignaturePrimitive> S_Prim_check_dense_input_shape, [1]: x_shape, [2]: ValueNode<StringImm> Dense}
#   3: @L_mindspore_nn_layer_basic_Dense_construct_25:CNode_72{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: x_shape}
#   4: @L_mindspore_nn_layer_basic_Dense_construct_25:CNode_73{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: CNode_72, [2]: ValueNode<Int64Imm> 2}
#   5: @L_mindspore_nn_layer_basic_Dense_construct_25:CNode_74{[0]: ValueNode<Primitive> Cond, [1]: CNode_73, [2]: ValueNode<BoolImm> false}
#   6: @L_mindspore_nn_layer_basic_Dense_construct_25:CNode_75{[0]: ValueNode<Primitive> Switch, [1]: CNode_74, [2]: ValueNode<FuncGraph> L_✓mindspore_nn_layer_basic_Dense_construct_76, [3]: ValueNode<FuncGraph> L_✗mindspore_nn_layer_basic_Dense_construct_77}
#   7: @L_mindspore_nn_layer_basic_Dense_construct_25:CNode_78{[0]: CNode_75}
#   8: @L_mindspore_nn_layer_basic_Dense_construct_25:CNode_79{[0]: ValueNode<FuncGraph> L_↓mindspore_nn_layer_basic_Dense_construct_26, [1]: CNode_78}
#   9: @L_mindspore_nn_layer_basic_Dense_construct_25:CNode_80{[0]: ValueNode<Primitive> Depend, [1]: CNode_79, [2]: CNode_71}
#  10: @L_mindspore_nn_layer_basic_Dense_construct_25:CNode_81{[0]: ValueNode<Primitive> Return, [1]: CNode_80}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_↓mindspore_nn_layer_basic_Dense_construct_26 : 0x13e648c18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_↓mindspore_nn_layer_basic_Dense_construct_26 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_25](%para35_) {

#------------------------> 11
  %1(CNode_82) = call @L_✓↓mindspore_nn_layer_basic_Dense_construct_27()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:628/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:628/        if self.has_bias:/
}
# Order:
#   1: @L_↓mindspore_nn_layer_basic_Dense_construct_26:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MatMul, [1]: param_фx, [2]: param_L_fc3.weight}
#   2: @L_↓mindspore_nn_layer_basic_Dense_construct_26:CNode_82{[0]: ValueNode<FuncGraph> L_✓↓mindspore_nn_layer_basic_Dense_construct_27}
#   3: @L_↓mindspore_nn_layer_basic_Dense_construct_26:CNode_83{[0]: ValueNode<Primitive> Return, [1]: CNode_82}


subgraph attr:
training : 1
subgraph instance: L_✓↓mindspore_nn_layer_basic_Dense_construct_27 : 0x13e649218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_27 parent: [subgraph @L_↓mindspore_nn_layer_basic_Dense_construct_26]() {

#------------------------> 12
  %1(CNode_84) = call @L_2↓mindspore_nn_layer_basic_Dense_construct_28()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:629/            x = self.bias_add(x, self.bias)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:629/            x = self.bias_add(x, self.bias)/
}
# Order:
#   1: @L_✓↓mindspore_nn_layer_basic_Dense_construct_27:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BiasAdd, [1]: x, [2]: param_L_fc3.bias}
#   2: @L_✓↓mindspore_nn_layer_basic_Dense_construct_27:CNode_84{[0]: ValueNode<FuncGraph> L_2↓mindspore_nn_layer_basic_Dense_construct_28}
#   3: @L_✓↓mindspore_nn_layer_basic_Dense_construct_27:CNode_85{[0]: ValueNode<Primitive> Return, [1]: CNode_84}


subgraph attr:
training : 1
subgraph instance: L_2↓mindspore_nn_layer_basic_Dense_construct_28 : 0x13e649818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_2↓mindspore_nn_layer_basic_Dense_construct_28 parent: [subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_27]() {

#------------------------> 13
  %1(CNode_86) = call @L_✗2↓mindspore_nn_layer_basic_Dense_construct_29()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:630/        if self.activation_flag:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:630/        if self.activation_flag:/
}
# Order:
#   1: @L_2↓mindspore_nn_layer_basic_Dense_construct_28:CNode_86{[0]: ValueNode<FuncGraph> L_✗2↓mindspore_nn_layer_basic_Dense_construct_29}
#   2: @L_2↓mindspore_nn_layer_basic_Dense_construct_28:CNode_87{[0]: ValueNode<Primitive> Return, [1]: CNode_86}


subgraph attr:
training : 1
subgraph instance: L_✗2↓mindspore_nn_layer_basic_Dense_construct_29 : 0x13e649e18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_✗2↓mindspore_nn_layer_basic_Dense_construct_29 parent: [subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_27]() {

#------------------------> 14
  %1(CNode_88) = call @L_3↓mindspore_nn_layer_basic_Dense_construct_30()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:630/        if self.activation_flag:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:630/        if self.activation_flag:/
}
# Order:
#   1: @L_✗2↓mindspore_nn_layer_basic_Dense_construct_29:CNode_88{[0]: ValueNode<FuncGraph> L_3↓mindspore_nn_layer_basic_Dense_construct_30}
#   2: @L_✗2↓mindspore_nn_layer_basic_Dense_construct_29:CNode_89{[0]: ValueNode<Primitive> Return, [1]: CNode_88}


subgraph attr:
training : 1
subgraph instance: L_3↓mindspore_nn_layer_basic_Dense_construct_30 : 0x13e64a418
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_3↓mindspore_nn_layer_basic_Dense_construct_30 parent: [subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_27]() {
  %1(x_shape) = $(L_mindspore_nn_layer_basic_Dense_construct_25):S_Prim_Shape(%para32_x)
      : (<Tensor[Float32], (32, 576)>) -> (<Tuple[Int64*2], TupleShape(NoShape, NoShape)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:623/        x_shape = self.shape_op(x)/
  %2(CNode_90) = S_Prim_inner_len(%1)
      : (<Tuple[Int64*2], TupleShape(NoShape, NoShape)>) -> (<Int64, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
  %3(CNode_91) = S_Prim_not_equal(%2, I64(2))
      : (<Int64, NoShape>, <Int64, NoShape>) -> (<Bool, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
  %4(CNode_92) = Cond(%3, Bool(0))
      : (<Bool, NoShape>, <Bool, NoShape>) -> (<Bool, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
  %5(CNode_93) = Switch(%4, @L_✓3↓mindspore_nn_layer_basic_Dense_construct_94, @L_✗3↓mindspore_nn_layer_basic_Dense_construct_31)
      : (<Bool, NoShape>, <Func, NoShape>, <Func, NoShape>) -> (<Func, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/

#------------------------> 15
  %6(CNode_95) = %5()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
  %7(CNode_97) = call @L_4↓mindspore_nn_layer_basic_Dense_construct_96(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:126/        x = self.fc3(x)/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_3↓mindspore_nn_layer_basic_Dense_construct_30:CNode_90{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: x_shape}
#   2: @L_3↓mindspore_nn_layer_basic_Dense_construct_30:CNode_91{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: CNode_90, [2]: ValueNode<Int64Imm> 2}
#   3: @L_3↓mindspore_nn_layer_basic_Dense_construct_30:CNode_92{[0]: ValueNode<Primitive> Cond, [1]: CNode_91, [2]: ValueNode<BoolImm> false}
#   4: @L_3↓mindspore_nn_layer_basic_Dense_construct_30:CNode_93{[0]: ValueNode<Primitive> Switch, [1]: CNode_92, [2]: ValueNode<FuncGraph> L_✓3↓mindspore_nn_layer_basic_Dense_construct_94, [3]: ValueNode<FuncGraph> L_✗3↓mindspore_nn_layer_basic_Dense_construct_31}
#   5: @L_3↓mindspore_nn_layer_basic_Dense_construct_30:CNode_95{[0]: CNode_93}
#   6: @L_3↓mindspore_nn_layer_basic_Dense_construct_30:CNode_97{[0]: ValueNode<FuncGraph> L_4↓mindspore_nn_layer_basic_Dense_construct_96, [1]: CNode_95}
#   7: @L_3↓mindspore_nn_layer_basic_Dense_construct_30:CNode_98{[0]: ValueNode<Primitive> Return, [1]: CNode_97}


subgraph attr:
training : 1
subgraph instance: L_✗3↓mindspore_nn_layer_basic_Dense_construct_31 : 0x13e64aa18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_✗3↓mindspore_nn_layer_basic_Dense_construct_31 parent: [subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_27]() {

#------------------------> 16
  %1(x) = $(L_↓mindspore_nn_layer_basic_Dense_construct_26):S_Prim_MatMul[output_names: ["output"], transpose_a: Bool(0), input_names: ["x1", "x2"], transpose_x2: Bool(1), transpose_x1: Bool(0), transpose_b: Bool(1)](%para35_фx, %para34_L_fc3.weight)
      : (<Tensor[Float32], (32, 576)>, <Ref[Tensor[Float32]], (120, 400)>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:627/        x = self.matmul(x, self.weight)/
  %2(x) = $(L_✓↓mindspore_nn_layer_basic_Dense_construct_27):S_Prim_BiasAdd[output_names: ["output"], format: "NCHW", input_names: ["x", "b"]](%1, %para33_L_fc3.bias)
      : (<null>, <Ref[Tensor[Float32]], (120)>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:629/            x = self.bias_add(x, self.bias)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_✗3↓mindspore_nn_layer_basic_Dense_construct_31:CNode_99{[0]: ValueNode<Primitive> Return, [1]: x}


# ===============================================================================================
# The total of function graphs in evaluation stack: 17/18 (Ignored 1 internal frames).
# ===============================================================================================


# ===============================================================================================
# The rest function graphs are the following:
# ===============================================================================================
subgraph attr:
training : 1
subgraph instance: _no_sens_impl_36 : 0x11fb29218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @_no_sens_impl_36 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11](%para36_inputs) {
  %1(CNode_39) = call @✗_no_sens_impl_100()
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:437/        if self.return_grad:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:437/        if self.return_grad:/
}
# Order:
#   1: @_no_sens_impl_36:loss{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.40, [1]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_101, [2]: param_inputs}
#   2: @_no_sens_impl_36:grads{[0]: ValueNode<UnpackGraphPrimitive> UnpackGraph, [1]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_101, [2]: param_inputs}
#   3: @_no_sens_impl_36:grads{[0]: ValueNode<DoSignaturePrimitive> S_Prim_grad, [1]: grads, [2]: CNode_42}
#   4: @_no_sens_impl_36:grads{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.43, [1]: grads, [2]: param_inputs}
#   5: @_no_sens_impl_36:grads{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Identity_construct_102, [1]: grads}
#   6: @_no_sens_impl_36:CNode_45{[0]: ValueNode<FuncGraph> mindspore_nn_optim_momentum_Momentum_construct_103, [1]: grads}
#   7: @_no_sens_impl_36:loss{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Depend, [1]: loss, [2]: CNode_45}
#   8: @_no_sens_impl_36:CNode_39{[0]: ValueNode<FuncGraph> ✗_no_sens_impl_100}
#   9: @_no_sens_impl_36:CNode_47{[0]: ValueNode<Primitive> Return, [1]: CNode_39}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: mindspore_nn_optim_momentum_Momentum_construct_103 : 0x11fc3ba18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:215/    def construct(self, gradients):/
subgraph @mindspore_nn_optim_momentum_Momentum_construct_103 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11](%para37_gradients) {
  %1(CNode_104) = S_Prim_AssignAdd[output_names: ["ref"], side_effect_mem: Bool(1), input_names: ["ref", "value"]](%para11_global_step, Tensor(shape=[1], dtype=Int32, value=[1]))
      : (<Ref[Tensor[Int32]], (1), ref_key=:global_step>, <null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:223/        self.assignadd(self.global_step, self.global_step_increase_tensor)/
  %2(CNode_105) = StopGradient(%1)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:214/    @jit/
  %3(CNode_107) = call @✗mindspore_nn_optim_momentum_Momentum_construct_106()
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:224/        if self.use_dist_optimizer:/
  %4(CNode_108) = Depend[side_effect_propagate: I64(1)](%3, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:224/        if self.use_dist_optimizer:/
  Return(%4)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:224/        if self.use_dist_optimizer:/
}
# Order:
#   1: @mindspore_nn_optim_momentum_Momentum_construct_103:gradients{[0]: ValueNode<FuncGraph> flatten_gradients_109, [1]: param_gradients}
#   2: @mindspore_nn_optim_momentum_Momentum_construct_103:gradients{[0]: ValueNode<FuncGraph> decay_weight_110, [1]: gradients}
#   3: @mindspore_nn_optim_momentum_Momentum_construct_103:gradients{[0]: ValueNode<FuncGraph> gradients_centralization_111, [1]: gradients}
#   4: @mindspore_nn_optim_momentum_Momentum_construct_103:gradients{[0]: ValueNode<FuncGraph> scale_grad_112, [1]: gradients}
#   5: @mindspore_nn_optim_momentum_Momentum_construct_103:lr{[0]: ValueNode<FuncGraph> get_lr_113}
#   6: @mindspore_nn_optim_momentum_Momentum_construct_103:CNode_104{[0]: ValueNode<DoSignaturePrimitive> S_Prim_AssignAdd, [1]: param_global_step, [2]: ValueNode<Tensor> Tensor(shape=[1], dtype=Int32, value=[1])}
#   7: @mindspore_nn_optim_momentum_Momentum_construct_103:CNode_107{[0]: ValueNode<FuncGraph> ✗mindspore_nn_optim_momentum_Momentum_construct_106}
#   8: @mindspore_nn_optim_momentum_Momentum_construct_103:CNode_114{[0]: ValueNode<Primitive> Return, [1]: CNode_108}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Identity_construct_102 : 0x11fc3b418
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:505/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Identity_construct_102(%para38_x) {
  Return(%para38_x)
      : (<null>)
      #scope: (Default/grad_reducer-Identity)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:506/        return x/
}
# Order:
#   1: @mindspore_nn_layer_basic_Identity_construct_102:CNode_115{[0]: ValueNode<Primitive> Return, [1]: param_x}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_101 : 0x11fc56218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:120/    def construct(self, data, label):/
subgraph @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_101 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11](%para39_data, %para40_label) {
  %1(out) = call @__main___LeNet5_construct_116(%para39_data)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:121/        out = self._backbone(data)/
  %2(CNode_52) = call @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_117(%1, %para40_label)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:122/        return self._loss_fn(out, label)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:122/        return self._loss_fn(out, label)/
}
# Order:
#   1: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_101:out{[0]: ValueNode<FuncGraph> __main___LeNet5_construct_116, [1]: param_data}
#   2: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_101:CNode_52{[0]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_117, [1]: out, [2]: param_label}
#   3: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_101:CNode_53{[0]: ValueNode<Primitive> Return, [1]: CNode_52}


subgraph attr:
training : 1
subgraph instance: ✗_no_sens_impl_100 : 0x11fc9ca18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @✗_no_sens_impl_100 parent: [subgraph @_no_sens_impl_36]() {
  %1(CNode_48) = call @↓_no_sens_impl_118()
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:437/        if self.return_grad:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:437/        if self.return_grad:/
}
# Order:
#   1: @✗_no_sens_impl_100:CNode_48{[0]: ValueNode<FuncGraph> ↓_no_sens_impl_118}
#   2: @✗_no_sens_impl_100:CNode_49{[0]: ValueNode<Primitive> Return, [1]: CNode_48}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: scale_grad_112 : 0x11fc50818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:471/    def scale_grad(self, gradients):/
subgraph @scale_grad_112(%para41_gradients) {
  %1(CNode_120) = call @✗scale_grad_119()
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:485/        if self.need_scale:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:485/        if self.need_scale:/
}
# Order:
#   1: @scale_grad_112:CNode_120{[0]: ValueNode<FuncGraph> ✗scale_grad_119}
#   2: @scale_grad_112:CNode_121{[0]: ValueNode<Primitive> Return, [1]: CNode_120}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: gradients_centralization_111 : 0x11fc4d618
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:452/    def gradients_centralization(self, gradients):/
subgraph @gradients_centralization_111(%para42_gradients) {
  %1(CNode_123) = call @✗gradients_centralization_122()
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:466/        if self.is_group:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:466/        if self.is_group:/
}
# Order:
#   1: @gradients_centralization_111:CNode_123{[0]: ValueNode<FuncGraph> ✗gradients_centralization_122}
#   2: @gradients_centralization_111:CNode_124{[0]: ValueNode<Primitive> Return, [1]: CNode_123}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: decay_weight_110 : 0x11fc4c418
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:429/    def decay_weight(self, gradients):/
subgraph @decay_weight_110(%para43_gradients) {
  %1(CNode_126) = call @✗decay_weight_125()
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:442/        if self.exec_weight_decay:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:442/        if self.exec_weight_decay:/
}
# Order:
#   1: @decay_weight_110:CNode_126{[0]: ValueNode<FuncGraph> ✗decay_weight_125}
#   2: @decay_weight_110:CNode_127{[0]: ValueNode<Primitive> Return, [1]: CNode_126}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: flatten_gradients_109 : 0x11fc3c018
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:410/    def flatten_gradients(self, gradients):/
subgraph @flatten_gradients_109(%para44_gradients) {
  %1(CNode_129) = call @✗flatten_gradients_128()
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:424/        if self._use_flattened_params:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:424/        if self._use_flattened_params:/
}
# Order:
#   1: @flatten_gradients_109:CNode_129{[0]: ValueNode<FuncGraph> ✗flatten_gradients_128}
#   2: @flatten_gradients_109:CNode_130{[0]: ValueNode<Primitive> Return, [1]: CNode_129}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: get_lr_113 : 0x11fc4f618
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:739/    def get_lr(self):/
subgraph @get_lr_113 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11]() {
  %1(CNode_132) = call @✗get_lr_131()
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:748/        if self.dynamic_lr:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:748/        if self.dynamic_lr:/
}
# Order:
#   1: @get_lr_113:CNode_132{[0]: ValueNode<FuncGraph> ✗get_lr_131}
#   2: @get_lr_113:CNode_133{[0]: ValueNode<Primitive> Return, [1]: CNode_132}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ✗mindspore_nn_optim_momentum_Momentum_construct_106 : 0x11fc51a18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:215/    def construct(self, gradients):/
subgraph @✗mindspore_nn_optim_momentum_Momentum_construct_106 parent: [subgraph @mindspore_nn_optim_momentum_Momentum_construct_103]() {
  %1(CNode_135) = call @2✗mindspore_nn_optim_momentum_Momentum_construct_134()
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:234/            if self.is_group_lr:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:234/            if self.is_group_lr:/
}
# Order:
#   1: @✗mindspore_nn_optim_momentum_Momentum_construct_106:CNode_135{[0]: ValueNode<FuncGraph> 2✗mindspore_nn_optim_momentum_Momentum_construct_134}
#   2: @✗mindspore_nn_optim_momentum_Momentum_construct_106:CNode_136{[0]: ValueNode<Primitive> Return, [1]: CNode_135}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_117 : 0x11fc97e18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_117(%para45_logits, %para46_labels) {
  %1(CNode_137) = S_Prim__check_is_tensor[constexpr_prim: Bool(1)]("logits", %para45_logits, "SoftmaxCrossEntropyWithLogits")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:778/        _check_is_tensor('logits', logits, self.cls_name)/
  %2(CNode_138) = S_Prim__check_is_tensor[constexpr_prim: Bool(1)]("labels", %para46_labels, "SoftmaxCrossEntropyWithLogits")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:779/        _check_is_tensor('labels', labels, self.cls_name)/
  %3(CNode_139) = MakeTuple(%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:777/    def construct(self, logits, labels):/
  %4(CNode_140) = StopGradient(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:777/    def construct(self, logits, labels):/
  %5(CNode_142) = call @✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_141()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:780/        if self.sparse:/
  %6(CNode_143) = Depend[side_effect_propagate: I64(1)](%5, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:780/        if self.sparse:/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:780/        if self.sparse:/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_117:CNode_137{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_is_tensor, [1]: ValueNode<StringImm> logits, [2]: param_logits, [3]: ValueNode<StringImm> SoftmaxCrossEntropyWithLogits}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_117:CNode_138{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_is_tensor, [1]: ValueNode<StringImm> labels, [2]: param_labels, [3]: ValueNode<StringImm> SoftmaxCrossEntropyWithLogits}
#   3: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_117:CNode_142{[0]: ValueNode<FuncGraph> ✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_141}
#   4: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_117:CNode_144{[0]: ValueNode<Primitive> Return, [1]: CNode_143}


subgraph attr:
training : 1
subgraph instance: __main___LeNet5_construct_116 : 0x11fc5c018
# In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:120/    def construct(self, x):/
subgraph @__main___LeNet5_construct_116 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11](%para47_x) {
  %1(CNode_55) = call @mindspore_nn_layer_conv_Conv2d_construct_145(%para47_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:121/        x = self.max_pool2d(self.relu(self.conv1(x)))/
  %2(CNode_57) = call @mindspore_nn_layer_activation_ReLU_construct_146(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:121/        x = self.max_pool2d(self.relu(self.conv1(x)))/
  %3(x) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_147(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:121/        x = self.max_pool2d(self.relu(self.conv1(x)))/
  %4(CNode_60) = call @mindspore_nn_layer_conv_Conv2d_construct_148(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:122/        x = self.max_pool2d(self.relu(self.conv2(x)))/
  %5(CNode_61) = call @mindspore_nn_layer_activation_ReLU_construct_146(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:122/        x = self.max_pool2d(self.relu(self.conv2(x)))/
  %6(x) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_147(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:122/        x = self.max_pool2d(self.relu(self.conv2(x)))/
  %7(x) = call @mindspore_nn_layer_basic_Flatten_construct_149(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:123/        x = self.flatten(x)/
  %8(CNode_63) = call @mindspore_nn_layer_basic_Dense_construct_150(%7)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:124/        x = self.relu(self.fc1(x))/
  %9(x) = call @mindspore_nn_layer_activation_ReLU_construct_146(%8)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:124/        x = self.relu(self.fc1(x))/
  %10(CNode_65) = call @mindspore_nn_layer_basic_Dense_construct_151(%9)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:125/        x = self.relu(self.fc2(x))/
  %11(x) = call @mindspore_nn_layer_activation_ReLU_construct_146(%10)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:125/        x = self.relu(self.fc2(x))/
  %12(x) = call @mindspore_nn_layer_basic_Dense_construct_152(%11)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:126/        x = self.fc3(x)/
  Return(%12)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:127/        return x/
}
# Order:
#   1: @__main___LeNet5_construct_116:CNode_55{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_145, [1]: param_x}
#   2: @__main___LeNet5_construct_116:CNode_57{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_146, [1]: CNode_55}
#   3: @__main___LeNet5_construct_116:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_147, [1]: CNode_57}
#   4: @__main___LeNet5_construct_116:CNode_60{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_148, [1]: x}
#   5: @__main___LeNet5_construct_116:CNode_61{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_146, [1]: CNode_60}
#   6: @__main___LeNet5_construct_116:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_147, [1]: CNode_61}
#   7: @__main___LeNet5_construct_116:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Flatten_construct_149, [1]: x}
#   8: @__main___LeNet5_construct_116:CNode_63{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_150, [1]: x}
#   9: @__main___LeNet5_construct_116:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_146, [1]: CNode_63}
#  10: @__main___LeNet5_construct_116:CNode_65{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_151, [1]: x}
#  11: @__main___LeNet5_construct_116:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_146, [1]: CNode_65}
#  12: @__main___LeNet5_construct_116:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_152, [1]: x}
#  13: @__main___LeNet5_construct_116:CNode_67{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: ↓_no_sens_impl_118 : 0x11fc9d018
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @↓_no_sens_impl_118 parent: [subgraph @_no_sens_impl_36]() {
  %1(loss) = $(_no_sens_impl_36):UnpackCall_unpack_call(@mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_101, %para36_inputs)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:433/        loss = self.network(*inputs)/
  %2(grads) = $(_no_sens_impl_36):UnpackGraph(@mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_101, %para36_inputs)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %3(CNode_42) = $(_no_sens_impl_36):MakeTuple(%para3_conv1.weight, %para4_conv2.weight, %para5_fc1.weight, %para6_fc1.bias, %para7_fc2.weight, %para8_fc2.bias, %para9_fc3.weight, %para10_fc3.bias)
      : (<Ref[Tensor[Float32]], (6, 1, 3, 3), ref_key=:conv1.weight>, <Ref[Tensor[Float32]], (16, 6, 3, 3), ref_key=:conv2.weight>, <Ref[Tensor[Float32]], (120, 400), ref_key=:fc1.weight>, <Ref[Tensor[Float32]], (120), ref_key=:fc1.bias>, <Ref[Tensor[Float32]], (84, 120), ref_key=:fc2.weight>, <Ref[Tensor[Float32]], (84), ref_key=:fc2.bias>, <Ref[Tensor[Float32]], (10, 84), ref_key=:fc3.weight>, <Ref[Tensor[Float32]], (10), ref_key=:fc3.bias>) -> (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %4(grads) = $(_no_sens_impl_36):S_Prim_grad(%2, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %5(grads) = $(_no_sens_impl_36):UnpackCall_unpack_call(%4, %para36_inputs)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %6(grads) = $(_no_sens_impl_36):call @mindspore_nn_layer_basic_Identity_construct_102(%5)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:435/        grads = self.grad_reducer(grads)/
  %7(CNode_45) = $(_no_sens_impl_36):call @mindspore_nn_optim_momentum_Momentum_construct_103(%6)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:436/        loss = F.depend(loss, self.optimizer(grads))/
  %8(loss) = $(_no_sens_impl_36):S_Prim_Depend[side_effect_propagate: I64(1)](%1, %7)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:436/        loss = F.depend(loss, self.optimizer(grads))/
  Return(%8)
      : (<null>)
      #scope: (Default)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:442/        return loss/
}
# Order:
#   1: @↓_no_sens_impl_118:CNode_50{[0]: ValueNode<Primitive> Return, [1]: loss}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ✗scale_grad_119 : 0x11fc50e18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:471/    def scale_grad(self, gradients):/
subgraph @✗scale_grad_119 parent: [subgraph @scale_grad_112]() {
  %1(CNode_154) = call @↓scale_grad_153()
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:485/        if self.need_scale:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:485/        if self.need_scale:/
}
# Order:
#   1: @✗scale_grad_119:CNode_154{[0]: ValueNode<FuncGraph> ↓scale_grad_153}
#   2: @✗scale_grad_119:CNode_155{[0]: ValueNode<Primitive> Return, [1]: CNode_154}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ✗gradients_centralization_122 : 0x11fc4dc18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:452/    def gradients_centralization(self, gradients):/
subgraph @✗gradients_centralization_122 parent: [subgraph @gradients_centralization_111]() {
  %1(CNode_157) = call @↓gradients_centralization_156()
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:466/        if self.is_group:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:466/        if self.is_group:/
}
# Order:
#   1: @✗gradients_centralization_122:CNode_157{[0]: ValueNode<FuncGraph> ↓gradients_centralization_156}
#   2: @✗gradients_centralization_122:CNode_158{[0]: ValueNode<Primitive> Return, [1]: CNode_157}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ✗decay_weight_125 : 0x11fc4ca18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:429/    def decay_weight(self, gradients):/
subgraph @✗decay_weight_125 parent: [subgraph @decay_weight_110]() {
  %1(CNode_160) = call @↓decay_weight_159()
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:442/        if self.exec_weight_decay:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:442/        if self.exec_weight_decay:/
}
# Order:
#   1: @✗decay_weight_125:CNode_160{[0]: ValueNode<FuncGraph> ↓decay_weight_159}
#   2: @✗decay_weight_125:CNode_161{[0]: ValueNode<Primitive> Return, [1]: CNode_160}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ✗flatten_gradients_128 : 0x11fc3c618
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:410/    def flatten_gradients(self, gradients):/
subgraph @✗flatten_gradients_128 parent: [subgraph @flatten_gradients_109]() {
  %1(CNode_163) = call @↓flatten_gradients_162()
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:424/        if self._use_flattened_params:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:424/        if self._use_flattened_params:/
}
# Order:
#   1: @✗flatten_gradients_128:CNode_163{[0]: ValueNode<FuncGraph> ↓flatten_gradients_162}
#   2: @✗flatten_gradients_128:CNode_164{[0]: ValueNode<Primitive> Return, [1]: CNode_163}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ✗get_lr_131 : 0x11fc4fc18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:739/    def get_lr(self):/
subgraph @✗get_lr_131 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11]() {
  %1(CNode_166) = call @↓get_lr_165()
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:748/        if self.dynamic_lr:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:748/        if self.dynamic_lr:/
}
# Order:
#   1: @✗get_lr_131:CNode_166{[0]: ValueNode<FuncGraph> ↓get_lr_165}
#   2: @✗get_lr_131:CNode_167{[0]: ValueNode<Primitive> Return, [1]: CNode_166}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: 2✗mindspore_nn_optim_momentum_Momentum_construct_134 : 0x11fc52018
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:215/    def construct(self, gradients):/
subgraph @2✗mindspore_nn_optim_momentum_Momentum_construct_134 parent: [subgraph @mindspore_nn_optim_momentum_Momentum_construct_103]() {
  %1(CNode_169) = call @↓✗mindspore_nn_optim_momentum_Momentum_construct_168()
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:238/                success = self.hyper_map_reverse(F.partial(_momentum_opt, self.opt, self.momentum, lr),/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:238/                success = self.hyper_map_reverse(F.partial(_momentum_opt, self.opt, self.momentum, lr),/
}
# Order:
#   1: @2✗mindspore_nn_optim_momentum_Momentum_construct_134:CNode_170{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Partial, [1]: ValueNode<DoSignaturePrimitive> S_Prim_momentum_opt, [2]: ValueNode<DoSignaturePrimitive> S_Prim_ApplyMomentum, [3]: param_momentum, [4]: lr}
#   2: @2✗mindspore_nn_optim_momentum_Momentum_construct_134:success{[0]: ValueNode<DoSignaturePrimitive> S_Prim_hyper_map, [1]: CNode_170, [2]: gradients, [3]: CNode_171, [4]: CNode_172, [5]: ValueNode<ValueTuple> (false, false, false, false, false, false, false, false), [6]: ValueNode<ValueTuple> (false, false, false, false, false, false, false, false)}
#   3: @2✗mindspore_nn_optim_momentum_Momentum_construct_134:CNode_169{[0]: ValueNode<FuncGraph> ↓✗mindspore_nn_optim_momentum_Momentum_construct_168}
#   4: @2✗mindspore_nn_optim_momentum_Momentum_construct_134:CNode_173{[0]: ValueNode<Primitive> Return, [1]: CNode_169}


subgraph attr:
training : 1
subgraph instance: ✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_141 : 0x11fc98418
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:777/    def construct(self, logits, labels):/
subgraph @✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_141 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_117]() {
  %1(CNode_174) = S_Prim_equal("mean", "mean")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:781/            if self.reduction == 'mean':/
  %2(CNode_175) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:781/            if self.reduction == 'mean':/
  %3(CNode_176) = Switch(%2, @2✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_177, @✗✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_178)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:781/            if self.reduction == 'mean':/
  %4(CNode_179) = %3()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:781/            if self.reduction == 'mean':/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:781/            if self.reduction == 'mean':/
}
# Order:
#   1: @✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_141:CNode_174{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: ValueNode<StringImm> mean, [2]: ValueNode<StringImm> mean}
#   2: @✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_141:CNode_175{[0]: ValueNode<Primitive> Cond, [1]: CNode_174, [2]: ValueNode<BoolImm> false}
#   3: @✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_141:CNode_176{[0]: ValueNode<Primitive> Switch, [1]: CNode_175, [2]: ValueNode<FuncGraph> 2✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_177, [3]: ValueNode<FuncGraph> ✗✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_178}
#   4: @✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_141:CNode_179{[0]: CNode_176}
#   5: @✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_141:CNode_180{[0]: ValueNode<Primitive> Return, [1]: CNode_179}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct_152 : 0x11fc97818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct_152 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11](%para48_x) {
  %1(CNode_182) = call @L_mindspore_nn_layer_basic_Dense_construct_181(%para48_x, %para10_fc3.bias, %para9_fc3.weight)
      : (<null>, <Ref[Tensor[Float32]], (10), ref_key=:fc3.bias>, <Ref[Tensor[Float32]], (10, 84), ref_key=:fc3.weight>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct_152:CNode_182{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_181, [1]: param_x, [2]: param_fc3.bias, [3]: param_fc3.weight}
#   2: @mindspore_nn_layer_basic_Dense_construct_152:CNode_81{[0]: ValueNode<Primitive> Return, [1]: CNode_182}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_activation_ReLU_construct_146 : 0x11fc97218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/activation.py:437/    def construct(self, x):/
subgraph @mindspore_nn_layer_activation_ReLU_construct_146(%para49_x) {
  %1(CNode_183) = S_Prim_ReLU[output_names: ["output"], input_names: ["x"]](%para49_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/relu-ReLU)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/activation.py:438/        return self.relu(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/relu-ReLU)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/activation.py:438/        return self.relu(x)/
}
# Order:
#   1: @mindspore_nn_layer_activation_ReLU_construct_146:CNode_183{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: param_x}
#   2: @mindspore_nn_layer_activation_ReLU_construct_146:CNode_184{[0]: ValueNode<Primitive> Return, [1]: CNode_183}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct_151 : 0x11fc96c18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct_151 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11](%para50_x) {
  %1(CNode_185) = call @L_mindspore_nn_layer_basic_Dense_construct_181(%para50_x, %para8_fc2.bias, %para7_fc2.weight)
      : (<null>, <Ref[Tensor[Float32]], (84), ref_key=:fc2.bias>, <Ref[Tensor[Float32]], (84, 120), ref_key=:fc2.weight>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc2-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct_151:CNode_185{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_181, [1]: param_x, [2]: param_fc2.bias, [3]: param_fc2.weight}
#   2: @mindspore_nn_layer_basic_Dense_construct_151:CNode_186{[0]: ValueNode<Primitive> Return, [1]: CNode_185}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct_150 : 0x11fc55a18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct_150 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11](%para51_x) {
  %1(CNode_68) = call @L_mindspore_nn_layer_basic_Dense_construct_181(%para51_x, %para6_fc1.bias, %para5_fc1.weight)
      : (<null>, <Ref[Tensor[Float32]], (120), ref_key=:fc1.bias>, <Ref[Tensor[Float32]], (120, 400), ref_key=:fc1.weight>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc1-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct_150:CNode_68{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_181, [1]: param_x, [2]: param_fc1.bias, [3]: param_fc1.weight}
#   2: @mindspore_nn_layer_basic_Dense_construct_150:CNode_69{[0]: ValueNode<Primitive> Return, [1]: CNode_68}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Flatten_construct_149 : 0x11fc6ae18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:461/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Flatten_construct_149(%para52_x) {
  %1(x_rank) = call @rank_187(%para52_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:462/        x_rank = F.rank(x)/
  %2(CNode_188) = S_Prim_not_equal(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
  %3(CNode_189) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
  %4(CNode_190) = Switch(%3, @↰mindspore_nn_layer_basic_Flatten_construct_191, @↱mindspore_nn_layer_basic_Flatten_construct_192)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
  %5(ndim) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
  %6(CNode_194) = call @check_axis_valid_193(I64(1), %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:464/        self.check_axis_valid(self.start_dim, ndim)/
  %7(CNode_195) = call @check_axis_valid_193(I64(-1), %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:465/        self.check_axis_valid(self.end_dim, ndim)/
  %8(CNode_196) = MakeTuple(%6, %7)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:461/    def construct(self, x):/
  %9(CNode_197) = StopGradient(%8)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:461/    def construct(self, x):/
  %10(CNode_198) = S_Prim_MakeTuple(%para52_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %11(CNode_199) = S_Prim_MakeTuple("start_dim", "end_dim")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %12(CNode_200) = S_Prim_MakeTuple(I64(1), I64(-1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %13(CNode_201) = S_Prim_make_dict(%11, %12)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %14(CNode_202) = UnpackCall_unpack_call(@flatten_203, %10, %13)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %15(CNode_204) = Depend[side_effect_propagate: I64(1)](%14, %9)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%15)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
}
# Order:
#   1: @mindspore_nn_layer_basic_Flatten_construct_149:x_rank{[0]: ValueNode<FuncGraph> rank_187, [1]: param_x}
#   2: @mindspore_nn_layer_basic_Flatten_construct_149:CNode_188{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: x_rank, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_basic_Flatten_construct_149:CNode_189{[0]: ValueNode<Primitive> Cond, [1]: CNode_188, [2]: ValueNode<BoolImm> false}
#   4: @mindspore_nn_layer_basic_Flatten_construct_149:CNode_190{[0]: ValueNode<Primitive> Switch, [1]: CNode_189, [2]: ValueNode<FuncGraph> ↰mindspore_nn_layer_basic_Flatten_construct_191, [3]: ValueNode<FuncGraph> ↱mindspore_nn_layer_basic_Flatten_construct_192}
#   5: @mindspore_nn_layer_basic_Flatten_construct_149:ndim{[0]: CNode_190}
#   6: @mindspore_nn_layer_basic_Flatten_construct_149:CNode_194{[0]: ValueNode<FuncGraph> check_axis_valid_193, [1]: ValueNode<Int64Imm> 1, [2]: ndim}
#   7: @mindspore_nn_layer_basic_Flatten_construct_149:CNode_195{[0]: ValueNode<FuncGraph> check_axis_valid_193, [1]: ValueNode<Int64Imm> -1, [2]: ndim}
#   8: @mindspore_nn_layer_basic_Flatten_construct_149:CNode_198{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: param_x}
#   9: @mindspore_nn_layer_basic_Flatten_construct_149:CNode_199{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<StringImm> start_dim, [2]: ValueNode<StringImm> end_dim}
#  10: @mindspore_nn_layer_basic_Flatten_construct_149:CNode_200{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<Int64Imm> 1, [2]: ValueNode<Int64Imm> -1}
#  11: @mindspore_nn_layer_basic_Flatten_construct_149:CNode_201{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_dict, [1]: CNode_199, [2]: CNode_200}
#  12: @mindspore_nn_layer_basic_Flatten_construct_149:CNode_202{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.205, [1]: ValueNode<FuncGraph> flatten_203, [2]: CNode_198, [3]: CNode_201}
#  13: @mindspore_nn_layer_basic_Flatten_construct_149:CNode_206{[0]: ValueNode<Primitive> Return, [1]: CNode_204}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_147 : 0x11fc5ee18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_147(%para53_x) {
  %1(CNode_207) = getattr(%para53_x, "ndim")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:570/        if x.ndim == 3:/
  %2(CNode_208) = S_Prim_equal(%1, I64(3))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:570/        if x.ndim == 3:/
  %3(CNode_209) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:570/        if x.ndim == 3:/
  %4(CNode_210) = Switch(%3, @✓mindspore_nn_layer_pooling_MaxPool2d_construct_211, @✗mindspore_nn_layer_pooling_MaxPool2d_construct_212)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:570/        if x.ndim == 3:/
  %5(CNode_213) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:570/        if x.ndim == 3:/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:570/        if x.ndim == 3:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_147:CNode_207{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> ndim}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_147:CNode_208{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: CNode_207, [2]: ValueNode<Int64Imm> 3}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_147:CNode_209{[0]: ValueNode<Primitive> Cond, [1]: CNode_208, [2]: ValueNode<BoolImm> false}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_147:CNode_210{[0]: ValueNode<Primitive> Switch, [1]: CNode_209, [2]: ValueNode<FuncGraph> ✓mindspore_nn_layer_pooling_MaxPool2d_construct_211, [3]: ValueNode<FuncGraph> ✗mindspore_nn_layer_pooling_MaxPool2d_construct_212}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_147:CNode_213{[0]: CNode_210}
#   6: @mindspore_nn_layer_pooling_MaxPool2d_construct_147:CNode_214{[0]: ValueNode<Primitive> Return, [1]: CNode_213}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_148 : 0x11fc5ae18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_148 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11](%para54_x) {
  %1(CNode_216) = call @✗mindspore_nn_layer_conv_Conv2d_construct_215()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv2-Conv2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv2-Conv2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_148:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_conv2.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_148:CNode_216{[0]: ValueNode<FuncGraph> ✗mindspore_nn_layer_conv_Conv2d_construct_215}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_148:CNode_217{[0]: ValueNode<Primitive> Return, [1]: CNode_216}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_145 : 0x11fc4ea18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_145 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11](%para55_x) {
  %1(CNode_219) = call @✗mindspore_nn_layer_conv_Conv2d_construct_218()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv1-Conv2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv1-Conv2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_145:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_conv1.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_145:CNode_219{[0]: ValueNode<FuncGraph> ✗mindspore_nn_layer_conv_Conv2d_construct_218}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_145:CNode_220{[0]: ValueNode<Primitive> Return, [1]: CNode_219}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ↓scale_grad_153 : 0x11fc51418
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:471/    def scale_grad(self, gradients):/
subgraph @↓scale_grad_153 parent: [subgraph @scale_grad_112]() {
  Return(%para41_gradients)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:488/        return gradients/
}
# Order:
#   1: @↓scale_grad_153:CNode_221{[0]: ValueNode<Primitive> Return, [1]: param_gradients}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ↓gradients_centralization_156 : 0x11fc4f018
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:452/    def gradients_centralization(self, gradients):/
subgraph @↓gradients_centralization_156 parent: [subgraph @gradients_centralization_111]() {
  Return(%para42_gradients)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:469/        return gradients/
}
# Order:
#   1: @↓gradients_centralization_156:CNode_222{[0]: ValueNode<Primitive> Return, [1]: param_gradients}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ↓decay_weight_159 : 0x11fc4d018
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:429/    def decay_weight(self, gradients):/
subgraph @↓decay_weight_159 parent: [subgraph @decay_weight_110]() {
  Return(%para43_gradients)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:450/        return gradients/
}
# Order:
#   1: @↓decay_weight_159:CNode_223{[0]: ValueNode<Primitive> Return, [1]: param_gradients}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ↓flatten_gradients_162 : 0x11fc3cc18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:410/    def flatten_gradients(self, gradients):/
subgraph @↓flatten_gradients_162 parent: [subgraph @flatten_gradients_109]() {
  Return(%para44_gradients)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:427/        return gradients/
}
# Order:
#   1: @↓flatten_gradients_162:CNode_224{[0]: ValueNode<Primitive> Return, [1]: param_gradients}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ↓get_lr_165 : 0x11fc50218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:739/    def get_lr(self):/
subgraph @↓get_lr_165 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11]() {
  Return(%para21_learning_rate)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:756/        return lr/
}
# Order:
#   1: @↓get_lr_165:CNode_225{[0]: ValueNode<Primitive> Return, [1]: param_learning_rate}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ↓✗mindspore_nn_optim_momentum_Momentum_construct_168 : 0x11f902218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:215/    def construct(self, gradients):/
subgraph @↓✗mindspore_nn_optim_momentum_Momentum_construct_168 parent: [subgraph @2✗mindspore_nn_optim_momentum_Momentum_construct_134]() {
  %1(CNode_227) = call @↓mindspore_nn_optim_momentum_Momentum_construct_226()
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:234/            if self.is_group_lr:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:234/            if self.is_group_lr:/
}
# Order:
#   1: @↓✗mindspore_nn_optim_momentum_Momentum_construct_168:CNode_227{[0]: ValueNode<FuncGraph> ↓mindspore_nn_optim_momentum_Momentum_construct_226}
#   2: @↓✗mindspore_nn_optim_momentum_Momentum_construct_168:CNode_228{[0]: ValueNode<Primitive> Return, [1]: CNode_227}


subgraph attr:
training : 1
subgraph instance: 2✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_177 : 0x11fc9c418
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:777/    def construct(self, logits, labels):/
subgraph @2✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_177 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_117]() {
  %1(x) = S_Prim_SparseSoftmaxCrossEntropyWithLogits[output_names: ["output"], input_names: ["features", "labels"], sens: F32(1), is_grad: Bool(0)](%para45_logits, %para46_labels)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:782/                x = self.sparse_softmax_cross_entropy(logits, labels)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:783/                return x/
}
# Order:
#   1: @2✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_177:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_SparseSoftmaxCrossEntropyWithLogits, [1]: param_logits, [2]: param_labels}
#   2: @2✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_177:CNode_229{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: ✗✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_178 : 0x11fc98a18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:777/    def construct(self, logits, labels):/
subgraph @✗✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_178 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_117]() {
  %1(CNode_231) = call @↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_230()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:781/            if self.reduction == 'mean':/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:781/            if self.reduction == 'mean':/
}
# Order:
#   1: @✗✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_178:CNode_231{[0]: ValueNode<FuncGraph> ↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_230}
#   2: @✗✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_178:CNode_232{[0]: ValueNode<Primitive> Return, [1]: CNode_231}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_181 : 0x11fc72a18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_181(%para56_x, %para57_, %para58_) {
  %1(x_shape) = S_Prim_Shape(%para56_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:623/        x_shape = self.shape_op(x)/
  %2(CNode_70) = S_Prim_check_dense_input_shape[constexpr_prim: Bool(1)](%1, "Dense")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:624/        check_dense_input_shape(x_shape, self.cls_name)/
  %3(CNode_71) = StopGradient(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
  %4(CNode_72) = S_Prim_inner_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
  %5(CNode_73) = S_Prim_not_equal(%4, I64(2))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
  %6(CNode_74) = Cond(%5, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
  %7(CNode_75) = Switch(%6, @L_✓mindspore_nn_layer_basic_Dense_construct_233, @L_✗mindspore_nn_layer_basic_Dense_construct_234)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
  %8(CNode_78) = %7()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
  %9(CNode_79) = call @L_↓mindspore_nn_layer_basic_Dense_construct_235(%8)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:126/        x = self.fc3(x)/
  %10(CNode_80) = Depend[side_effect_propagate: I64(1)](%9, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:126/        x = self.fc3(x)/
  Return(%10)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_181:x_shape{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_x}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_181:CNode_70{[0]: ValueNode<DoSignaturePrimitive> S_Prim_check_dense_input_shape, [1]: x_shape, [2]: ValueNode<StringImm> Dense}
#   3: @L_mindspore_nn_layer_basic_Dense_construct_181:CNode_72{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: x_shape}
#   4: @L_mindspore_nn_layer_basic_Dense_construct_181:CNode_73{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: CNode_72, [2]: ValueNode<Int64Imm> 2}
#   5: @L_mindspore_nn_layer_basic_Dense_construct_181:CNode_74{[0]: ValueNode<Primitive> Cond, [1]: CNode_73, [2]: ValueNode<BoolImm> false}
#   6: @L_mindspore_nn_layer_basic_Dense_construct_181:CNode_75{[0]: ValueNode<Primitive> Switch, [1]: CNode_74, [2]: ValueNode<FuncGraph> L_✓mindspore_nn_layer_basic_Dense_construct_233, [3]: ValueNode<FuncGraph> L_✗mindspore_nn_layer_basic_Dense_construct_234}
#   7: @L_mindspore_nn_layer_basic_Dense_construct_181:CNode_78{[0]: CNode_75}
#   8: @L_mindspore_nn_layer_basic_Dense_construct_181:CNode_79{[0]: ValueNode<FuncGraph> L_↓mindspore_nn_layer_basic_Dense_construct_235, [1]: CNode_78}
#   9: @L_mindspore_nn_layer_basic_Dense_construct_181:CNode_80{[0]: ValueNode<Primitive> Depend, [1]: CNode_79, [2]: CNode_71}
#  10: @L_mindspore_nn_layer_basic_Dense_construct_181:CNode_81{[0]: ValueNode<Primitive> Return, [1]: CNode_80}


subgraph attr:
training : 1
subgraph instance: check_axis_valid_193 : 0x11fc6b418
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @check_axis_valid_193(%para59_axis, %para60_ndim) {
  %1(CNode_236) = S_Prim_negative(%para60_ndim)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  %2(CNode_237) = S_Prim_less(%para59_axis, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  %3(CNode_238) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  %4(CNode_239) = Switch(%3, @↰check_axis_valid_240, @↱check_axis_valid_241)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  %5(CNode_242) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  %6(CNode_243) = Cond(%5, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  %7(CNode_244) = Switch(%6, @✓check_axis_valid_245, @✗check_axis_valid_246)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  %8(CNode_247) = %7()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_193:CNode_236{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: param_ndim}
#   2: @check_axis_valid_193:CNode_237{[0]: ValueNode<DoSignaturePrimitive> S_Prim_less, [1]: param_axis, [2]: CNode_236}
#   3: @check_axis_valid_193:CNode_238{[0]: ValueNode<Primitive> Cond, [1]: CNode_237, [2]: ValueNode<BoolImm> false}
#   4: @check_axis_valid_193:CNode_239{[0]: ValueNode<Primitive> Switch, [1]: CNode_238, [2]: ValueNode<FuncGraph> ↰check_axis_valid_240, [3]: ValueNode<FuncGraph> ↱check_axis_valid_241}
#   5: @check_axis_valid_193:CNode_242{[0]: CNode_239}
#   6: @check_axis_valid_193:CNode_243{[0]: ValueNode<Primitive> Cond, [1]: CNode_242, [2]: ValueNode<BoolImm> false}
#   7: @check_axis_valid_193:CNode_244{[0]: ValueNode<Primitive> Switch, [1]: CNode_243, [2]: ValueNode<FuncGraph> ✓check_axis_valid_245, [3]: ValueNode<FuncGraph> ✗check_axis_valid_246}
#   8: @check_axis_valid_193:CNode_247{[0]: CNode_244}
#   9: @check_axis_valid_193:CNode_248{[0]: ValueNode<Primitive> Return, [1]: CNode_247}


subgraph attr:
subgraph instance: rank_187 : 0x11fc57618
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1541/def rank(input_x):/
subgraph @rank_187(%para61_input_x) {
  %1(CNode_249) = S_Prim_Rank(%para61_input_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1571/    return rank_(input_x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1571/    return rank_(input_x)/
}
# Order:
#   1: @rank_187:CNode_249{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Rank, [1]: param_input_x}
#   2: @rank_187:CNode_250{[0]: ValueNode<Primitive> Return, [1]: CNode_249}


subgraph attr:
training : 1
subgraph instance: ↰mindspore_nn_layer_basic_Flatten_construct_191 : 0x11fc55418
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
subgraph @↰mindspore_nn_layer_basic_Flatten_construct_191 parent: [subgraph @mindspore_nn_layer_basic_Flatten_construct_149]() {
  %1(x_rank) = $(mindspore_nn_layer_basic_Flatten_construct_149):call @rank_187(%para52_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:462/        x_rank = F.rank(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
}
# Order:
#   1: @↰mindspore_nn_layer_basic_Flatten_construct_191:CNode_251{[0]: ValueNode<Primitive> Return, [1]: x_rank}


subgraph attr:
training : 1
subgraph instance: ↱mindspore_nn_layer_basic_Flatten_construct_192 : 0x11fc54e18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
subgraph @↱mindspore_nn_layer_basic_Flatten_construct_192() {
  Return(I64(1))
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
}
# Order:
#   1: @↱mindspore_nn_layer_basic_Flatten_construct_192:CNode_252{[0]: ValueNode<Primitive> Return, [1]: ValueNode<Int64Imm> 1}


subgraph attr:
subgraph instance: flatten_203 : 0x11fc4e218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_203(%para62_input, %para63_order, %para64_start_dim, %para65_end_dim) {
  %1(CNode_253) = S_Prim_isinstance(%para62_input, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1731/    if not isinstance(input, Tensor):/
  %2(CNode_254) = S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1731/    if not isinstance(input, Tensor):/
  %3(CNode_255) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1731/    if not isinstance(input, Tensor):/
  %4(CNode_256) = Switch(%3, @✓flatten_257, @✗flatten_258)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1731/    if not isinstance(input, Tensor):/
  %5(CNode_259) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1731/    if not isinstance(input, Tensor):/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1731/    if not isinstance(input, Tensor):/
}
# Order:
#   1: @flatten_203:CNode_253{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_input, [2]: ValueNode<ClassType> class 'mindspore.common.tensor.Tensor'}
#   2: @flatten_203:CNode_254{[0]: ValueNode<DoSignaturePrimitive> S_Prim_logical_not, [1]: CNode_253}
#   3: @flatten_203:CNode_255{[0]: ValueNode<Primitive> Cond, [1]: CNode_254, [2]: ValueNode<BoolImm> false}
#   4: @flatten_203:CNode_256{[0]: ValueNode<Primitive> Switch, [1]: CNode_255, [2]: ValueNode<FuncGraph> ✓flatten_257, [3]: ValueNode<FuncGraph> ✗flatten_258}
#   5: @flatten_203:CNode_259{[0]: CNode_256}
#   6: @flatten_203:CNode_260{[0]: ValueNode<Primitive> Return, [1]: CNode_259}


subgraph attr:
training : 1
subgraph instance: ✓mindspore_nn_layer_pooling_MaxPool2d_construct_211 : 0x11fc6a818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @✓mindspore_nn_layer_pooling_MaxPool2d_construct_211 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_147]() {
  %1(CNode_261) = getattr(%para53_x, "unsqueeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:571/            x = x.unsqueeze(0)/
  %2(x) = %1(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:571/            x = x.unsqueeze(0)/
  %3(CNode_263) = call @↓mindspore_nn_layer_pooling_MaxPool2d_construct_262(%2, Bool(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:122/        x = self.max_pool2d(self.relu(self.conv2(x)))/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:571/            x = x.unsqueeze(0)/
}
# Order:
#   1: @✓mindspore_nn_layer_pooling_MaxPool2d_construct_211:CNode_261{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> unsqueeze}
#   2: @✓mindspore_nn_layer_pooling_MaxPool2d_construct_211:x{[0]: CNode_261, [1]: ValueNode<Int64Imm> 0}
#   3: @✓mindspore_nn_layer_pooling_MaxPool2d_construct_211:CNode_264{[0]: ValueNode<Primitive> Return, [1]: CNode_263}
#   4: @✓mindspore_nn_layer_pooling_MaxPool2d_construct_211:CNode_263{[0]: ValueNode<FuncGraph> ↓mindspore_nn_layer_pooling_MaxPool2d_construct_262, [1]: x, [2]: ValueNode<BoolImm> true}


subgraph attr:
training : 1
subgraph instance: ✗mindspore_nn_layer_pooling_MaxPool2d_construct_212 : 0x11fc5f418
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @✗mindspore_nn_layer_pooling_MaxPool2d_construct_212 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_147]() {
  %1(CNode_265) = call @↓mindspore_nn_layer_pooling_MaxPool2d_construct_262(%para53_x, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:122/        x = self.max_pool2d(self.relu(self.conv2(x)))/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:570/        if x.ndim == 3:/
}
# Order:
#   1: @✗mindspore_nn_layer_pooling_MaxPool2d_construct_212:CNode_266{[0]: ValueNode<Primitive> Return, [1]: CNode_265}
#   2: @✗mindspore_nn_layer_pooling_MaxPool2d_construct_212:CNode_265{[0]: ValueNode<FuncGraph> ↓mindspore_nn_layer_pooling_MaxPool2d_construct_262, [1]: param_x, [2]: ValueNode<BoolImm> false}


subgraph attr:
training : 1
subgraph instance: ✗mindspore_nn_layer_conv_Conv2d_construct_215 : 0x11fc5e218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:360/    def construct(self, x):/
subgraph @✗mindspore_nn_layer_conv_Conv2d_construct_215 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_148]() {
  %1(CNode_268) = call @↓mindspore_nn_layer_conv_Conv2d_construct_267()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv2-Conv2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv2-Conv2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @✗mindspore_nn_layer_conv_Conv2d_construct_215:CNode_268{[0]: ValueNode<FuncGraph> ↓mindspore_nn_layer_conv_Conv2d_construct_267}
#   2: @✗mindspore_nn_layer_conv_Conv2d_construct_215:CNode_269{[0]: ValueNode<Primitive> Return, [1]: CNode_268}


subgraph attr:
training : 1
subgraph instance: ✗mindspore_nn_layer_conv_Conv2d_construct_218 : 0x11fc28c18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:360/    def construct(self, x):/
subgraph @✗mindspore_nn_layer_conv_Conv2d_construct_218 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_145]() {
  %1(CNode_271) = call @↓mindspore_nn_layer_conv_Conv2d_construct_270()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv1-Conv2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv1-Conv2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @✗mindspore_nn_layer_conv_Conv2d_construct_218:CNode_271{[0]: ValueNode<FuncGraph> ↓mindspore_nn_layer_conv_Conv2d_construct_270}
#   2: @✗mindspore_nn_layer_conv_Conv2d_construct_218:CNode_272{[0]: ValueNode<Primitive> Return, [1]: CNode_271}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ↓mindspore_nn_optim_momentum_Momentum_construct_226 : 0x11f902818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:215/    def construct(self, gradients):/
subgraph @↓mindspore_nn_optim_momentum_Momentum_construct_226 parent: [subgraph @2✗mindspore_nn_optim_momentum_Momentum_construct_134]() {
  %1(lr) = $(mindspore_nn_optim_momentum_Momentum_construct_103):call @get_lr_113()
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:222/        lr = self.get_lr()/
  %2(CNode_170) = $(2✗mindspore_nn_optim_momentum_Momentum_construct_134):S_Prim_Partial[side_effect_propagate: I64(1)](S_Prim_momentum_opt, S_Prim_ApplyMomentum[output_names: ["output"], side_effect_mem: Bool(1), use_nesterov: Bool(0), input_names: ["variable", "accumulation", "learning_rate", "gradient", "momentum"], use_locking: Bool(0), gradient_scale: F32(1)], %para20_momentum, %1)
      : (<null>, <null>, <Ref[Tensor[Float32]], (), ref_key=:momentum>, <null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:238/                success = self.hyper_map_reverse(F.partial(_momentum_opt, self.opt, self.momentum, lr),/
  %3(gradients) = $(mindspore_nn_optim_momentum_Momentum_construct_103):call @flatten_gradients_109(%para37_gradients)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:218/        gradients = self.flatten_gradients(gradients)/
  %4(gradients) = $(mindspore_nn_optim_momentum_Momentum_construct_103):call @decay_weight_110(%3)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:219/        gradients = self.decay_weight(gradients)/
  %5(gradients) = $(mindspore_nn_optim_momentum_Momentum_construct_103):call @gradients_centralization_111(%4)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:220/        gradients = self.gradients_centralization(gradients)/
  %6(gradients) = $(mindspore_nn_optim_momentum_Momentum_construct_103):call @scale_grad_112(%5)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:221/        gradients = self.scale_grad(gradients)/
  %7(CNode_171) = $(mindspore_nn_optim_momentum_Momentum_construct_103):MakeTuple(%para3_conv1.weight, %para4_conv2.weight, %para5_fc1.weight, %para6_fc1.bias, %para7_fc2.weight, %para8_fc2.bias, %para9_fc3.weight, %para10_fc3.bias)
      : (<Ref[Tensor[Float32]], (6, 1, 3, 3), ref_key=:conv1.weight>, <Ref[Tensor[Float32]], (16, 6, 3, 3), ref_key=:conv2.weight>, <Ref[Tensor[Float32]], (120, 400), ref_key=:fc1.weight>, <Ref[Tensor[Float32]], (120), ref_key=:fc1.bias>, <Ref[Tensor[Float32]], (84, 120), ref_key=:fc2.weight>, <Ref[Tensor[Float32]], (84), ref_key=:fc2.bias>, <Ref[Tensor[Float32]], (10, 84), ref_key=:fc3.weight>, <Ref[Tensor[Float32]], (10), ref_key=:fc3.bias>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:216/        params = self.params/
  %8(CNode_172) = $(mindspore_nn_optim_momentum_Momentum_construct_103):MakeTuple(%para12_moments.conv1.weight, %para13_moments.conv2.weight, %para14_moments.fc1.weight, %para15_moments.fc1.bias, %para16_moments.fc2.weight, %para17_moments.fc2.bias, %para18_moments.fc3.weight, %para19_moments.fc3.bias)
      : (<Ref[Tensor[Float32]], (6, 1, 3, 3), ref_key=:moments.conv1.weight>, <Ref[Tensor[Float32]], (16, 6, 3, 3), ref_key=:moments.conv2.weight>, <Ref[Tensor[Float32]], (120, 400), ref_key=:moments.fc1.weight>, <Ref[Tensor[Float32]], (120), ref_key=:moments.fc1.bias>, <Ref[Tensor[Float32]], (84, 120), ref_key=:moments.fc2.weight>, <Ref[Tensor[Float32]], (84), ref_key=:moments.fc2.bias>, <Ref[Tensor[Float32]], (10, 84), ref_key=:moments.fc3.weight>, <Ref[Tensor[Float32]], (10), ref_key=:moments.fc3.bias>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:217/        moments = self.moments/
  %9(success) = $(2✗mindspore_nn_optim_momentum_Momentum_construct_134):S_Prim_hyper_map(%2, %6, %7, %8, (Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0)), (Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0)))
      : (<null>, <null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:238/                success = self.hyper_map_reverse(F.partial(_momentum_opt, self.opt, self.momentum, lr),/
  Return(%9)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:240/        return success/
}
# Order:
#   1: @↓mindspore_nn_optim_momentum_Momentum_construct_226:CNode_273{[0]: ValueNode<Primitive> Return, [1]: success}


subgraph attr:
after_block : 1
training : 1
subgraph instance: ↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_230 : 0x11fc99018
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:777/    def construct(self, logits, labels):/
subgraph @↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_230 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_117]() {
  %1(CNode_275) = call @↓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_274()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:781/            if self.reduction == 'mean':/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:781/            if self.reduction == 'mean':/
}
# Order:
#   1: @↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_230:CNode_276{[0]: ValueNode<FuncGraph> shape_277, [1]: param_logits}
#   2: @↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_230:CNode_278{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   3: @↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_230:CNode_279{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_276, [2]: CNode_278}
#   4: @↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_230:labels{[0]: ValueNode<DoSignaturePrimitive> S_Prim_OneHot, [1]: param_labels, [2]: CNode_279, [3]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=1), [4]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=0)}
#   5: @↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_230:CNode_275{[0]: ValueNode<FuncGraph> ↓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_274}
#   6: @↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_230:CNode_280{[0]: ValueNode<Primitive> Return, [1]: CNode_275}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_↓mindspore_nn_layer_basic_Dense_construct_235 : 0x11fc73c18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_↓mindspore_nn_layer_basic_Dense_construct_235 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_181](%para66_) {
  %1(CNode_82) = call @L_✓↓mindspore_nn_layer_basic_Dense_construct_281()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:628/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:628/        if self.has_bias:/
}
# Order:
#   1: @L_↓mindspore_nn_layer_basic_Dense_construct_235:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MatMul, [1]: param_фx, [2]: param_L_fc3.weight}
#   2: @L_↓mindspore_nn_layer_basic_Dense_construct_235:CNode_82{[0]: ValueNode<FuncGraph> L_✓↓mindspore_nn_layer_basic_Dense_construct_281}
#   3: @L_↓mindspore_nn_layer_basic_Dense_construct_235:CNode_83{[0]: ValueNode<Primitive> Return, [1]: CNode_82}


subgraph attr:
training : 1
subgraph instance: L_✓mindspore_nn_layer_basic_Dense_construct_233 : 0x11fc73618
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_✓mindspore_nn_layer_basic_Dense_construct_233 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_181]() {
  %1(CNode_282) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  %2(x_shape) = $(L_mindspore_nn_layer_basic_Dense_construct_181):S_Prim_Shape(%para56_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:623/        x_shape = self.shape_op(x)/
  %3(CNode_283) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  %4(CNode_284) = S_Prim_getitem(%2, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  %5(CNode_285) = S_Prim_MakeTuple(%1, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  %6(x) = S_Prim_Reshape[output_names: ["output"], input_names: ["tensor", "shape"]](%para56_x, %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
}
# Order:
#   1: @L_✓mindspore_nn_layer_basic_Dense_construct_233:CNode_282{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @L_✓mindspore_nn_layer_basic_Dense_construct_233:CNode_283{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   3: @L_✓mindspore_nn_layer_basic_Dense_construct_233:CNode_284{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: CNode_283}
#   4: @L_✓mindspore_nn_layer_basic_Dense_construct_233:CNode_285{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_282, [2]: CNode_284}
#   5: @L_✓mindspore_nn_layer_basic_Dense_construct_233:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_x, [2]: CNode_285}
#   6: @L_✓mindspore_nn_layer_basic_Dense_construct_233:CNode_286{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: L_✗mindspore_nn_layer_basic_Dense_construct_234 : 0x11fc73018
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_✗mindspore_nn_layer_basic_Dense_construct_234 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_181]() {
  Return(%para56_x)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_✗mindspore_nn_layer_basic_Dense_construct_234:CNode_287{[0]: ValueNode<Primitive> Return, [1]: param_x}


subgraph attr:
training : 1
subgraph instance: ✓check_axis_valid_245 : 0x11fc5a018
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @✓check_axis_valid_245() {
  %1(CNode_288) = raise[side_effect_io: Bool(1)]("ValueError", "'start_dim' or 'end_dim' out of range.", "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:459/            raise ValueError("'start_dim' or 'end_dim' out of range.")/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:459/            raise ValueError("'start_dim' or 'end_dim' out of range.")/
}
# Order:
#   1: @✓check_axis_valid_245:CNode_288{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> ValueError, [2]: ValueNode<StringImm> 'start_dim' or 'end_dim' out of range., [3]: ValueNode<StringImm> None}
#   2: @✓check_axis_valid_245:CNode_289{[0]: ValueNode<Primitive> Return, [1]: CNode_288}


subgraph attr:
training : 1
subgraph instance: ✗check_axis_valid_246 : 0x11fc54218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @✗check_axis_valid_246() {
  %1(CNode_291) = call @↓check_axis_valid_290()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @✗check_axis_valid_246:CNode_291{[0]: ValueNode<FuncGraph> ↓check_axis_valid_290}
#   2: @✗check_axis_valid_246:CNode_292{[0]: ValueNode<Primitive> Return, [1]: CNode_291}


subgraph attr:
training : 1
subgraph instance: ↰check_axis_valid_240 : 0x11fc53c18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @↰check_axis_valid_240 parent: [subgraph @check_axis_valid_193]() {
  %1(CNode_236) = $(check_axis_valid_193):S_Prim_negative(%para60_ndim)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  %2(CNode_237) = $(check_axis_valid_193):S_Prim_less(%para59_axis, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @↰check_axis_valid_240:CNode_293{[0]: ValueNode<Primitive> Return, [1]: CNode_237}


subgraph attr:
training : 1
subgraph instance: ↱check_axis_valid_241 : 0x11fc53618
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @↱check_axis_valid_241 parent: [subgraph @check_axis_valid_193]() {
  %1(CNode_294) = S_Prim_greater_equal(%para59_axis, %para60_ndim)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @↱check_axis_valid_241:CNode_294{[0]: ValueNode<DoSignaturePrimitive> S_Prim_greater_equal, [1]: param_axis, [2]: param_ndim}
#   2: @↱check_axis_valid_241:CNode_295{[0]: ValueNode<Primitive> Return, [1]: CNode_294}


subgraph attr:
subgraph instance: ✓flatten_257 : 0x11fc57018
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✓flatten_257() {
  %1(CNode_296) = JoinedStr("For 'flatten', argument 'input' must be Tensor.")
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1732/        raise TypeError(f"For 'flatten', argument 'input' must be Tensor.")/
  %2(CNode_297) = raise[side_effect_io: Bool(1)]("TypeError", %1, "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1732/        raise TypeError(f"For 'flatten', argument 'input' must be Tensor.")/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1732/        raise TypeError(f"For 'flatten', argument 'input' must be Tensor.")/
}
# Order:
#   1: @✓flatten_257:CNode_296{[0]: ValueNode<Primitive> JoinedStr, [1]: ValueNode<StringImm> For 'flatten', argument 'input' must be Tensor.}
#   2: @✓flatten_257:CNode_297{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> TypeError, [2]: CNode_296, [3]: ValueNode<StringImm> None}
#   3: @✓flatten_257:CNode_298{[0]: ValueNode<Primitive> Return, [1]: CNode_297}


subgraph attr:
subgraph instance: ✗flatten_258 : 0x11fc27418
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✗flatten_258 parent: [subgraph @flatten_203]() {
  %1(CNode_300) = call @↓flatten_299()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1731/    if not isinstance(input, Tensor):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1731/    if not isinstance(input, Tensor):/
}
# Order:
#   1: @✗flatten_258:CNode_300{[0]: ValueNode<FuncGraph> ↓flatten_299}
#   2: @✗flatten_258:CNode_301{[0]: ValueNode<Primitive> Return, [1]: CNode_300}


subgraph attr:
after_block : 1
training : 1
subgraph instance: ↓mindspore_nn_layer_pooling_MaxPool2d_construct_262 : 0x11fc5fa18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @↓mindspore_nn_layer_pooling_MaxPool2d_construct_262(%para67_, %para68_) {
  %1(CNode_303) = call @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_302()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:573/        if self.use_pad:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:573/        if self.use_pad:/
}
# Order:
#   1: @↓mindspore_nn_layer_pooling_MaxPool2d_construct_262:CNode_303{[0]: ValueNode<FuncGraph> ✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_302}
#   2: @↓mindspore_nn_layer_pooling_MaxPool2d_construct_262:CNode_304{[0]: ValueNode<Primitive> Return, [1]: CNode_303}


subgraph attr:
training : 1
subgraph instance: ↓mindspore_nn_layer_conv_Conv2d_construct_267 : 0x11fc5e818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:360/    def construct(self, x):/
subgraph @↓mindspore_nn_layer_conv_Conv2d_construct_267 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_148]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_148):S_Prim_Conv2D[kernel_size: (I64(3), I64(3)), mode: I64(1), out_channel: I64(16), input_names: ["x", "w"], pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(2), format: "NCHW", pad_list: (I64(0), I64(0), I64(0), I64(0)), groups: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), group: I64(1), dilation: (I64(1), I64(1), I64(1), I64(1)), output_names: ["output"]](%para54_x, %para4_conv2.weight)
      : (<null>, <Ref[Tensor[Float32]], (16, 6, 3, 3), ref_key=:conv2.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv2-Conv2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv2-Conv2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:364/        return output/
}
# Order:
#   1: @↓mindspore_nn_layer_conv_Conv2d_construct_267:CNode_305{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: ↓mindspore_nn_layer_conv_Conv2d_construct_270 : 0x11fc5a818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:360/    def construct(self, x):/
subgraph @↓mindspore_nn_layer_conv_Conv2d_construct_270 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_145]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_145):S_Prim_Conv2D[kernel_size: (I64(3), I64(3)), mode: I64(1), out_channel: I64(6), input_names: ["x", "w"], pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(2), format: "NCHW", pad_list: (I64(0), I64(0), I64(0), I64(0)), groups: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), group: I64(1), dilation: (I64(1), I64(1), I64(1), I64(1)), output_names: ["output"]](%para55_x, %para3_conv1.weight)
      : (<null>, <Ref[Tensor[Float32]], (6, 1, 3, 3), ref_key=:conv1.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv1-Conv2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv1-Conv2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:364/        return output/
}
# Order:
#   1: @↓mindspore_nn_layer_conv_Conv2d_construct_270:CNode_306{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
subgraph instance: shape_277 : 0x11fc99618
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1484/def shape(input_x):/
subgraph @shape_277(%para69_input_x) {
  %1(CNode_307) = S_Prim_Shape(%para69_input_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1510/    return shape_(input_x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1510/    return shape_(input_x)/
}
# Order:
#   1: @shape_277:CNode_307{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_input_x}
#   2: @shape_277:CNode_308{[0]: ValueNode<Primitive> Return, [1]: CNode_307}


subgraph attr:
training : 1
subgraph instance: ↓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_274 : 0x11fc99c18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:777/    def construct(self, logits, labels):/
subgraph @↓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_274 parent: [subgraph @↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_230]() {
  %1(CNode_276) = $(↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_230):call @shape_277(%para45_logits)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:784/            labels = self.one_hot(labels, F.shape(logits)[-1], self.on_value, self.off_value)/
  %2(CNode_278) = $(↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_230):S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:784/            labels = self.one_hot(labels, F.shape(logits)[-1], self.on_value, self.off_value)/
  %3(CNode_279) = $(↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_230):S_Prim_getitem(%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:784/            labels = self.one_hot(labels, F.shape(logits)[-1], self.on_value, self.off_value)/
  %4(labels) = $(↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_230):S_Prim_OneHot[output_names: ["output"], input_names: ["indices", "depth", "on_value", "off_value"], axis: I64(-1)](%para46_labels, %3, Tensor(shape=[], dtype=Float32, value=1), Tensor(shape=[], dtype=Float32, value=0))
      : (<null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:784/            labels = self.one_hot(labels, F.shape(logits)[-1], self.on_value, self.off_value)/
  %5(CNode_309) = S_Prim_SoftmaxCrossEntropyWithLogits(%para45_logits, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:785/        x = self.softmax_cross_entropy(logits, labels)[0]/
  %6(x) = S_Prim_getitem(%5, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:785/        x = self.softmax_cross_entropy(logits, labels)[0]/
  %7(CNode_311) = call @get_loss_310(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:786/        return self.get_loss(x)/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:786/        return self.get_loss(x)/
}
# Order:
#   1: @↓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_274:CNode_309{[0]: ValueNode<DoSignaturePrimitive> S_Prim_SoftmaxCrossEntropyWithLogits, [1]: param_logits, [2]: labels}
#   2: @↓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_274:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_309, [2]: ValueNode<Int64Imm> 0}
#   3: @↓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_274:CNode_311{[0]: ValueNode<FuncGraph> get_loss_310, [1]: x}
#   4: @↓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_274:CNode_312{[0]: ValueNode<Primitive> Return, [1]: CNode_311}


subgraph attr:
training : 1
subgraph instance: L_✓↓mindspore_nn_layer_basic_Dense_construct_281 : 0x11fc93c18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_281 parent: [subgraph @L_↓mindspore_nn_layer_basic_Dense_construct_235]() {
  %1(CNode_84) = call @L_2↓mindspore_nn_layer_basic_Dense_construct_313()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:629/            x = self.bias_add(x, self.bias)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:629/            x = self.bias_add(x, self.bias)/
}
# Order:
#   1: @L_✓↓mindspore_nn_layer_basic_Dense_construct_281:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BiasAdd, [1]: x, [2]: param_L_fc3.bias}
#   2: @L_✓↓mindspore_nn_layer_basic_Dense_construct_281:CNode_84{[0]: ValueNode<FuncGraph> L_2↓mindspore_nn_layer_basic_Dense_construct_313}
#   3: @L_✓↓mindspore_nn_layer_basic_Dense_construct_281:CNode_85{[0]: ValueNode<Primitive> Return, [1]: CNode_84}


subgraph attr:
after_block : 1
training : 1
subgraph instance: ↓check_axis_valid_290 : 0x11fc54818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @↓check_axis_valid_290() {
  Return(None)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:457/    def check_axis_valid(self, axis, ndim):/
}
# Order:
#   1: @↓check_axis_valid_290:CNode_314{[0]: ValueNode<Primitive> Return, [1]: ValueNode<None> None}


subgraph attr:
after_block : 1
subgraph instance: ↓flatten_299 : 0x11fc5b418
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↓flatten_299 parent: [subgraph @flatten_203]() {
  %1(CNode_315) = S_Prim_isinstance(%para64_start_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %2(CNode_316) = S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %3(CNode_317) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %4(CNode_318) = Switch(%3, @↰↓flatten_319, @↱↓flatten_320)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %5(CNode_321) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %6(CNode_322) = Cond(%5, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %7(CNode_323) = Switch(%6, @✓↓flatten_324, @✗↓flatten_325)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %8(CNode_326) = %7()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @↓flatten_299:CNode_315{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_start_dim, [2]: ValueNode<ClassType> class 'int'}
#   2: @↓flatten_299:CNode_316{[0]: ValueNode<DoSignaturePrimitive> S_Prim_logical_not, [1]: CNode_315}
#   3: @↓flatten_299:CNode_317{[0]: ValueNode<Primitive> Cond, [1]: CNode_316, [2]: ValueNode<BoolImm> false}
#   4: @↓flatten_299:CNode_318{[0]: ValueNode<Primitive> Switch, [1]: CNode_317, [2]: ValueNode<FuncGraph> ↰↓flatten_319, [3]: ValueNode<FuncGraph> ↱↓flatten_320}
#   5: @↓flatten_299:CNode_321{[0]: CNode_318}
#   6: @↓flatten_299:CNode_322{[0]: ValueNode<Primitive> Cond, [1]: CNode_321, [2]: ValueNode<BoolImm> false}
#   7: @↓flatten_299:CNode_323{[0]: ValueNode<Primitive> Switch, [1]: CNode_322, [2]: ValueNode<FuncGraph> ✓↓flatten_324, [3]: ValueNode<FuncGraph> ✗↓flatten_325}
#   8: @↓flatten_299:CNode_326{[0]: CNode_323}
#   9: @↓flatten_299:CNode_327{[0]: ValueNode<Primitive> Return, [1]: CNode_326}


subgraph attr:
training : 1
subgraph instance: ✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_302 : 0x11fc60018
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_302 parent: [subgraph @↓mindspore_nn_layer_pooling_MaxPool2d_construct_262]() {
  %1(CNode_329) = call @2↓mindspore_nn_layer_pooling_MaxPool2d_construct_328()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:581/            out = self.max_pool(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:581/            out = self.max_pool(x)/
}
# Order:
#   1: @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_302:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MaxPool, [1]: param_фx}
#   2: @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_302:CNode_329{[0]: ValueNode<FuncGraph> 2↓mindspore_nn_layer_pooling_MaxPool2d_construct_328}
#   3: @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_302:CNode_330{[0]: ValueNode<Primitive> Return, [1]: CNode_329}


subgraph attr:
training : 1
subgraph instance: get_loss_310 : 0x11fc9a218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @get_loss_310(%para70_x, %para71_weights) {
  %1(CNode_332) = call @✓get_loss_331()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:143/        if self.reduce and self.average:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:143/        if self.reduce and self.average:/
}
# Order:
#   1: @get_loss_310:input_dtype{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> dtype}
#   2: @get_loss_310:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Cast, [1]: param_x, [2]: ValueNode<Float> Float32}
#   3: @get_loss_310:weights{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Cast, [1]: param_weights, [2]: ValueNode<Float> Float32}
#   4: @get_loss_310:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Mul, [1]: weights, [2]: x}
#   5: @get_loss_310:CNode_332{[0]: ValueNode<FuncGraph> ✓get_loss_331}
#   6: @get_loss_310:CNode_333{[0]: ValueNode<Primitive> Return, [1]: CNode_332}


subgraph attr:
training : 1
subgraph instance: L_2↓mindspore_nn_layer_basic_Dense_construct_313 : 0x11fc94218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_2↓mindspore_nn_layer_basic_Dense_construct_313 parent: [subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_281]() {
  %1(CNode_86) = call @L_✗2↓mindspore_nn_layer_basic_Dense_construct_334()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:630/        if self.activation_flag:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:630/        if self.activation_flag:/
}
# Order:
#   1: @L_2↓mindspore_nn_layer_basic_Dense_construct_313:CNode_86{[0]: ValueNode<FuncGraph> L_✗2↓mindspore_nn_layer_basic_Dense_construct_334}
#   2: @L_2↓mindspore_nn_layer_basic_Dense_construct_313:CNode_87{[0]: ValueNode<Primitive> Return, [1]: CNode_86}


subgraph attr:
subgraph instance: ✓↓flatten_324 : 0x11fc52c18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✓↓flatten_324() {
  %1(CNode_335) = JoinedStr("For 'flatten', both 'start_dim' and 'end_dim' must be int.")
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1735/        raise TypeError(f"For 'flatten', both 'start_dim' and 'end_dim' must be int.")/
  %2(CNode_336) = raise[side_effect_io: Bool(1)]("TypeError", %1, "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1735/        raise TypeError(f"For 'flatten', both 'start_dim' and 'end_dim' must be int.")/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1735/        raise TypeError(f"For 'flatten', both 'start_dim' and 'end_dim' must be int.")/
}
# Order:
#   1: @✓↓flatten_324:CNode_335{[0]: ValueNode<Primitive> JoinedStr, [1]: ValueNode<StringImm> For 'flatten', both 'start_dim' and 'end_dim' must be int.}
#   2: @✓↓flatten_324:CNode_336{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> TypeError, [2]: CNode_335, [3]: ValueNode<StringImm> None}
#   3: @✓↓flatten_324:CNode_337{[0]: ValueNode<Primitive> Return, [1]: CNode_336}


subgraph attr:
subgraph instance: ✗↓flatten_325 : 0x11fc76a18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✗↓flatten_325 parent: [subgraph @flatten_203]() {
  %1(CNode_339) = call @2↓flatten_338()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @✗↓flatten_325:CNode_339{[0]: ValueNode<FuncGraph> 2↓flatten_338}
#   2: @✗↓flatten_325:CNode_340{[0]: ValueNode<Primitive> Return, [1]: CNode_339}


subgraph attr:
subgraph instance: ↰↓flatten_319 : 0x11fc76418
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↰↓flatten_319 parent: [subgraph @↓flatten_299]() {
  %1(CNode_315) = $(↓flatten_299):S_Prim_isinstance(%para64_start_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %2(CNode_316) = $(↓flatten_299):S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @↰↓flatten_319:CNode_341{[0]: ValueNode<Primitive> Return, [1]: CNode_316}


subgraph attr:
subgraph instance: ↱↓flatten_320 : 0x11fc5ba18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↱↓flatten_320 parent: [subgraph @flatten_203]() {
  %1(CNode_342) = S_Prim_isinstance(%para65_end_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %2(CNode_343) = S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %3(CNode_344) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %4(CNode_345) = Switch(%3, @↰↱↓flatten_346, @2↱↓flatten_347)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %5(CNode_348) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @↱↓flatten_320:CNode_342{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_end_dim, [2]: ValueNode<ClassType> class 'int'}
#   2: @↱↓flatten_320:CNode_343{[0]: ValueNode<DoSignaturePrimitive> S_Prim_logical_not, [1]: CNode_342}
#   3: @↱↓flatten_320:CNode_344{[0]: ValueNode<Primitive> Cond, [1]: CNode_343, [2]: ValueNode<BoolImm> false}
#   4: @↱↓flatten_320:CNode_345{[0]: ValueNode<Primitive> Switch, [1]: CNode_344, [2]: ValueNode<FuncGraph> ↰↱↓flatten_346, [3]: ValueNode<FuncGraph> 2↱↓flatten_347}
#   5: @↱↓flatten_320:CNode_348{[0]: CNode_345}
#   6: @↱↓flatten_320:CNode_349{[0]: ValueNode<Primitive> Return, [1]: CNode_348}


subgraph attr:
training : 1
subgraph instance: 2↓mindspore_nn_layer_pooling_MaxPool2d_construct_328 : 0x11fc60618
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @2↓mindspore_nn_layer_pooling_MaxPool2d_construct_328 parent: [subgraph @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_302]() {
  %1(CNode_350) = Cond(%para68_фexpand_batch, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:582/        if expand_batch:/
  %2(CNode_351) = Switch(%1, @✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_352, @✗2↓mindspore_nn_layer_pooling_MaxPool2d_construct_353)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:582/        if expand_batch:/
  %3(CNode_354) = %2()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:582/        if expand_batch:/
  %4(CNode_356) = call @3↓mindspore_nn_layer_pooling_MaxPool2d_construct_355(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:122/        x = self.max_pool2d(self.relu(self.conv2(x)))/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:582/        if expand_batch:/
}
# Order:
#   1: @2↓mindspore_nn_layer_pooling_MaxPool2d_construct_328:CNode_350{[0]: ValueNode<Primitive> Cond, [1]: param_фexpand_batch, [2]: ValueNode<BoolImm> false}
#   2: @2↓mindspore_nn_layer_pooling_MaxPool2d_construct_328:CNode_351{[0]: ValueNode<Primitive> Switch, [1]: CNode_350, [2]: ValueNode<FuncGraph> ✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_352, [3]: ValueNode<FuncGraph> ✗2↓mindspore_nn_layer_pooling_MaxPool2d_construct_353}
#   3: @2↓mindspore_nn_layer_pooling_MaxPool2d_construct_328:CNode_354{[0]: CNode_351}
#   4: @2↓mindspore_nn_layer_pooling_MaxPool2d_construct_328:CNode_356{[0]: ValueNode<FuncGraph> 3↓mindspore_nn_layer_pooling_MaxPool2d_construct_355, [1]: CNode_354}
#   5: @2↓mindspore_nn_layer_pooling_MaxPool2d_construct_328:CNode_357{[0]: ValueNode<Primitive> Return, [1]: CNode_356}


subgraph attr:
training : 1
subgraph instance: ✓get_loss_331 : 0x11fc72218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @✓get_loss_331 parent: [subgraph @get_loss_310]() {
  %1(CNode_359) = call @↓get_loss_358()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:144/            x = self.reduce_mean(x, self.get_axis(x))/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:144/            x = self.reduce_mean(x, self.get_axis(x))/
}
# Order:
#   1: @✓get_loss_331:CNode_360{[0]: ValueNode<FuncGraph> get_axis_361, [1]: x}
#   2: @✓get_loss_331:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReduceMean, [1]: x, [2]: CNode_360}
#   3: @✓get_loss_331:CNode_359{[0]: ValueNode<FuncGraph> ↓get_loss_358}
#   4: @✓get_loss_331:CNode_362{[0]: ValueNode<Primitive> Return, [1]: CNode_359}


subgraph attr:
training : 1
subgraph instance: L_✗2↓mindspore_nn_layer_basic_Dense_construct_334 : 0x11fc94818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_✗2↓mindspore_nn_layer_basic_Dense_construct_334 parent: [subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_281]() {
  %1(CNode_88) = call @L_3↓mindspore_nn_layer_basic_Dense_construct_363()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:630/        if self.activation_flag:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:630/        if self.activation_flag:/
}
# Order:
#   1: @L_✗2↓mindspore_nn_layer_basic_Dense_construct_334:CNode_88{[0]: ValueNode<FuncGraph> L_3↓mindspore_nn_layer_basic_Dense_construct_363}
#   2: @L_✗2↓mindspore_nn_layer_basic_Dense_construct_334:CNode_89{[0]: ValueNode<Primitive> Return, [1]: CNode_88}


subgraph attr:
after_block : 1
subgraph instance: 2↓flatten_338 : 0x11fc77818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @2↓flatten_338 parent: [subgraph @flatten_203]() {
  %1(CNode_364) = S_Prim_check_flatten_order[constexpr_prim: Bool(1)](%para63_order)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1736/    check_flatten_order_const(order)/
  %2(CNode_365) = StopGradient(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
  %3(CNode_366) = S_Prim_equal(%para63_order, "F")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1737/    if order == 'F':/
  %4(CNode_367) = Cond(%3, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1737/    if order == 'F':/
  %5(CNode_368) = Switch(%4, @✓2↓flatten_369, @✗2↓flatten_370)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1737/    if order == 'F':/
  %6(CNode_371) = %5()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1737/    if order == 'F':/
  %7(CNode_372) = Depend[side_effect_propagate: I64(1)](%6, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1737/    if order == 'F':/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1737/    if order == 'F':/
}
# Order:
#   1: @2↓flatten_338:CNode_364{[0]: ValueNode<DoSignaturePrimitive> S_Prim_check_flatten_order, [1]: param_order}
#   2: @2↓flatten_338:CNode_366{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: param_order, [2]: ValueNode<StringImm> F}
#   3: @2↓flatten_338:CNode_367{[0]: ValueNode<Primitive> Cond, [1]: CNode_366, [2]: ValueNode<BoolImm> false}
#   4: @2↓flatten_338:CNode_368{[0]: ValueNode<Primitive> Switch, [1]: CNode_367, [2]: ValueNode<FuncGraph> ✓2↓flatten_369, [3]: ValueNode<FuncGraph> ✗2↓flatten_370}
#   5: @2↓flatten_338:CNode_371{[0]: CNode_368}
#   6: @2↓flatten_338:CNode_373{[0]: ValueNode<Primitive> Return, [1]: CNode_372}


subgraph attr:
subgraph instance: ↰↱↓flatten_346 : 0x11fc75e18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↰↱↓flatten_346 parent: [subgraph @↱↓flatten_320]() {
  %1(CNode_342) = $(↱↓flatten_320):S_Prim_isinstance(%para65_end_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %2(CNode_343) = $(↱↓flatten_320):S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
}
# Order:
#   1: @↰↱↓flatten_346:CNode_374{[0]: ValueNode<Primitive> Return, [1]: CNode_343}


subgraph attr:
subgraph instance: 2↱↓flatten_347 : 0x11fc74a18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @2↱↓flatten_347 parent: [subgraph @flatten_203]() {
  %1(CNode_375) = S_Prim_isinstance(%para64_start_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %2(CNode_376) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %3(CNode_377) = Switch(%2, @↰2↱↓flatten_378, @3↱↓flatten_379)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %4(CNode_380) = %3()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @2↱↓flatten_347:CNode_375{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_start_dim, [2]: ValueNode<ClassType> class 'bool'}
#   2: @2↱↓flatten_347:CNode_376{[0]: ValueNode<Primitive> Cond, [1]: CNode_375, [2]: ValueNode<BoolImm> false}
#   3: @2↱↓flatten_347:CNode_377{[0]: ValueNode<Primitive> Switch, [1]: CNode_376, [2]: ValueNode<FuncGraph> ↰2↱↓flatten_378, [3]: ValueNode<FuncGraph> 3↱↓flatten_379}
#   4: @2↱↓flatten_347:CNode_380{[0]: CNode_377}
#   5: @2↱↓flatten_347:CNode_381{[0]: ValueNode<Primitive> Return, [1]: CNode_380}


subgraph attr:
after_block : 1
training : 1
subgraph instance: 3↓mindspore_nn_layer_pooling_MaxPool2d_construct_355 : 0x11fb29818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @3↓mindspore_nn_layer_pooling_MaxPool2d_construct_355(%para72_) {
  %1(CNode_383) = call @✗3↓mindspore_nn_layer_pooling_MaxPool2d_construct_382()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:587/        if self.use_pad and not self.return_indices:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:587/        if self.use_pad and not self.return_indices:/
}
# Order:
#   1: @3↓mindspore_nn_layer_pooling_MaxPool2d_construct_355:CNode_383{[0]: ValueNode<FuncGraph> ✗3↓mindspore_nn_layer_pooling_MaxPool2d_construct_382}
#   2: @3↓mindspore_nn_layer_pooling_MaxPool2d_construct_355:CNode_384{[0]: ValueNode<Primitive> Return, [1]: CNode_383}


subgraph attr:
training : 1
subgraph instance: ✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_352 : 0x11fc62018
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_352 parent: [subgraph @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_302]() {
  %1(out) = $(✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_302):S_Prim_MaxPool[pad_mode: I64(2), output_names: ["output"], kernel_size: (I64(1), I64(1), I64(2), I64(2)), format: "NCHW", strides: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"]](%para67_фx)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_385) = S_Prim_isinstance(%1, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:583/            if isinstance(out, tuple):/
  %3(CNode_386) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:583/            if isinstance(out, tuple):/
  %4(CNode_387) = Switch(%3, @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_388, @✗✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_389)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:583/            if isinstance(out, tuple):/
  %5(CNode_390) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:583/            if isinstance(out, tuple):/
  %6(CNode_392) = call @↓✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_391(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:122/        x = self.max_pool2d(self.relu(self.conv2(x)))/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:583/            if isinstance(out, tuple):/
}
# Order:
#   1: @✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_352:CNode_385{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: out, [2]: ValueNode<ClassType> class 'tuple'}
#   2: @✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_352:CNode_386{[0]: ValueNode<Primitive> Cond, [1]: CNode_385, [2]: ValueNode<BoolImm> false}
#   3: @✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_352:CNode_387{[0]: ValueNode<Primitive> Switch, [1]: CNode_386, [2]: ValueNode<FuncGraph> 2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_388, [3]: ValueNode<FuncGraph> ✗✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_389}
#   4: @✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_352:CNode_390{[0]: CNode_387}
#   5: @✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_352:CNode_392{[0]: ValueNode<FuncGraph> ↓✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_391, [1]: CNode_390}
#   6: @✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_352:CNode_393{[0]: ValueNode<Primitive> Return, [1]: CNode_392}


subgraph attr:
training : 1
subgraph instance: ✗2↓mindspore_nn_layer_pooling_MaxPool2d_construct_353 : 0x11fc61a18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @✗2↓mindspore_nn_layer_pooling_MaxPool2d_construct_353 parent: [subgraph @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_302]() {
  %1(out) = $(✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_302):S_Prim_MaxPool[pad_mode: I64(2), output_names: ["output"], kernel_size: (I64(1), I64(1), I64(2), I64(2)), format: "NCHW", strides: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"]](%para67_фx)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:581/            out = self.max_pool(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:582/        if expand_batch:/
}
# Order:
#   1: @✗2↓mindspore_nn_layer_pooling_MaxPool2d_construct_353:CNode_394{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: get_axis_361 : 0x11fc9a818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:113/    def get_axis(self, x):/
subgraph @get_axis_361(%para73_x) {
  %1(shape) = call @shape_277(%para73_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:120/        shape = F.shape(x)/
  %2(length) = S_Prim_sequence_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:121/        length = F.tuple_len(shape)/
  %3(perm) = S_Prim_make_range(I64(0), %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:122/        perm = F.make_range(0, length)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:123/        return perm/
}
# Order:
#   1: @get_axis_361:shape{[0]: ValueNode<FuncGraph> shape_277, [1]: param_x}
#   2: @get_axis_361:length{[0]: ValueNode<DoSignaturePrimitive> S_Prim_sequence_len, [1]: shape}
#   3: @get_axis_361:perm{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_range, [1]: ValueNode<Int64Imm> 0, [2]: length}
#   4: @get_axis_361:CNode_395{[0]: ValueNode<Primitive> Return, [1]: perm}


subgraph attr:
training : 1
subgraph instance: ↓get_loss_358 : 0x11fc9ae18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @↓get_loss_358 parent: [subgraph @✓get_loss_331]() {
  %1(CNode_397) = call @✗↓get_loss_396()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:145/        if self.reduce and not self.average:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:145/        if self.reduce and not self.average:/
}
# Order:
#   1: @↓get_loss_358:CNode_397{[0]: ValueNode<FuncGraph> ✗↓get_loss_396}
#   2: @↓get_loss_358:CNode_398{[0]: ValueNode<Primitive> Return, [1]: CNode_397}


subgraph attr:
training : 1
subgraph instance: L_3↓mindspore_nn_layer_basic_Dense_construct_363 : 0x11fc94e18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_3↓mindspore_nn_layer_basic_Dense_construct_363 parent: [subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_281]() {
  %1(x_shape) = $(L_mindspore_nn_layer_basic_Dense_construct_181):S_Prim_Shape(%para56_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:623/        x_shape = self.shape_op(x)/
  %2(CNode_90) = S_Prim_inner_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
  %3(CNode_91) = S_Prim_not_equal(%2, I64(2))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
  %4(CNode_92) = Cond(%3, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
  %5(CNode_93) = Switch(%4, @L_✓3↓mindspore_nn_layer_basic_Dense_construct_399, @L_✗3↓mindspore_nn_layer_basic_Dense_construct_400)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
  %6(CNode_95) = %5()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
  %7(CNode_97) = call @L_4↓mindspore_nn_layer_basic_Dense_construct_401(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /Users/qi7876/projects/GC2/task02/task02-source-code/lenet/lenet.py:126/        x = self.fc3(x)/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_3↓mindspore_nn_layer_basic_Dense_construct_363:CNode_90{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: x_shape}
#   2: @L_3↓mindspore_nn_layer_basic_Dense_construct_363:CNode_91{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: CNode_90, [2]: ValueNode<Int64Imm> 2}
#   3: @L_3↓mindspore_nn_layer_basic_Dense_construct_363:CNode_92{[0]: ValueNode<Primitive> Cond, [1]: CNode_91, [2]: ValueNode<BoolImm> false}
#   4: @L_3↓mindspore_nn_layer_basic_Dense_construct_363:CNode_93{[0]: ValueNode<Primitive> Switch, [1]: CNode_92, [2]: ValueNode<FuncGraph> L_✓3↓mindspore_nn_layer_basic_Dense_construct_399, [3]: ValueNode<FuncGraph> L_✗3↓mindspore_nn_layer_basic_Dense_construct_400}
#   5: @L_3↓mindspore_nn_layer_basic_Dense_construct_363:CNode_95{[0]: CNode_93}
#   6: @L_3↓mindspore_nn_layer_basic_Dense_construct_363:CNode_97{[0]: ValueNode<FuncGraph> L_4↓mindspore_nn_layer_basic_Dense_construct_401, [1]: CNode_95}
#   7: @L_3↓mindspore_nn_layer_basic_Dense_construct_363:CNode_98{[0]: ValueNode<Primitive> Return, [1]: CNode_97}


subgraph attr:
subgraph instance: ✓2↓flatten_369 : 0x11fc82a18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✓2↓flatten_369 parent: [subgraph @flatten_203]() {
  %1(x_rank) = S_Prim_Rank(%para62_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1738/        x_rank = rank_(input)/
  %2(CNode_402) = S_Prim_MakeTuple(I64(0), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1740/        if x_rank in (0, 1):/
  %3(CNode_403) = S_Prim_in(%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1740/        if x_rank in (0, 1):/
  %4(CNode_404) = Cond(%3, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1740/        if x_rank in (0, 1):/
  %5(CNode_405) = Switch(%4, @2✓2↓flatten_406, @✗✓2↓flatten_407)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1740/        if x_rank in (0, 1):/
  %6(CNode_408) = %5()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1740/        if x_rank in (0, 1):/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1740/        if x_rank in (0, 1):/
}
# Order:
#   1: @✓2↓flatten_369:x_rank{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Rank, [1]: param_input}
#   2: @✓2↓flatten_369:CNode_402{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<Int64Imm> 0, [2]: ValueNode<Int64Imm> 1}
#   3: @✓2↓flatten_369:CNode_403{[0]: ValueNode<DoSignaturePrimitive> S_Prim_in, [1]: x_rank, [2]: CNode_402}
#   4: @✓2↓flatten_369:CNode_404{[0]: ValueNode<Primitive> Cond, [1]: CNode_403, [2]: ValueNode<BoolImm> false}
#   5: @✓2↓flatten_369:CNode_405{[0]: ValueNode<Primitive> Switch, [1]: CNode_404, [2]: ValueNode<FuncGraph> 2✓2↓flatten_406, [3]: ValueNode<FuncGraph> ✗✓2↓flatten_407}
#   6: @✓2↓flatten_369:CNode_408{[0]: CNode_405}
#   7: @✓2↓flatten_369:CNode_409{[0]: ValueNode<Primitive> Return, [1]: CNode_408}


subgraph attr:
subgraph instance: ✗2↓flatten_370 : 0x11fc78618
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✗2↓flatten_370 parent: [subgraph @flatten_203]() {
  %1(CNode_411) = call @3↓flatten_410(%para62_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1737/    if order == 'F':/
}
# Order:
#   1: @✗2↓flatten_370:CNode_412{[0]: ValueNode<Primitive> Return, [1]: CNode_411}
#   2: @✗2↓flatten_370:CNode_411{[0]: ValueNode<FuncGraph> 3↓flatten_410, [1]: param_input}


subgraph attr:
subgraph instance: ↰2↱↓flatten_378 : 0x11fc75818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↰2↱↓flatten_378 parent: [subgraph @2↱↓flatten_347]() {
  %1(CNode_375) = $(2↱↓flatten_347):S_Prim_isinstance(%para64_start_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
}
# Order:
#   1: @↰2↱↓flatten_378:CNode_413{[0]: ValueNode<Primitive> Return, [1]: CNode_375}


subgraph attr:
subgraph instance: 3↱↓flatten_379 : 0x11fc75218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @3↱↓flatten_379 parent: [subgraph @flatten_203]() {
  %1(CNode_414) = S_Prim_isinstance(%para65_end_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @3↱↓flatten_379:CNode_414{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_end_dim, [2]: ValueNode<ClassType> class 'bool'}
#   2: @3↱↓flatten_379:CNode_415{[0]: ValueNode<Primitive> Return, [1]: CNode_414}


subgraph attr:
training : 1
subgraph instance: ✗3↓mindspore_nn_layer_pooling_MaxPool2d_construct_382 : 0x11fc69c18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @✗3↓mindspore_nn_layer_pooling_MaxPool2d_construct_382 parent: [subgraph @3↓mindspore_nn_layer_pooling_MaxPool2d_construct_355]() {
  %1(CNode_417) = call @4↓mindspore_nn_layer_pooling_MaxPool2d_construct_416()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:587/        if self.use_pad and not self.return_indices:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:587/        if self.use_pad and not self.return_indices:/
}
# Order:
#   1: @✗3↓mindspore_nn_layer_pooling_MaxPool2d_construct_382:CNode_417{[0]: ValueNode<FuncGraph> 4↓mindspore_nn_layer_pooling_MaxPool2d_construct_416}
#   2: @✗3↓mindspore_nn_layer_pooling_MaxPool2d_construct_382:CNode_418{[0]: ValueNode<Primitive> Return, [1]: CNode_417}


subgraph attr:
after_block : 1
training : 1
subgraph instance: ↓✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_391 : 0x11fc63218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @↓✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_391(%para74_) {
  Return(%para74_фout)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:583/            if isinstance(out, tuple):/
}
# Order:
#   1: @↓✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_391:CNode_419{[0]: ValueNode<Primitive> Return, [1]: param_фout}


subgraph attr:
training : 1
subgraph instance: 2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_388 : 0x11fc62c18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_388 parent: [subgraph @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_302]() {
  %1(out) = $(✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_302):S_Prim_MaxPool[pad_mode: I64(2), output_names: ["output"], kernel_size: (I64(1), I64(1), I64(2), I64(2)), format: "NCHW", strides: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"]](%para67_фx)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_420) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %3(CNode_421) = getattr(%2, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %4(CNode_422) = %3(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %5(CNode_423) = S_Prim_getitem(%1, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %6(CNode_424) = getattr(%5, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %7(CNode_425) = %6(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %8(out) = S_Prim_MakeTuple(%4, %7)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
}
# Order:
#   1: @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_388:CNode_420{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: out, [2]: ValueNode<Int64Imm> 0}
#   2: @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_388:CNode_421{[0]: ValueNode<Primitive> getattr, [1]: CNode_420, [2]: ValueNode<StringImm> squeeze}
#   3: @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_388:CNode_422{[0]: CNode_421, [1]: ValueNode<Int64Imm> 0}
#   4: @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_388:CNode_423{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: out, [2]: ValueNode<Int64Imm> 1}
#   5: @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_388:CNode_424{[0]: ValueNode<Primitive> getattr, [1]: CNode_423, [2]: ValueNode<StringImm> squeeze}
#   6: @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_388:CNode_425{[0]: CNode_424, [1]: ValueNode<Int64Imm> 0}
#   7: @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_388:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_422, [2]: CNode_425}
#   8: @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_388:CNode_426{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: ✗✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_389 : 0x11fc62618
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @✗✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_389 parent: [subgraph @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_302]() {
  %1(out) = $(✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_302):S_Prim_MaxPool[pad_mode: I64(2), output_names: ["output"], kernel_size: (I64(1), I64(1), I64(2), I64(2)), format: "NCHW", strides: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"]](%para67_фx)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_427) = getattr(%1, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:586/                out = out.squeeze(0)/
  %3(out) = %2(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:586/                out = out.squeeze(0)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:586/                out = out.squeeze(0)/
}
# Order:
#   1: @✗✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_389:CNode_427{[0]: ValueNode<Primitive> getattr, [1]: out, [2]: ValueNode<StringImm> squeeze}
#   2: @✗✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_389:out{[0]: CNode_427, [1]: ValueNode<Int64Imm> 0}
#   3: @✗✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_389:CNode_428{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: ✗↓get_loss_396 : 0x11fc9b818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @✗↓get_loss_396 parent: [subgraph @✓get_loss_331]() {
  %1(CNode_430) = call @2↓get_loss_429()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:145/        if self.reduce and not self.average:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:145/        if self.reduce and not self.average:/
}
# Order:
#   1: @✗↓get_loss_396:CNode_430{[0]: ValueNode<FuncGraph> 2↓get_loss_429}
#   2: @✗↓get_loss_396:CNode_431{[0]: ValueNode<Primitive> Return, [1]: CNode_430}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_4↓mindspore_nn_layer_basic_Dense_construct_401 : 0x11fc96618
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_4↓mindspore_nn_layer_basic_Dense_construct_401(%para75_) {
  Return(%para75_фx)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:635/        return x/
}
# Order:
#   1: @L_4↓mindspore_nn_layer_basic_Dense_construct_401:CNode_432{[0]: ValueNode<Primitive> Return, [1]: param_фx}


subgraph attr:
training : 1
subgraph instance: L_✓3↓mindspore_nn_layer_basic_Dense_construct_399 : 0x11fc95a18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_✓3↓mindspore_nn_layer_basic_Dense_construct_399 parent: [subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_281]() {
  %1(x) = $(L_↓mindspore_nn_layer_basic_Dense_construct_235):S_Prim_MatMul[output_names: ["output"], transpose_a: Bool(0), input_names: ["x1", "x2"], transpose_x2: Bool(1), transpose_x1: Bool(0), transpose_b: Bool(1)](%para66_фx, %para58_L_fc3.weight)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:627/        x = self.matmul(x, self.weight)/
  %2(x) = $(L_✓↓mindspore_nn_layer_basic_Dense_construct_281):S_Prim_BiasAdd[output_names: ["output"], format: "NCHW", input_names: ["x", "b"]](%1, %para57_L_fc3.bias)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:629/            x = self.bias_add(x, self.bias)/
  %3(x_shape) = $(L_mindspore_nn_layer_basic_Dense_construct_181):S_Prim_Shape(%para56_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:623/        x_shape = self.shape_op(x)/
  %4(CNode_433) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %5(CNode_434) = S_Prim_make_slice(None, %4, None)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %6(CNode_435) = S_Prim_getitem(%3, %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %7(CNode_437) = call @L_shape_436(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %8(CNode_438) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %9(CNode_439) = S_Prim_getitem(%7, %8)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %10(CNode_440) = S_Prim_MakeTuple(%9)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %11(out_shape) = S_Prim_add(%6, %10)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %12(x) = S_Prim_Reshape[output_names: ["output"], input_names: ["tensor", "shape"]](%2, %11)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:634/            x = self.reshape(x, out_shape)/
  Return(%12)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
}
# Order:
#   1: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_399:CNode_433{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_399:CNode_434{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_slice, [1]: ValueNode<None> None, [2]: CNode_433, [3]: ValueNode<None> None}
#   3: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_399:CNode_435{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: CNode_434}
#   4: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_399:CNode_437{[0]: ValueNode<FuncGraph> L_shape_436, [1]: x}
#   5: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_399:CNode_438{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   6: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_399:CNode_439{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_437, [2]: CNode_438}
#   7: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_399:CNode_440{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_439}
#   8: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_399:out_shape{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: CNode_435, [2]: CNode_440}
#   9: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_399:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: x, [2]: out_shape}
#  10: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_399:CNode_441{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: L_✗3↓mindspore_nn_layer_basic_Dense_construct_400 : 0x11fc95418
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_✗3↓mindspore_nn_layer_basic_Dense_construct_400 parent: [subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_281]() {
  %1(x) = $(L_↓mindspore_nn_layer_basic_Dense_construct_235):S_Prim_MatMul[output_names: ["output"], transpose_a: Bool(0), input_names: ["x1", "x2"], transpose_x2: Bool(1), transpose_x1: Bool(0), transpose_b: Bool(1)](%para66_фx, %para58_L_fc3.weight)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:627/        x = self.matmul(x, self.weight)/
  %2(x) = $(L_✓↓mindspore_nn_layer_basic_Dense_construct_281):S_Prim_BiasAdd[output_names: ["output"], format: "NCHW", input_names: ["x", "b"]](%1, %para57_L_fc3.bias)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:629/            x = self.bias_add(x, self.bias)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_✗3↓mindspore_nn_layer_basic_Dense_construct_400:CNode_99{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
subgraph instance: 2✓2↓flatten_406 : 0x11fc52618
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @2✓2↓flatten_406 parent: [subgraph @flatten_203]() {
  %1(CNode_442) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1741/            return reshape_(input, (-1,))/
  %2(CNode_443) = S_Prim_MakeTuple(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1741/            return reshape_(input, (-1,))/
  %3(CNode_444) = S_Prim_Reshape[output_names: ["output"], input_names: ["tensor", "shape"]](%para62_input, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1741/            return reshape_(input, (-1,))/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1741/            return reshape_(input, (-1,))/
}
# Order:
#   1: @2✓2↓flatten_406:CNode_442{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @2✓2↓flatten_406:CNode_443{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_442}
#   3: @2✓2↓flatten_406:CNode_444{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_input, [2]: CNode_443}
#   4: @2✓2↓flatten_406:CNode_445{[0]: ValueNode<Primitive> Return, [1]: CNode_444}


subgraph attr:
subgraph instance: ✗✓2↓flatten_407 : 0x11fc83818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✗✓2↓flatten_407 parent: [subgraph @✓2↓flatten_369]() {
  %1(CNode_447) = call @↓✓2↓flatten_446()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1740/        if x_rank in (0, 1):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1740/        if x_rank in (0, 1):/
}
# Order:
#   1: @✗✓2↓flatten_407:CNode_447{[0]: ValueNode<FuncGraph> ↓✓2↓flatten_446}
#   2: @✗✓2↓flatten_407:CNode_448{[0]: ValueNode<Primitive> Return, [1]: CNode_447}


subgraph attr:
after_block : 1
subgraph instance: 3↓flatten_410 : 0x11fc79418
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @3↓flatten_410 parent: [subgraph @flatten_203](%para76_) {
  %1(CNode_449) = S_Prim_equal(%para64_start_dim, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %2(CNode_450) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %3(CNode_451) = Switch(%2, @↰3↓flatten_452, @↱3↓flatten_453)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %4(CNode_454) = %3()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %5(CNode_455) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %6(CNode_456) = Switch(%5, @✓3↓flatten_457, @✗3↓flatten_458)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %7(CNode_459) = %6()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
}
# Order:
#   1: @3↓flatten_410:x_shape{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_фinput}
#   2: @3↓flatten_410:x_rank{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Rank, [1]: param_фinput}
#   3: @3↓flatten_410:CNode_449{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: param_start_dim, [2]: ValueNode<Int64Imm> 1}
#   4: @3↓flatten_410:CNode_450{[0]: ValueNode<Primitive> Cond, [1]: CNode_449, [2]: ValueNode<BoolImm> false}
#   5: @3↓flatten_410:CNode_451{[0]: ValueNode<Primitive> Switch, [1]: CNode_450, [2]: ValueNode<FuncGraph> ↰3↓flatten_452, [3]: ValueNode<FuncGraph> ↱3↓flatten_453}
#   6: @3↓flatten_410:CNode_454{[0]: CNode_451}
#   7: @3↓flatten_410:CNode_455{[0]: ValueNode<Primitive> Cond, [1]: CNode_454, [2]: ValueNode<BoolImm> false}
#   8: @3↓flatten_410:CNode_456{[0]: ValueNode<Primitive> Switch, [1]: CNode_455, [2]: ValueNode<FuncGraph> ✓3↓flatten_457, [3]: ValueNode<FuncGraph> ✗3↓flatten_458}
#   9: @3↓flatten_410:CNode_459{[0]: CNode_456}
#  10: @3↓flatten_410:CNode_460{[0]: ValueNode<Primitive> Return, [1]: CNode_459}


subgraph attr:
training : 1
subgraph instance: 4↓mindspore_nn_layer_pooling_MaxPool2d_construct_416 : 0x11fc6a218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @4↓mindspore_nn_layer_pooling_MaxPool2d_construct_416 parent: [subgraph @3↓mindspore_nn_layer_pooling_MaxPool2d_construct_355]() {
  Return(%para72_фout)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:589/        return out/
}
# Order:
#   1: @4↓mindspore_nn_layer_pooling_MaxPool2d_construct_416:CNode_461{[0]: ValueNode<Primitive> Return, [1]: param_фout}


subgraph attr:
training : 1
subgraph instance: 2↓get_loss_429 : 0x11fc9be18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @2↓get_loss_429 parent: [subgraph @✓get_loss_331]() {
  %1(weights) = $(get_loss_310):S_Prim_Cast[output_names: ["output"], input_names: ["x", "dst_type"]](%para71_weights, F32)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:141/        weights = self.cast(weights, mstype.float32)/
  %2(x) = $(get_loss_310):S_Prim_Cast[output_names: ["output"], input_names: ["x", "dst_type"]](%para70_x, F32)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:140/        x = self.cast(x, mstype.float32)/
  %3(x) = $(get_loss_310):S_Prim_Mul[output_names: ["output"], input_names: ["x", "y"]](%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:142/        x = self.mul(weights, x)/
  %4(CNode_360) = $(✓get_loss_331):call @get_axis_361(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:144/            x = self.reduce_mean(x, self.get_axis(x))/
  %5(x) = $(✓get_loss_331):S_Prim_ReduceMean[output_names: ["y"], keep_dims: Bool(0), input_names: ["input_x", "axis"]](%3, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:144/            x = self.reduce_mean(x, self.get_axis(x))/
  %6(input_dtype) = $(get_loss_310):getattr(%para70_x, "dtype")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:139/        input_dtype = x.dtype/
  %7(x) = S_Prim_Cast[output_names: ["output"], input_names: ["x", "dst_type"]](%5, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:147/        x = self.cast(x, input_dtype)/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:148/        return x/
}
# Order:
#   1: @2↓get_loss_429:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Cast, [1]: x, [2]: input_dtype}
#   2: @2↓get_loss_429:CNode_462{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
subgraph instance: L_shape_436 : 0x11fc96018
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1484/def shape(input_x):/
subgraph @L_shape_436(%para77_input_x) {
  %1(CNode_307) = S_Prim_Shape(%para77_input_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1510/    return shape_(input_x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1510/    return shape_(input_x)/
}
# Order:
#   1: @L_shape_436:CNode_307{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_input_x}
#   2: @L_shape_436:CNode_308{[0]: ValueNode<Primitive> Return, [1]: CNode_307}


subgraph attr:
after_block : 1
subgraph instance: ↓✓2↓flatten_446 : 0x11fc61418
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↓✓2↓flatten_446 parent: [subgraph @✓2↓flatten_369]() {
  %1(CNode_464) = call @_get_cache_prim_463(ClassType)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1744/        input = _get_cache_prim(P.Transpose)()(input, new_order)/
  %2(CNode_465) = %1()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1744/        input = _get_cache_prim(P.Transpose)()(input, new_order)/
  %3(x_rank) = $(✓2↓flatten_369):S_Prim_Rank(%para62_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1738/        x_rank = rank_(input)/
  %4(perm) = S_Prim_make_range(I64(0), %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1742/        perm = ops.make_range(0, x_rank)/
  %5(new_order) = S_Prim_tuple_reversed(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1743/        new_order = ops.tuple_reversed(perm)/
  %6(input) = %2(%para62_input, %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1744/        input = _get_cache_prim(P.Transpose)()(input, new_order)/
  %7(CNode_466) = call @3↓flatten_410(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1738/        x_rank = rank_(input)/
}
# Order:
#   1: @↓✓2↓flatten_446:perm{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_range, [1]: ValueNode<Int64Imm> 0, [2]: x_rank}
#   2: @↓✓2↓flatten_446:new_order{[0]: ValueNode<DoSignaturePrimitive> S_Prim_tuple_reversed, [1]: perm}
#   3: @↓✓2↓flatten_446:CNode_464{[0]: ValueNode<FuncGraph> _get_cache_prim_463, [1]: ValueNode<ClassType> class 'mindspore.ops.operations.array_ops.Transpose'}
#   4: @↓✓2↓flatten_446:CNode_465{[0]: CNode_464}
#   5: @↓✓2↓flatten_446:input{[0]: CNode_465, [1]: param_input, [2]: new_order}
#   6: @↓✓2↓flatten_446:CNode_467{[0]: ValueNode<Primitive> Return, [1]: CNode_466}
#   7: @↓✓2↓flatten_446:CNode_466{[0]: ValueNode<FuncGraph> 3↓flatten_410, [1]: input}


subgraph attr:
subgraph instance: ✓3↓flatten_457 : 0x11fc68c18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✓3↓flatten_457 parent: [subgraph @3↓flatten_410]() {
  %1(x_rank) = $(3↓flatten_410):S_Prim_Rank(%para76_фinput)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1748/    x_rank = rank_(input)/
  %2(CNode_468) = S_Prim_MakeTuple(I64(0), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1750/        if x_rank in (0, 1):/
  %3(CNode_469) = S_Prim_in(%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1750/        if x_rank in (0, 1):/
  %4(CNode_470) = Cond(%3, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1750/        if x_rank in (0, 1):/
  %5(CNode_471) = Switch(%4, @2✓3↓flatten_472, @✗✓3↓flatten_473)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1750/        if x_rank in (0, 1):/
  %6(CNode_474) = %5()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1750/        if x_rank in (0, 1):/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1750/        if x_rank in (0, 1):/
}
# Order:
#   1: @✓3↓flatten_457:CNode_468{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<Int64Imm> 0, [2]: ValueNode<Int64Imm> 1}
#   2: @✓3↓flatten_457:CNode_469{[0]: ValueNode<DoSignaturePrimitive> S_Prim_in, [1]: x_rank, [2]: CNode_468}
#   3: @✓3↓flatten_457:CNode_470{[0]: ValueNode<Primitive> Cond, [1]: CNode_469, [2]: ValueNode<BoolImm> false}
#   4: @✓3↓flatten_457:CNode_471{[0]: ValueNode<Primitive> Switch, [1]: CNode_470, [2]: ValueNode<FuncGraph> 2✓3↓flatten_472, [3]: ValueNode<FuncGraph> ✗✓3↓flatten_473}
#   5: @✓3↓flatten_457:CNode_474{[0]: CNode_471}
#   6: @✓3↓flatten_457:CNode_475{[0]: ValueNode<Primitive> Return, [1]: CNode_474}


subgraph attr:
subgraph instance: ✗3↓flatten_458 : 0x11fc7ae18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✗3↓flatten_458 parent: [subgraph @3↓flatten_410]() {
  %1(CNode_477) = call @4↓flatten_476()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
}
# Order:
#   1: @✗3↓flatten_458:CNode_477{[0]: ValueNode<FuncGraph> 4↓flatten_476}
#   2: @✗3↓flatten_458:CNode_478{[0]: ValueNode<Primitive> Return, [1]: CNode_477}


subgraph attr:
subgraph instance: ↰3↓flatten_452 : 0x11fc7a818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↰3↓flatten_452 parent: [subgraph @flatten_203]() {
  %1(CNode_479) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %2(CNode_480) = S_Prim_equal(%para65_end_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
}
# Order:
#   1: @↰3↓flatten_452:CNode_479{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @↰3↓flatten_452:CNode_480{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: param_end_dim, [2]: CNode_479}
#   3: @↰3↓flatten_452:CNode_481{[0]: ValueNode<Primitive> Return, [1]: CNode_480}


subgraph attr:
subgraph instance: ↱3↓flatten_453 : 0x11fc7a218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↱3↓flatten_453 parent: [subgraph @3↓flatten_410]() {
  %1(CNode_449) = $(3↓flatten_410):S_Prim_equal(%para64_start_dim, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
}
# Order:
#   1: @↱3↓flatten_453:CNode_482{[0]: ValueNode<Primitive> Return, [1]: CNode_449}


subgraph attr:
subgraph instance: _get_cache_prim_463 : 0x11fc81218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/_primitive_cache.py:35/def _get_cache_prim(cls: Primitive) -> Primitive:/
subgraph @_get_cache_prim_463(%para78_cls) {
  %1(CNode_484) = call @✓_get_cache_prim_483()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/_primitive_cache.py:88/    if _is_need_compile(_temp_func): # @jit.cond: True/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/_primitive_cache.py:88/    if _is_need_compile(_temp_func): # @jit.cond: True/
}
# Order:
#   1: @_get_cache_prim_463:CNode_484{[0]: ValueNode<FuncGraph> ✓_get_cache_prim_483}
#   2: @_get_cache_prim_463:CNode_485{[0]: ValueNode<Primitive> Return, [1]: CNode_484}


subgraph attr:
subgraph instance: 2✓3↓flatten_472 : 0x11fc82418
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @2✓3↓flatten_472 parent: [subgraph @3↓flatten_410]() {
  %1(CNode_486) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1751/            return reshape_(input, (-1,))/
  %2(CNode_487) = S_Prim_MakeTuple(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1751/            return reshape_(input, (-1,))/
  %3(CNode_488) = S_Prim_Reshape[output_names: ["output"], input_names: ["tensor", "shape"]](%para76_фinput, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1751/            return reshape_(input, (-1,))/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1751/            return reshape_(input, (-1,))/
}
# Order:
#   1: @2✓3↓flatten_472:CNode_486{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @2✓3↓flatten_472:CNode_487{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_486}
#   3: @2✓3↓flatten_472:CNode_488{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_фinput, [2]: CNode_487}
#   4: @2✓3↓flatten_472:CNode_489{[0]: ValueNode<Primitive> Return, [1]: CNode_488}


subgraph attr:
subgraph instance: ✗✓3↓flatten_473 : 0x11fc69218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✗✓3↓flatten_473 parent: [subgraph @3↓flatten_410]() {
  %1(CNode_491) = call @↓✓3↓flatten_490()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1750/        if x_rank in (0, 1):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1750/        if x_rank in (0, 1):/
}
# Order:
#   1: @✗✓3↓flatten_473:CNode_491{[0]: ValueNode<FuncGraph> ↓✓3↓flatten_490}
#   2: @✗✓3↓flatten_473:CNode_492{[0]: ValueNode<Primitive> Return, [1]: CNode_491}


subgraph attr:
after_block : 1
subgraph instance: 4↓flatten_476 : 0x11fc7b418
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @4↓flatten_476 parent: [subgraph @3↓flatten_410]() {
  %1(x_rank) = $(3↓flatten_410):S_Prim_Rank(%para76_фinput)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1748/    x_rank = rank_(input)/
  %2(idx) = call @canonicalize_axis_493(%para64_start_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1755/    start_dim = canonicalize_axis(start_dim, x_rank)/
  %3(end_dim) = call @canonicalize_axis_493(%para65_end_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1756/    end_dim = canonicalize_axis(end_dim, x_rank)/
  %4(CNode_495) = call @check_dim_valid_494(%2, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1757/    check_dim_valid(start_dim, end_dim)/
  %5(CNode_496) = StopGradient(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
  %6(CNode_497) = S_Prim_MakeTuple(I64(0), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1759/    if x_rank in (0, 1):/
  %7(CNode_498) = S_Prim_in(%1, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1759/    if x_rank in (0, 1):/
  %8(CNode_499) = Cond(%7, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1759/    if x_rank in (0, 1):/
  %9(CNode_500) = Switch(%8, @✓4↓flatten_501, @✗4↓flatten_502)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1759/    if x_rank in (0, 1):/
  %10(CNode_503) = %9()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1759/    if x_rank in (0, 1):/
  %11(CNode_504) = Depend[side_effect_propagate: I64(1)](%10, %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1759/    if x_rank in (0, 1):/
  Return(%11)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1759/    if x_rank in (0, 1):/
}
# Order:
#   1: @4↓flatten_476:idx{[0]: ValueNode<FuncGraph> canonicalize_axis_493, [1]: param_start_dim, [2]: x_rank}
#   2: @4↓flatten_476:end_dim{[0]: ValueNode<FuncGraph> canonicalize_axis_493, [1]: param_end_dim, [2]: x_rank}
#   3: @4↓flatten_476:CNode_495{[0]: ValueNode<FuncGraph> check_dim_valid_494, [1]: idx, [2]: end_dim}
#   4: @4↓flatten_476:CNode_497{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<Int64Imm> 0, [2]: ValueNode<Int64Imm> 1}
#   5: @4↓flatten_476:CNode_498{[0]: ValueNode<DoSignaturePrimitive> S_Prim_in, [1]: x_rank, [2]: CNode_497}
#   6: @4↓flatten_476:CNode_499{[0]: ValueNode<Primitive> Cond, [1]: CNode_498, [2]: ValueNode<BoolImm> false}
#   7: @4↓flatten_476:CNode_500{[0]: ValueNode<Primitive> Switch, [1]: CNode_499, [2]: ValueNode<FuncGraph> ✓4↓flatten_501, [3]: ValueNode<FuncGraph> ✗4↓flatten_502}
#   8: @4↓flatten_476:CNode_503{[0]: CNode_500}
#   9: @4↓flatten_476:CNode_505{[0]: ValueNode<Primitive> Return, [1]: CNode_504}


subgraph attr:
subgraph instance: ✓_get_cache_prim_483 : 0x11fc81818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/_primitive_cache.py:35/def _get_cache_prim(cls: Primitive) -> Primitive:/
subgraph @✓_get_cache_prim_483 parent: [subgraph @_get_cache_prim_463]() {
  Return(@_new_prim_for_graph_506)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/_primitive_cache.py:89/        return _new_prim_for_graph/
}
# Order:
#   1: @✓_get_cache_prim_483:CNode_507{[0]: ValueNode<Primitive> Return, [1]: ValueNode<FuncGraph> _new_prim_for_graph_506}


subgraph attr:
after_block : 1
subgraph instance: ↓✓3↓flatten_490 : 0x11fc80c18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↓✓3↓flatten_490 parent: [subgraph @3↓flatten_410]() {
  %1(CNode_508) = call @_get_cache_prim_463(ClassType)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1752/        return _get_cache_prim(P.Flatten)()(input)/
  %2(CNode_509) = %1()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1752/        return _get_cache_prim(P.Flatten)()(input)/
  %3(CNode_510) = %2(%para76_фinput)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1752/        return _get_cache_prim(P.Flatten)()(input)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1752/        return _get_cache_prim(P.Flatten)()(input)/
}
# Order:
#   1: @↓✓3↓flatten_490:CNode_508{[0]: ValueNode<FuncGraph> _get_cache_prim_463, [1]: ValueNode<ClassType> class 'mindspore.ops.operations.nn_ops.Flatten'}
#   2: @↓✓3↓flatten_490:CNode_509{[0]: CNode_508}
#   3: @↓✓3↓flatten_490:CNode_510{[0]: CNode_509, [1]: param_фinput}
#   4: @↓✓3↓flatten_490:CNode_511{[0]: ValueNode<Primitive> Return, [1]: CNode_510}


subgraph attr:
subgraph instance: check_dim_valid_494 : 0x11fc67418
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1721/    def check_dim_valid(start_dim, end_dim):/
subgraph @check_dim_valid_494(%para79_start_dim, %para80_end_dim) {
  %1(CNode_512) = S_Prim_greater(%para79_start_dim, %para80_end_dim)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1722/        if start_dim > end_dim:/
  %2(CNode_513) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1722/        if start_dim > end_dim:/
  %3(CNode_514) = Switch(%2, @✓check_dim_valid_515, @✗check_dim_valid_516)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1722/        if start_dim > end_dim:/
  %4(CNode_517) = %3()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1722/        if start_dim > end_dim:/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1722/        if start_dim > end_dim:/
}
# Order:
#   1: @check_dim_valid_494:CNode_512{[0]: ValueNode<DoSignaturePrimitive> S_Prim_greater, [1]: param_start_dim, [2]: param_end_dim}
#   2: @check_dim_valid_494:CNode_513{[0]: ValueNode<Primitive> Cond, [1]: CNode_512, [2]: ValueNode<BoolImm> false}
#   3: @check_dim_valid_494:CNode_514{[0]: ValueNode<Primitive> Switch, [1]: CNode_513, [2]: ValueNode<FuncGraph> ✓check_dim_valid_515, [3]: ValueNode<FuncGraph> ✗check_dim_valid_516}
#   4: @check_dim_valid_494:CNode_517{[0]: CNode_514}
#   5: @check_dim_valid_494:CNode_518{[0]: ValueNode<Primitive> Return, [1]: CNode_517}


subgraph attr:
subgraph instance: canonicalize_axis_493 : 0x11fc7ea18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1725/    def canonicalize_axis(axis, x_rank):/
subgraph @canonicalize_axis_493(%para81_axis, %para82_x_rank) {
  %1(CNode_519) = S_Prim_not_equal(%para82_x_rank, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %2(CNode_520) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %3(CNode_521) = Switch(%2, @↰canonicalize_axis_522, @↱canonicalize_axis_523)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %4(ndim) = %3()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %5(CNode_525) = call @check_axis_valid_524(%para81_axis, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1727/        check_axis_valid(axis, ndim)/
  %6(CNode_526) = StopGradient(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1725/    def canonicalize_axis(axis, x_rank):/
  %7(CNode_527) = S_Prim_greater_equal(%para81_axis, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  %8(CNode_528) = Cond(%7, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  %9(CNode_529) = Switch(%8, @↰canonicalize_axis_530, @↱canonicalize_axis_531)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  %10(CNode_532) = %9()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  %11(CNode_533) = Depend[side_effect_propagate: I64(1)](%10, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  Return(%11)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
}
# Order:
#   1: @canonicalize_axis_493:CNode_519{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: param_x_rank, [2]: ValueNode<Int64Imm> 0}
#   2: @canonicalize_axis_493:CNode_520{[0]: ValueNode<Primitive> Cond, [1]: CNode_519, [2]: ValueNode<BoolImm> false}
#   3: @canonicalize_axis_493:CNode_521{[0]: ValueNode<Primitive> Switch, [1]: CNode_520, [2]: ValueNode<FuncGraph> ↰canonicalize_axis_522, [3]: ValueNode<FuncGraph> ↱canonicalize_axis_523}
#   4: @canonicalize_axis_493:ndim{[0]: CNode_521}
#   5: @canonicalize_axis_493:CNode_525{[0]: ValueNode<FuncGraph> check_axis_valid_524, [1]: param_axis, [2]: ndim}
#   6: @canonicalize_axis_493:CNode_527{[0]: ValueNode<DoSignaturePrimitive> S_Prim_greater_equal, [1]: param_axis, [2]: ValueNode<Int64Imm> 0}
#   7: @canonicalize_axis_493:CNode_528{[0]: ValueNode<Primitive> Cond, [1]: CNode_527, [2]: ValueNode<BoolImm> false}
#   8: @canonicalize_axis_493:CNode_529{[0]: ValueNode<Primitive> Switch, [1]: CNode_528, [2]: ValueNode<FuncGraph> ↰canonicalize_axis_530, [3]: ValueNode<FuncGraph> ↱canonicalize_axis_531}
#   9: @canonicalize_axis_493:CNode_532{[0]: CNode_529}
#  10: @canonicalize_axis_493:CNode_534{[0]: ValueNode<Primitive> Return, [1]: CNode_533}


subgraph attr:
subgraph instance: ✓4↓flatten_501 : 0x11fc79a18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✓4↓flatten_501 parent: [subgraph @3↓flatten_410]() {
  %1(CNode_535) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1760/        return reshape_(input, (-1,))/
  %2(CNode_536) = S_Prim_MakeTuple(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1760/        return reshape_(input, (-1,))/
  %3(CNode_537) = S_Prim_Reshape[output_names: ["output"], input_names: ["tensor", "shape"]](%para76_фinput, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1760/        return reshape_(input, (-1,))/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1760/        return reshape_(input, (-1,))/
}
# Order:
#   1: @✓4↓flatten_501:CNode_535{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @✓4↓flatten_501:CNode_536{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_535}
#   3: @✓4↓flatten_501:CNode_537{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_фinput, [2]: CNode_536}
#   4: @✓4↓flatten_501:CNode_538{[0]: ValueNode<Primitive> Return, [1]: CNode_537}


subgraph attr:
subgraph instance: ✗4↓flatten_502 : 0x11fc7ba18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✗4↓flatten_502 parent: [subgraph @4↓flatten_476]() {
  %1(CNode_540) = call @5↓flatten_539()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1759/    if x_rank in (0, 1):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1759/    if x_rank in (0, 1):/
}
# Order:
#   1: @✗4↓flatten_502:CNode_540{[0]: ValueNode<FuncGraph> 5↓flatten_539}
#   2: @✗4↓flatten_502:CNode_541{[0]: ValueNode<Primitive> Return, [1]: CNode_540}


subgraph attr:
subgraph instance: _new_prim_for_graph_506 : 0x11fc81e18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/_primitive_cache.py:67/    def _new_prim_for_graph(*args, **kwargs) -> Primitive:/
subgraph @_new_prim_for_graph_506 parent: [subgraph @_get_cache_prim_463](%para83_args, %para84_kwargs) {
  %1(CNode_542) = UnpackCall_unpack_call(%para78_cls, %para83_args, %para84_kwargs)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/_primitive_cache.py:68/        return cls(*args, **kwargs)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/_primitive_cache.py:68/        return cls(*args, **kwargs)/
}
# Order:
#   1: @_new_prim_for_graph_506:CNode_542{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.543, [1]: param_cls, [2]: param_args, [3]: param_kwargs}
#   2: @_new_prim_for_graph_506:CNode_544{[0]: ValueNode<Primitive> Return, [1]: CNode_542}


subgraph attr:
subgraph instance: ✓check_dim_valid_515 : 0x11fc68618
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1721/    def check_dim_valid(start_dim, end_dim):/
subgraph @✓check_dim_valid_515() {
  %1(CNode_545) = raise[side_effect_io: Bool(1)]("ValueError", "For 'flatten', 'start_dim' cannot come after 'end_dim'.", "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1723/            raise ValueError("For 'flatten', 'start_dim' cannot come after 'end_dim'.")/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1723/            raise ValueError("For 'flatten', 'start_dim' cannot come after 'end_dim'.")/
}
# Order:
#   1: @✓check_dim_valid_515:CNode_545{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> ValueError, [2]: ValueNode<StringImm> For 'flatten', 'start_dim' cannot come after 'end_dim'., [3]: ValueNode<StringImm> None}
#   2: @✓check_dim_valid_515:CNode_546{[0]: ValueNode<Primitive> Return, [1]: CNode_545}


subgraph attr:
subgraph instance: ✗check_dim_valid_516 : 0x11fc67a18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1721/    def check_dim_valid(start_dim, end_dim):/
subgraph @✗check_dim_valid_516() {
  %1(CNode_548) = call @↓check_dim_valid_547()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1722/        if start_dim > end_dim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1722/        if start_dim > end_dim:/
}
# Order:
#   1: @✗check_dim_valid_516:CNode_548{[0]: ValueNode<FuncGraph> ↓check_dim_valid_547}
#   2: @✗check_dim_valid_516:CNode_549{[0]: ValueNode<Primitive> Return, [1]: CNode_548}


subgraph attr:
subgraph instance: check_axis_valid_524 : 0x11fc65018
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @check_axis_valid_524(%para85_axis, %para86_ndim) {
  %1(CNode_550) = S_Prim_negative(%para86_ndim)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %2(CNode_551) = S_Prim_less(%para85_axis, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %3(CNode_552) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %4(CNode_553) = Switch(%3, @↰check_axis_valid_554, @↱check_axis_valid_555)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %5(CNode_556) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %6(CNode_557) = Cond(%5, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %7(CNode_558) = Switch(%6, @✓check_axis_valid_559, @✗check_axis_valid_560)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %8(CNode_561) = %7()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_524:CNode_550{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: param_ndim}
#   2: @check_axis_valid_524:CNode_551{[0]: ValueNode<DoSignaturePrimitive> S_Prim_less, [1]: param_axis, [2]: CNode_550}
#   3: @check_axis_valid_524:CNode_552{[0]: ValueNode<Primitive> Cond, [1]: CNode_551, [2]: ValueNode<BoolImm> false}
#   4: @check_axis_valid_524:CNode_553{[0]: ValueNode<Primitive> Switch, [1]: CNode_552, [2]: ValueNode<FuncGraph> ↰check_axis_valid_554, [3]: ValueNode<FuncGraph> ↱check_axis_valid_555}
#   5: @check_axis_valid_524:CNode_556{[0]: CNode_553}
#   6: @check_axis_valid_524:CNode_557{[0]: ValueNode<Primitive> Cond, [1]: CNode_556, [2]: ValueNode<BoolImm> false}
#   7: @check_axis_valid_524:CNode_558{[0]: ValueNode<Primitive> Switch, [1]: CNode_557, [2]: ValueNode<FuncGraph> ✓check_axis_valid_559, [3]: ValueNode<FuncGraph> ✗check_axis_valid_560}
#   8: @check_axis_valid_524:CNode_561{[0]: CNode_558}
#   9: @check_axis_valid_524:CNode_562{[0]: ValueNode<Primitive> Return, [1]: CNode_561}


subgraph attr:
subgraph instance: ↰canonicalize_axis_522 : 0x11fc64a18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
subgraph @↰canonicalize_axis_522 parent: [subgraph @canonicalize_axis_493]() {
  Return(%para82_x_rank)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
}
# Order:
#   1: @↰canonicalize_axis_522:CNode_563{[0]: ValueNode<Primitive> Return, [1]: param_x_rank}


subgraph attr:
subgraph instance: ↱canonicalize_axis_523 : 0x11fc64418
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
subgraph @↱canonicalize_axis_523() {
  Return(I64(1))
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
}
# Order:
#   1: @↱canonicalize_axis_523:CNode_564{[0]: ValueNode<Primitive> Return, [1]: ValueNode<Int64Imm> 1}


subgraph attr:
subgraph instance: ↰canonicalize_axis_530 : 0x11fc63e18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
subgraph @↰canonicalize_axis_530 parent: [subgraph @canonicalize_axis_493]() {
  Return(%para81_axis)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
}
# Order:
#   1: @↰canonicalize_axis_530:CNode_565{[0]: ValueNode<Primitive> Return, [1]: param_axis}


subgraph attr:
subgraph instance: ↱canonicalize_axis_531 : 0x11fc63818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
subgraph @↱canonicalize_axis_531 parent: [subgraph @canonicalize_axis_493]() {
  %1(CNode_519) = $(canonicalize_axis_493):S_Prim_not_equal(%para82_x_rank, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %2(CNode_520) = $(canonicalize_axis_493):Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %3(CNode_521) = $(canonicalize_axis_493):Switch(%2, @↰canonicalize_axis_522, @↱canonicalize_axis_523)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %4(ndim) = $(canonicalize_axis_493):%3()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %5(CNode_566) = S_Prim_add(%para81_axis, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
}
# Order:
#   1: @↱canonicalize_axis_531:CNode_566{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: param_axis, [2]: ndim}
#   2: @↱canonicalize_axis_531:CNode_567{[0]: ValueNode<Primitive> Return, [1]: CNode_566}


subgraph attr:
after_block : 1
subgraph instance: 5↓flatten_539 : 0x11fc7c018
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @5↓flatten_539 parent: [subgraph @4↓flatten_476]() {
  %1(x_rank) = $(3↓flatten_410):S_Prim_Rank(%para76_фinput)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1748/    x_rank = rank_(input)/
  %2(idx) = $(4↓flatten_476):call @canonicalize_axis_493(%para64_start_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1755/    start_dim = canonicalize_axis(start_dim, x_rank)/
  %3(end_dim) = $(4↓flatten_476):call @canonicalize_axis_493(%para65_end_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1756/    end_dim = canonicalize_axis(end_dim, x_rank)/
  %4(CNode_568) = S_Prim_equal(%2, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1762/    if start_dim == end_dim:/
  %5(CNode_569) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1762/    if start_dim == end_dim:/
  %6(CNode_570) = Switch(%5, @✓5↓flatten_571, @✗5↓flatten_572)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1762/    if start_dim == end_dim:/
  %7(CNode_573) = %6()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1762/    if start_dim == end_dim:/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1762/    if start_dim == end_dim:/
}
# Order:
#   1: @5↓flatten_539:CNode_568{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: idx, [2]: end_dim}
#   2: @5↓flatten_539:CNode_569{[0]: ValueNode<Primitive> Cond, [1]: CNode_568, [2]: ValueNode<BoolImm> false}
#   3: @5↓flatten_539:CNode_570{[0]: ValueNode<Primitive> Switch, [1]: CNode_569, [2]: ValueNode<FuncGraph> ✓5↓flatten_571, [3]: ValueNode<FuncGraph> ✗5↓flatten_572}
#   4: @5↓flatten_539:CNode_573{[0]: CNode_570}
#   5: @5↓flatten_539:CNode_574{[0]: ValueNode<Primitive> Return, [1]: CNode_573}


subgraph attr:
after_block : 1
subgraph instance: ↓check_dim_valid_547 : 0x11fc68018
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1721/    def check_dim_valid(start_dim, end_dim):/
subgraph @↓check_dim_valid_547() {
  Return(None)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
}
# Order:
#   1: @↓check_dim_valid_547:CNode_575{[0]: ValueNode<Primitive> Return, [1]: ValueNode<None> None}


subgraph attr:
subgraph instance: ✓check_axis_valid_559 : 0x11fc66e18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @✓check_axis_valid_559() {
  %1(CNode_576) = raise[side_effect_io: Bool(1)]("ValueError", "'start_dim' or 'end_dim' out of range.", "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1719/            raise ValueError("'start_dim' or 'end_dim' out of range.")/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1719/            raise ValueError("'start_dim' or 'end_dim' out of range.")/
}
# Order:
#   1: @✓check_axis_valid_559:CNode_576{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> ValueError, [2]: ValueNode<StringImm> 'start_dim' or 'end_dim' out of range., [3]: ValueNode<StringImm> None}
#   2: @✓check_axis_valid_559:CNode_577{[0]: ValueNode<Primitive> Return, [1]: CNode_576}


subgraph attr:
subgraph instance: ✗check_axis_valid_560 : 0x11fc66218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @✗check_axis_valid_560() {
  %1(CNode_579) = call @↓check_axis_valid_578()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @✗check_axis_valid_560:CNode_579{[0]: ValueNode<FuncGraph> ↓check_axis_valid_578}
#   2: @✗check_axis_valid_560:CNode_580{[0]: ValueNode<Primitive> Return, [1]: CNode_579}


subgraph attr:
subgraph instance: ↰check_axis_valid_554 : 0x11fc65c18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @↰check_axis_valid_554 parent: [subgraph @check_axis_valid_524]() {
  %1(CNode_550) = $(check_axis_valid_524):S_Prim_negative(%para86_ndim)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %2(CNode_551) = $(check_axis_valid_524):S_Prim_less(%para85_axis, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @↰check_axis_valid_554:CNode_581{[0]: ValueNode<Primitive> Return, [1]: CNode_551}


subgraph attr:
subgraph instance: ↱check_axis_valid_555 : 0x11fc65618
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @↱check_axis_valid_555 parent: [subgraph @check_axis_valid_524]() {
  %1(CNode_582) = S_Prim_greater_equal(%para85_axis, %para86_ndim)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @↱check_axis_valid_555:CNode_582{[0]: ValueNode<DoSignaturePrimitive> S_Prim_greater_equal, [1]: param_axis, [2]: param_ndim}
#   2: @↱check_axis_valid_555:CNode_583{[0]: ValueNode<Primitive> Return, [1]: CNode_582}


subgraph attr:
subgraph instance: ✓5↓flatten_571 : 0x11fc7e418
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✓5↓flatten_571 parent: [subgraph @3↓flatten_410]() {
  Return(%para76_фinput)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1763/        return input/
}
# Order:
#   1: @✓5↓flatten_571:CNode_584{[0]: ValueNode<Primitive> Return, [1]: param_фinput}


subgraph attr:
subgraph instance: ✗5↓flatten_572 : 0x11fc7c618
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✗5↓flatten_572 parent: [subgraph @4↓flatten_476]() {
  %1(CNode_586) = call @6↓flatten_585()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1762/    if start_dim == end_dim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1762/    if start_dim == end_dim:/
}
# Order:
#   1: @✗5↓flatten_572:CNode_586{[0]: ValueNode<FuncGraph> 6↓flatten_585}
#   2: @✗5↓flatten_572:CNode_587{[0]: ValueNode<Primitive> Return, [1]: CNode_586}


subgraph attr:
after_block : 1
subgraph instance: ↓check_axis_valid_578 : 0x11fc66818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @↓check_axis_valid_578() {
  Return(None)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
}
# Order:
#   1: @↓check_axis_valid_578:CNode_588{[0]: ValueNode<Primitive> Return, [1]: ValueNode<None> None}


subgraph attr:
after_block : 1
subgraph instance: 6↓flatten_585 : 0x11fc7cc18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @6↓flatten_585 parent: [subgraph @4↓flatten_476]() {
  %1(x_rank) = $(3↓flatten_410):S_Prim_Rank(%para76_фinput)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1748/    x_rank = rank_(input)/
  %2(idx) = $(4↓flatten_476):call @canonicalize_axis_493(%para64_start_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1755/    start_dim = canonicalize_axis(start_dim, x_rank)/
  %3(CNode_590) = call @↵6↓flatten_589(%2, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1767/    while idx <= end_dim:/
}
# Order:
#   1: @6↓flatten_585:CNode_591{[0]: ValueNode<Primitive> Return, [1]: CNode_590}
#   2: @6↓flatten_585:CNode_590{[0]: ValueNode<FuncGraph> ↵6↓flatten_589, [1]: idx, [2]: ValueNode<Int64Imm> 1}


subgraph attr:
is_while_header : 1
subgraph instance: ↵6↓flatten_589 : 0x11fc7d218
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↵6↓flatten_589 parent: [subgraph @4↓flatten_476](%para87_, %para88_) {
  %1(x_rank) = $(3↓flatten_410):S_Prim_Rank(%para76_фinput)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1748/    x_rank = rank_(input)/
  %2(end_dim) = $(4↓flatten_476):call @canonicalize_axis_493(%para65_end_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1756/    end_dim = canonicalize_axis(end_dim, x_rank)/
  %3(CNode_592) = S_Prim_less_equal(%para87_фidx, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1767/    while idx <= end_dim:/
  %4(force_while_cond_CNode_592) = Cond(%3, Bool(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1767/    while idx <= end_dim:/
  %5(CNode_593) = Switch(%4, @↻6↓flatten_594, @7↓flatten_595)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1767/    while idx <= end_dim:/
  %6(CNode_596) = %5()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1767/    while idx <= end_dim:/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1767/    while idx <= end_dim:/
}
# Order:
#   1: @↵6↓flatten_589:CNode_592{[0]: ValueNode<DoSignaturePrimitive> S_Prim_less_equal, [1]: param_фidx, [2]: end_dim}
#   2: @↵6↓flatten_589:force_while_cond_CNode_592{[0]: ValueNode<Primitive> Cond, [1]: CNode_592, [2]: ValueNode<BoolImm> true}
#   3: @↵6↓flatten_589:CNode_593{[0]: ValueNode<Primitive> Switch, [1]: force_while_cond_CNode_592, [2]: ValueNode<FuncGraph> ↻6↓flatten_594, [3]: ValueNode<FuncGraph> 7↓flatten_595}
#   4: @↵6↓flatten_589:CNode_596{[0]: CNode_593}
#   5: @↵6↓flatten_589:CNode_597{[0]: ValueNode<Primitive> Return, [1]: CNode_596}


subgraph attr:
subgraph instance: ↻6↓flatten_594 : 0x11fc7de18
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↻6↓flatten_594 parent: [subgraph @↵6↓flatten_589]() {
  %1(idx) = S_Prim_add(%para87_фidx, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1769/        idx += 1/
  %2(x_shape) = $(3↓flatten_410):S_Prim_Shape(%para76_фinput)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1747/    x_shape = shape_(input)/
  %3(CNode_598) = S_Prim_getitem(%2, %para87_фidx)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1768/        dim_length *= x_shape[idx]/
  %4(dim_length) = S_Prim_mul(%para88_фdim_length, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1768/        dim_length *= x_shape[idx]/
  %5(CNode_599) = call @↵6↓flatten_589(%1, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1767/    while idx <= end_dim:/
}
# Order:
#   1: @↻6↓flatten_594:CNode_598{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: param_фidx}
#   2: @↻6↓flatten_594:dim_length{[0]: ValueNode<DoSignaturePrimitive> S_Prim_mul, [1]: param_фdim_length, [2]: CNode_598}
#   3: @↻6↓flatten_594:idx{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: param_фidx, [2]: ValueNode<Int64Imm> 1}
#   4: @↻6↓flatten_594:CNode_600{[0]: ValueNode<Primitive> Return, [1]: CNode_599}
#   5: @↻6↓flatten_594:CNode_599{[0]: ValueNode<FuncGraph> ↵6↓flatten_589, [1]: idx, [2]: dim_length}


subgraph attr:
subgraph instance: 7↓flatten_595 : 0x11fc7d818
# In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @7↓flatten_595 parent: [subgraph @↵6↓flatten_589]() {
  %1(x_shape) = $(3↓flatten_410):S_Prim_Shape(%para76_фinput)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1747/    x_shape = shape_(input)/
  %2(x_rank) = $(3↓flatten_410):S_Prim_Rank(%para76_фinput)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1748/    x_rank = rank_(input)/
  %3(idx) = $(4↓flatten_476):call @canonicalize_axis_493(%para64_start_dim, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1755/    start_dim = canonicalize_axis(start_dim, x_rank)/
  %4(CNode_601) = S_Prim_make_slice(None, %3, None)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %5(CNode_602) = S_Prim_getitem(%1, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %6(CNode_603) = S_Prim_MakeTuple(%para88_фdim_length)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %7(CNode_604) = S_Prim_add(%5, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %8(end_dim) = $(4↓flatten_476):call @canonicalize_axis_493(%para65_end_dim, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1756/    end_dim = canonicalize_axis(end_dim, x_rank)/
  %9(CNode_605) = S_Prim_add(%8, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %10(CNode_606) = S_Prim_make_slice(%9, None, None)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %11(CNode_607) = S_Prim_getitem(%1, %10)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %12(new_shape) = S_Prim_add(%7, %11)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %13(CNode_608) = S_Prim_Reshape[output_names: ["output"], input_names: ["tensor", "shape"]](%para76_фinput, %12)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1771/    return reshape_(input, new_shape)/
  Return(%13)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /Users/qi7876/miniconda3/envs/GC2/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1771/    return reshape_(input, new_shape)/
}
# Order:
#   1: @7↓flatten_595:CNode_601{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_slice, [1]: ValueNode<None> None, [2]: idx, [3]: ValueNode<None> None}
#   2: @7↓flatten_595:CNode_602{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: CNode_601}
#   3: @7↓flatten_595:CNode_603{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: param_фdim_length}
#   4: @7↓flatten_595:CNode_604{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: CNode_602, [2]: CNode_603}
#   5: @7↓flatten_595:CNode_605{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: end_dim, [2]: ValueNode<Int64Imm> 1}
#   6: @7↓flatten_595:CNode_606{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_slice, [1]: CNode_605, [2]: ValueNode<None> None, [3]: ValueNode<None> None}
#   7: @7↓flatten_595:CNode_607{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: CNode_606}
#   8: @7↓flatten_595:new_shape{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: CNode_604, [2]: CNode_607}
#   9: @7↓flatten_595:CNode_608{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_фinput, [2]: new_shape}
#  10: @7↓flatten_595:CNode_609{[0]: ValueNode<Primitive> Return, [1]: CNode_608}


